{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPu9dCdO0YRWXgU8kSoV4FY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**THE MODEL RISK**\n","\n","---"],"metadata":{"id":"zbSiKRQpQz5l"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"kKLSA_GRQ4om"}},{"cell_type":"markdown","source":["https://claude.ai/share/9d0f0712-4f31-47b6-8b40-205934c95f8d"],"metadata":{"id":"Xq6oL6GbdFoU"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"IfT23Z10Q6IH"}},{"cell_type":"markdown","source":["\n","\n","In algorithmic trading, model risk represents an invisible tax on alpha—the gap between what your models promise in backtests and what they deliver in live markets. This chapter builds a comprehensive framework for managing that risk through three interconnected pillars: explainability, robustness testing, and continuous monitoring.\n","\n","**Why This Matters**\n","\n","Every quantitative trading strategy operates under uncertainty. Markets evolve, regimes shift, and execution costs fluctuate. A model that performs brilliantly in historical data can fail catastrophically when deployed if it hasn't been stress-tested against realistic scenarios. Worse, without proper explainability tools, you won't understand why it's failing until significant losses accumulate. This isn't just about regulatory compliance—though that matters too—it's about building strategies that survive contact with reality.\n","\n","**The Core Philosophy**\n","\n","This notebook takes a \"governance-native\" approach, meaning risk management isn't bolted on after the fact—it's embedded from the first line of code. Every feature is constructed causally with explicit time lags. Every model prediction comes with diagnostics showing which features drove the decision. Every backtest includes transaction costs, and every result is hashed and traced for reproducibility. Think of it as defensive programming for quantitative finance: we assume things will go wrong and build systems to detect problems before they become disasters.\n","\n","**What You'll Learn**\n","\n","We start with explainability—the ability to understand what your models are doing and why. This includes global explanations (which features matter most overall), local explanations (what drove decisions on specific days), and sensitivity analysis (how fragile are your predictions to small input changes). Explainability isn't just for regulators; it's your primary debugging tool when live performance diverges from expectations.\n","\n","Next, we build a robustness test suite—a battery of stress tests that probe your strategy's weaknesses before the market does. We test temporal stability through walk-forward analysis, cross-sectional stability by randomly dropping assets, microstructure sensitivity by varying transaction costs and latency, regime robustness by conditioning on volatility states, and adversarial perturbations by injecting noise and missingness into features. Each test has pass/fail thresholds defined upfront, turning subjective judgment into objective gates.\n","\n","Finally, we implement continuous monitoring with causal online diagnostics. Markets don't stand still, and neither should your risk management. We track input drift (are feature distributions shifting?), output drift (is turnover spiking unexpectedly?), and outcome drift (is realized slippage diverging from expectations?). A state machine with persistence and hysteresis prevents alert fatigue while ensuring genuine problems escalate appropriately.\n","\n","**Practical Implementation**\n","\n","Throughout, we work with synthetic multi-asset data featuring realistic regime transitions, correlation structures, and tail events. This synthetic-first approach means the notebook runs end-to-end without external dependencies, while optional adapters show how to apply these techniques to real market data. Every artifact—model cards, data sheets, evaluation matrices, monitoring specs, and post-mortem templates—is automatically generated and saved with cryptographic hashes for audit trails.\n","\n","By the end, you'll have a production-grade risk management framework that treats model governance not as paperwork, but as an integral part of building strategies that actually work."],"metadata":{"id":"mfAybQP7cU_6"}},{"cell_type":"code","source":[],"metadata":{"id":"fAuscuvpQ7QS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIROMENT"],"metadata":{"id":"6HLbakghQ7ri"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib\n","matplotlib.use('Agg')  # non-interactive backend for Colab\n","import matplotlib.pyplot as plt\n","import math\n","import random\n","import itertools\n","from collections import defaultdict, Counter\n","import datetime\n","import json\n","import hashlib\n","import os\n","import textwrap\n","\n","# Configuration dictionary\n","CONFIG = {\n","    \"seed\": 42,\n","    \"horizon_H\": 1,  # prediction horizon in steps\n","    \"bar_frequency\": \"daily\",\n","    \"n_assets\": 10,\n","    \"n_steps\": 1000,\n","    \"train_len\": 500,  # Reduced from 600 to fit within 1000 steps\n","    \"test_len\": 300,   # Reduced from 400 to fit\n","    \"embargo\": 5,      # embargo period after train\n","    \"regime_params\": {\n","        \"n_regimes\": 2,\n","        \"transition_prob\": 0.02,  # probability of regime switch per step\n","        \"vol_low\": 0.01,\n","        \"vol_high\": 0.03,\n","        \"corr_low\": 0.3,\n","        \"corr_high\": 0.7,\n","    },\n","    \"cost_params\": {\n","        \"spread_bps\": 5.0,  # spread cost in bps\n","        \"impact_coef\": 0.5,  # quadratic impact coefficient\n","    },\n","    \"robustness_thresholds\": {\n","        \"min_net_sharpe\": 0.5,\n","        \"max_drawdown\": -0.15,\n","        \"max_turnover\": 2.0,  # max annualized turnover\n","        \"max_abs_weight\": 0.3,\n","    },\n","    \"monitoring_thresholds\": {\n","        \"feature_shift_z\": 3.0,  # z-score threshold for feature drift\n","        \"turnover_spike_mult\": 2.5,  # multiplier over rolling mean\n","        \"drawdown_alarm\": -0.10,\n","        \"alert_budget_per_100_steps\": 5,\n","    },\n","    \"output_dir\": \"/content/ch21_run\",\n","}\n","\n","SEED = CONFIG[\"seed\"]\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","print(f\"\\n[DETERMINISM] Global seed set to {SEED}\")\n","print(f\"[CONFIG] Loaded configuration with {len(CONFIG)} top-level keys\")\n","print(f\"[CONFIG] n_assets={CONFIG['n_assets']}, n_steps={CONFIG['n_steps']}\")\n","print(f\"[CONFIG] train_len={CONFIG['train_len']}, test_len={CONFIG['test_len']}, embargo={CONFIG['embargo']}\")\n","\n","# Verify split fits\n","train_len = CONFIG[\"train_len\"]\n","embargo = CONFIG[\"embargo\"]\n","test_len = CONFIG[\"test_len\"]\n","total_needed = train_len + embargo + test_len\n","print(f\"[CONFIG] Total steps needed: {total_needed} (train={train_len} + embargo={embargo} + test={test_len})\")\n","print(f\"[CONFIG] Available steps: {CONFIG['n_steps']}\")\n","assert total_needed <= CONFIG['n_steps'], \\\n","    f\"Split requires {total_needed} steps but only {CONFIG['n_steps']} available\"\n","print(f\"[CONFIG] Split validation passed: {total_needed} <= {CONFIG['n_steps']}\")\n","\n","# Cell 2 — Governance Utilities (Artifacts + Hashing)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 2: GOVERNANCE UTILITIES\")\n","print(\"=\" * 80)\n","\n","def make_run_id():\n","    \"\"\"Generate deterministic run ID from timestamp + seed-derived suffix.\"\"\"\n","    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    # Use seed to derive a deterministic suffix\n","    rng_state = np.random.RandomState(SEED)\n","    suffix = rng_state.randint(1000, 9999)\n","    return f\"{ts}_{suffix}\"\n","\n","def sha256_of_bytes(data: bytes) -> str:\n","    \"\"\"Compute SHA256 hash of bytes.\"\"\"\n","    return hashlib.sha256(data).hexdigest()\n","\n","def sha256_of_json(obj) -> str:\n","    \"\"\"Compute SHA256 hash of JSON-serializable object.\"\"\"\n","    serialized = json.dumps(obj, sort_keys=True, indent=None)\n","    return sha256_of_bytes(serialized.encode('utf-8'))\n","\n","def write_json(path: str, obj):\n","    \"\"\"Write JSON object to file.\"\"\"\n","    with open(path, 'w') as f:\n","        json.dump(obj, f, indent=2)\n","    print(f\"[ARTIFACT] Written: {path}\")\n","\n","def write_text(path: str, s: str):\n","    \"\"\"Write text string to file.\"\"\"\n","    with open(path, 'w') as f:\n","        f.write(s)\n","    print(f\"[ARTIFACT] Written: {path}\")\n","\n","# Artifact registry\n","artifact_registry = {\n","    \"run_id\": None,\n","    \"run_manifest\": {},\n","    \"config_hash\": None,\n","    \"code_hash_placeholder\": \"code_hash_would_be_computed_from_notebook_source\",\n","    \"dataset_hash\": None,\n","    \"artifact_files\": [],\n","}\n","\n","RUN_ID = make_run_id()\n","artifact_registry[\"run_id\"] = RUN_ID\n","OUTPUT_DIR = CONFIG[\"output_dir\"] + f\"_{RUN_ID}\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(f\"[RUN_ID] {RUN_ID}\")\n","print(f\"[OUTPUT_DIR] {OUTPUT_DIR}\")\n","\n","# Compute config hash\n","config_hash = sha256_of_json(CONFIG)\n","artifact_registry[\"config_hash\"] = config_hash\n","print(f\"[CONFIG_HASH] {config_hash[:16]}...\")\n","\n","# Write config to disk\n","config_path = os.path.join(OUTPUT_DIR, \"config.json\")\n","write_json(config_path, CONFIG)\n","artifact_registry[\"artifact_files\"].append(\"config.json\")\n","\n","# Run manifest generator\n","def generate_run_manifest():\n","    \"\"\"Generate run manifest with seeds, config, timestamps, environment.\"\"\"\n","    manifest = {\n","        \"run_id\": RUN_ID,\n","        \"timestamp_start\": datetime.datetime.now().isoformat(),\n","        \"seed\": SEED,\n","        \"config_hash\": config_hash,\n","        \"code_hash\": artifact_registry[\"code_hash_placeholder\"],\n","        \"environment\": {\n","            \"python_version\": \"3.x\",  # placeholder\n","            \"numpy_version\": np.__version__,\n","            \"platform\": \"Google Colab\",\n","        },\n","        \"chapter\": 21,\n","        \"scope\": \"Model Risk, Explainability, Robustness\",\n","    }\n","    return manifest\n","\n","run_manifest = generate_run_manifest()\n","artifact_registry[\"run_manifest\"] = run_manifest\n","manifest_path = os.path.join(OUTPUT_DIR, \"run_manifest.json\")\n","write_json(manifest_path, run_manifest)\n","artifact_registry[\"artifact_files\"].append(\"run_manifest.json\")\n","\n","print(\"[GOVERNANCE] Utilities initialized, run manifest created.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6MRAA0oQ9we","executionInfo":{"status":"ok","timestamp":1767118308538,"user_tz":360,"elapsed":90,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"ad9f4b00-6479-46c6-98c5-839f6e0d6814"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","CHAPTER 21: MODEL RISK, EXPLAINABILITY, ROBUSTNESS\n","Book: AI and Algorithmic Trading by Alejandro Reynoso\n","================================================================================\n","\n","SCOPE BOUNDARY (HARD):\n","- Stay strictly within Chapter 21 topics.\n","- May reference Ch1-20 as prerequisites.\n","- May include transition note to Ch22, but no Ch22-25 content.\n","\n","HARD CONSTRAINTS:\n","1) NO pandas. NumPy + Python stdlib only.\n","2) Synthetic-data first. Real data optional isolated adapter.\n","3) Time-aware: no shuffling, no leakage, causal features.\n","4) Governance-native: seeds, manifests, hashes, artifacts.\n","5) First-principles: explicit rolling, splits, embargoes.\n","6) Asserts for causality and no-leakage.\n","7) Libraries: numpy, matplotlib, math, random, itertools, collections,\n","   datetime, json, hashlib, os, textwrap.\n","================================================================================\n","\n","[DETERMINISM] Global seed set to 42\n","[CONFIG] Loaded configuration with 13 top-level keys\n","[CONFIG] n_assets=10, n_steps=1000\n","[CONFIG] train_len=500, test_len=300, embargo=5\n","[CONFIG] Total steps needed: 805 (train=500 + embargo=5 + test=300)\n","[CONFIG] Available steps: 1000\n","[CONFIG] Split validation passed: 805 <= 1000\n","\n","================================================================================\n","CELL 2: GOVERNANCE UTILITIES\n","================================================================================\n","[RUN_ID] 20251230_181149_8270\n","[OUTPUT_DIR] /content/ch21_run_20251230_181149_8270\n","[CONFIG_HASH] 4e40adc773598dd9...\n","[ARTIFACT] Written: /content/ch21_run_20251230_181149_8270/config.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181149_8270/run_manifest.json\n","[GOVERNANCE] Utilities initialized, run manifest created.\n"]}]},{"cell_type":"markdown","source":["##3.SYNTHETIC MARKET GENERATION"],"metadata":{"id":"uKcUzZpRQ-cA"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"HLGIvadORABH"}},{"cell_type":"markdown","source":["\n","\n","**Purpose and Philosophy**\n","\n","This section constructs a synthetic multi-asset market that serves as our testing laboratory throughout the chapter. Rather than using real market data that may be incomplete, proprietary, or subject to survivorship bias, we generate artificial price series with carefully controlled statistical properties. This approach offers several advantages: complete reproducibility through deterministic random seeds, explicit control over regime transitions and correlation structures, and the ability to inject specific stress scenarios like tail events. Most importantly, synthetic data ensures the notebook runs end-to-end without external dependencies while teaching core concepts that transfer directly to real markets.\n","\n","**The Regime Process – Markets Have Moods**\n","\n","At the heart of our generator lies a two-state regime process representing low-volatility and high-volatility market environments. Think of these as \"calm markets\" versus \"turbulent markets.\" The system uses a Markov chain with a small transition probability (2% per day) to switch between regimes, creating extended periods of relative stability punctuated by occasional regime shifts—mimicking how real markets tend to cluster in behavioral states.\n","\n","- **Low-volatility regime**: Returns have 1% daily volatility, assets show moderate correlation (0.3)\n","- **High-volatility regime**: Returns jump to 3% daily volatility, correlation increases to 0.7\n","- **Why correlation matters**: In crisis periods, diversification benefits evaporate as assets move together\n","\n","This regime structure is crucial for robustness testing later. A strategy that works beautifully in low-vol periods may implode when volatility spikes, and we need to detect this vulnerability before deployment.\n","\n","**Cross-Asset Correlation Structure**\n","\n","Each time step generates correlated returns across our 10 synthetic assets using Cholesky decomposition—a standard technique for creating multivariate normal distributions with specified correlation matrices. The correlation isn't constant; it shifts with the regime. This regime-dependent correlation is a stylized fact from real markets: asset correlations tend to increase during drawdowns, exactly when you need diversification most.\n","\n","The correlation matrix is constructed simply: off-diagonal elements equal the regime's correlation parameter, diagonal elements are 1.0. This uniform correlation is pedagogically clear while capturing the essential behavior that matters for portfolio construction and risk management.\n","\n","**Tail Events – Because Markets Have Fat Tails**\n","\n","Normal distributions understate the frequency of extreme events in financial markets. To inject realistic tail risk, we randomly select 1% of days and add large shocks (5-10% moves) to all assets simultaneously. These event shocks create the fat-tailed return distributions observed in practice, ensuring our strategies are tested against occasional market dislocations rather than only smooth Gaussian noise.\n","\n","This matters enormously for stress testing. A strategy that appears robust under normal distributions may be hiding unhedged tail exposure that only reveals itself during rare but devastating events.\n","\n","**Price Series Construction**\n","\n","Starting from an arbitrary initial price (100 for all assets), we construct price paths by exponentiating cumulative returns. This ensures prices remain positive and captures the multiplicative nature of returns—an important detail since log-returns are additive but prices compound multiplicatively. The result is 1,000 time steps of 10-asset price data with realistic statistical properties.\n","\n","**Governance and Traceability**\n","\n","Every aspect of generation is controlled by parameters in the CONFIG dictionary: number of assets, time steps, regime parameters, transition probabilities, and volatility levels. These parameters are hashed and stored in the dataset summary JSON file. This cryptographic fingerprinting ensures that if we regenerate data with the same seed, we get identical results—critical for reproducibility in research and regulatory review.\n","\n","The dataset hash becomes part of our audit trail. When we evaluate model performance later, we can definitively link results to specific data characteristics. If performance degrades in production, we can regenerate the exact training environment to debug what changed.\n","\n","**Key Takeaways**\n","\n","- Synthetic data provides controlled experimentation without real-world dependencies\n","- Regime processes capture clustering in market volatility and correlation\n","- Tail events inject realistic fat-tailed distributions beyond normal assumptions\n","- Deterministic generation with seeds ensures complete reproducibility\n","- Hashing and artifact tracking establish governance from the data layer upward\n","\n","This synthetic market becomes our ground truth throughout the chapter, allowing us to know the \"true\" regime at each time step and evaluate whether our monitoring systems can detect regime changes before they cause losses."],"metadata":{"id":"eLMU3GjreAAE"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"z2HHpNvCRCyT"}},{"cell_type":"code","source":["\n","# Cell 3 — Synthetic Market Generator (Time-Aware, Multi-Asset)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 3: SYNTHETIC MARKET GENERATOR\")\n","print(\"=\" * 80)\n","\n","N_ASSETS = CONFIG[\"n_assets\"]\n","N_STEPS = CONFIG[\"n_steps\"]\n","REGIME_PARAMS = CONFIG[\"regime_params\"]\n","\n","def generate_synthetic_market(n_assets, n_steps, regime_params, seed=None):\n","    \"\"\"\n","    Generate synthetic OHLC-like data (we'll use close prices) for N assets over T steps.\n","    Include regime process (low-vol vs high-vol) with correlation structure changes.\n","    Optional event shocks for tails.\n","\n","    Returns:\n","        prices: (T, N) array\n","        returns: (T, N) array (log returns)\n","        true_regime: (T,) array (0 or 1)\n","    \"\"\"\n","    if seed is not None:\n","        rng = np.random.RandomState(seed)\n","    else:\n","        rng = np.random.RandomState()\n","\n","    n_regimes = regime_params[\"n_regimes\"]\n","    trans_prob = regime_params[\"transition_prob\"]\n","    vol_low = regime_params[\"vol_low\"]\n","    vol_high = regime_params[\"vol_high\"]\n","    corr_low = regime_params[\"corr_low\"]\n","    corr_high = regime_params[\"corr_high\"]\n","\n","    # Regime process (Markov chain)\n","    true_regime = np.zeros(n_steps, dtype=int)\n","    current_regime = 0\n","    for t in range(n_steps):\n","        true_regime[t] = current_regime\n","        if rng.rand() < trans_prob:\n","            current_regime = 1 - current_regime  # flip regime\n","\n","    # Generate returns with regime-dependent vol and correlation\n","    returns = np.zeros((n_steps, n_assets))\n","    for t in range(n_steps):\n","        regime = true_regime[t]\n","        vol = vol_low if regime == 0 else vol_high\n","        corr = corr_low if regime == 0 else corr_high\n","\n","        # Correlation matrix: off-diagonal = corr, diagonal = 1\n","        corr_matrix = np.full((n_assets, n_assets), corr)\n","        np.fill_diagonal(corr_matrix, 1.0)\n","\n","        # Cholesky decomposition for correlated normals\n","        try:\n","            L = np.linalg.cholesky(corr_matrix)\n","        except np.linalg.LinAlgError:\n","            # fallback: identity\n","            L = np.eye(n_assets)\n","\n","        z = rng.randn(n_assets)\n","        corr_z = L @ z\n","        returns[t, :] = vol * corr_z\n","\n","    # Add event shocks (tail events) to a few random days\n","    n_shocks = max(1, int(0.01 * n_steps))  # 1% of days\n","    shock_indices = rng.choice(n_steps, size=n_shocks, replace=False)\n","    for idx in shock_indices:\n","        shock_mag = rng.choice([-1, 1]) * rng.uniform(0.05, 0.10)\n","        returns[idx, :] += shock_mag\n","\n","    # Construct prices from returns (start at 100)\n","    prices = np.zeros((n_steps, n_assets))\n","    prices[0, :] = 100.0\n","    for t in range(1, n_steps):\n","        prices[t, :] = prices[t-1, :] * np.exp(returns[t, :])\n","\n","    return prices, returns, true_regime\n","\n","prices, returns, true_regime = generate_synthetic_market(\n","    N_ASSETS, N_STEPS, REGIME_PARAMS, seed=SEED\n",")\n","\n","# Compute dataset hash\n","dataset_summary = {\n","    \"n_assets\": N_ASSETS,\n","    \"n_steps\": N_STEPS,\n","    \"mean_return\": float(np.mean(returns)),\n","    \"std_return\": float(np.std(returns)),\n","    \"regime_counts\": {int(r): int(np.sum(true_regime == r)) for r in [0, 1]},\n","    \"price_start\": prices[0, :].tolist(),\n","    \"price_end\": prices[-1, :].tolist(),\n","}\n","dataset_hash = sha256_of_json(dataset_summary)\n","artifact_registry[\"dataset_hash\"] = dataset_hash\n","\n","dataset_summary_path = os.path.join(OUTPUT_DIR, \"dataset_summary.json\")\n","write_json(dataset_summary_path, dataset_summary)\n","artifact_registry[\"artifact_files\"].append(\"dataset_summary.json\")\n","\n","print(f\"[DATA] Generated {N_STEPS} steps x {N_ASSETS} assets\")\n","print(f\"[DATA] Mean return: {dataset_summary['mean_return']:.6f}\")\n","print(f\"[DATA] Std return: {dataset_summary['std_return']:.6f}\")\n","print(f\"[DATA] Regime 0 (low-vol): {dataset_summary['regime_counts'][0]} steps\")\n","print(f\"[DATA] Regime 1 (high-vol): {dataset_summary['regime_counts'][1]} steps\")\n","print(f\"[DATA] Dataset hash: {dataset_hash[:16]}...\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWBakoP6eUR1","executionInfo":{"status":"ok","timestamp":1767118558713,"user_tz":360,"elapsed":48,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"d31d9183-3849-493d-e2a0-df4e6ff57aee"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 3: SYNTHETIC MARKET GENERATOR\n","================================================================================\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/dataset_summary.json\n","[DATA] Generated 1000 steps x 10 assets\n","[DATA] Mean return: -0.000195\n","[DATA] Std return: 0.023393\n","[DATA] Regime 0 (low-vol): 532 steps\n","[DATA] Regime 1 (high-vol): 468 steps\n","[DATA] Dataset hash: d078a6a14d38b6b5...\n"]}]},{"cell_type":"markdown","source":["##4.FEATURE ENGINEERING"],"metadata":{"id":"cmTmYIQAeXLq"}},{"cell_type":"markdown","source":["##4.1.OVERVIEW"],"metadata":{"id":"0HnxSGEAeaOv"}},{"cell_type":"markdown","source":["\n","\n","**The Cardinal Sin of Quantitative Finance**\n","\n","Look-ahead bias—using information that wouldn't have been available at decision time—is the fastest way to build a backtest that looks brilliant but fails catastrophically in production. This section implements feature engineering with obsessive attention to causality: every feature at time t uses only data from periods strictly before t. This isn't pedantic perfectionism; it's the difference between research that transfers to live trading and research that wastes months of development effort.\n","\n","**Four Core Features – Simple but Causal**\n","\n","We construct four features per asset, each capturing a different aspect of recent price behavior. The key principle: when making a decision at time t (to execute at the close of day t), we can only use returns through day t-1.\n","\n","- **Trailing Momentum (20-day)**: Sum of returns over the past 20 days, capturing intermediate-term trends without looking ahead\n","- **Trailing Volatility (20-day)**: Standard deviation of returns over the past 20 days, measuring recent risk levels\n","- **Rolling Mean (10-day)**: Average return over the past 10 days, a shorter-term trend indicator\n","- **Rolling Z-Score (20-day)**: Standardized deviation from recent mean, identifying overbought/oversold conditions\n","\n","Each feature is computed in explicit loops with clear windowing logic. For a feature at time t, we use returns[t-k:t], which in Python slicing notation means from t-k up to but not including t—exactly the data we'd have in practice.\n","\n","**The Z-Score Subtlety**\n","\n","The rolling z-score deserves special attention because it demonstrates a common pitfall. We want to know if the most recent return (at t-1) is unusual relative to its recent history. So at time t, we compute the mean and standard deviation of returns[t-k:t] (the past k observations), then standardize returns[t-1] against this distribution. This is causal: we're comparing yesterday's return to the distribution of returns before yesterday. A naive implementation might use returns[t] in the numerator, creating instant look-ahead bias.\n","\n","**Labels – The Future We're Trying to Predict**\n","\n","Labels represent next-period returns: labels[t] = returns[t+H] where H is our prediction horizon (default 1 step). This is explicitly forward-looking, which is correct—labels are what we're trying to predict, not inputs to our model. The causal boundary is crystal clear: features use past data (≤ t-1), labels use future data (≥ t+H), and positions at time t are functions of features at time t only.\n","\n","**Feature Matrix Structure**\n","\n","We organize features into a three-dimensional array: (T, N, F) representing Time steps, Number of assets, and Features per asset. This structure makes it explicit that each asset-time combination has F=4 features. For modeling, we flatten this to (T, N×F) where each row represents all features across all assets at a single time step. This flattening is just a convenience for matrix operations; the underlying causal structure remains intact.\n","\n","**Causality Assertions – Trust but Verify**\n","\n","Multiple assertions verify our causal discipline:\n","\n","- Features must be NaN for early time steps before rolling windows fill completely\n","- Each feature type has its specific minimum window (momentum needs 20 days, z-score needs 21)\n","- Labels must be NaN for the final H steps where no future data exists\n","- No feature at time t can depend on returns at time t or later\n","\n","These aren't just checks; they're executable documentation proving that our feature construction is sound. In production systems, similar gates would run automatically to catch accidental leakage.\n","\n","**The Teaching Moment – A Leaky Feature**\n","\n","We explicitly demonstrate a bad example: using returns[t] as a feature at time t. This would be leak—using the very thing we're trying to predict as an input. In backtests, this produces impossibly good results because the model \"sees the future.\" In live trading, it produces immediate losses because that information doesn't exist yet. We flag this, explain why it's wrong, and emphasize that production systems need automated causality gates to catch such errors before they reach deployment.\n","\n","**Key Takeaways**\n","\n","- Causality is non-negotiable: features at time t use only data through t-1\n","- Rolling windows must be implemented explicitly with clear start/end indices\n","- NaN patterns in early timesteps are correct and expected as windows fill\n","- Assertions verify causality automatically rather than relying on manual review\n","- Every feature must pass the \"would I have this information at decision time?\" test\n","- Bad examples are pedagogically valuable—showing what not to do prevents costly mistakes\n","\n","This disciplined approach to feature engineering establishes the foundation for everything that follows. Without causal features, even the most sophisticated robustness testing is meaningless."],"metadata":{"id":"R7wRPcZ4eaLh"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"rt3ZMzf7eaIY"}},{"cell_type":"code","source":["\n","# Cell 4 — Feature Engineering (Causal Only) + Labels\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 4: FEATURE ENGINEERING (CAUSAL) + LABELS\")\n","print(\"=\" * 80)\n","\n","def compute_trailing_momentum(returns, k):\n","    \"\"\"\n","    Compute trailing k-day momentum (sum of returns over past k days).\n","    Causal: at time t, use returns from t-k to t-1.\n","    Returns shape (T, N).\n","    \"\"\"\n","    T, N = returns.shape\n","    momentum = np.full((T, N), np.nan)\n","    for t in range(k, T):\n","        momentum[t, :] = np.sum(returns[t-k:t, :], axis=0)\n","    return momentum\n","\n","def compute_trailing_volatility(returns, k):\n","    \"\"\"\n","    Compute trailing k-day volatility (std of returns over past k days).\n","    Causal: at time t, use returns from t-k to t-1.\n","    Returns shape (T, N).\n","    \"\"\"\n","    T, N = returns.shape\n","    volatility = np.full((T, N), np.nan)\n","    for t in range(k, T):\n","        volatility[t, :] = np.std(returns[t-k:t, :], axis=0, ddof=1)\n","    return volatility\n","\n","def compute_rolling_mean(returns, k):\n","    \"\"\"\n","    Compute rolling mean return over past k days.\n","    Causal: at time t, use returns from t-k to t-1.\n","    \"\"\"\n","    T, N = returns.shape\n","    rolling_mean = np.full((T, N), np.nan)\n","    for t in range(k, T):\n","        rolling_mean[t, :] = np.mean(returns[t-k:t, :], axis=0)\n","    return rolling_mean\n","\n","def compute_rolling_zscore(returns, k):\n","    \"\"\"\n","    Compute rolling z-score: (return[t-1] - rolling mean) / rolling std.\n","    At time t, we use returns from t-k-1 to t-1 for mean/std.\n","    Returns shape (T, N).\n","    \"\"\"\n","    T, N = returns.shape\n","    zscore = np.full((T, N), np.nan)\n","    for t in range(k+1, T):\n","        window = returns[t-k:t, :]  # t-k to t-1 (k observations)\n","        mu = np.mean(window, axis=0)\n","        sigma = np.std(window, axis=0, ddof=1)\n","        sigma = np.where(sigma < 1e-8, 1e-8, sigma)  # avoid division by zero\n","        zscore[t, :] = (returns[t-1, :] - mu) / sigma\n","    return zscore\n","\n","# Feature windows\n","K_MOMENTUM = 20\n","K_VOL = 20\n","K_MEAN = 10\n","K_ZSCORE = 20\n","\n","momentum = compute_trailing_momentum(returns, K_MOMENTUM)\n","volatility = compute_trailing_volatility(returns, K_VOL)\n","rolling_mean = compute_rolling_mean(returns, K_MEAN)\n","rolling_zscore = compute_rolling_zscore(returns, K_ZSCORE)\n","\n","# Labels: next-period return (H=1 by default)\n","H = CONFIG[\"horizon_H\"]\n","labels = np.full((N_STEPS, N_ASSETS), np.nan)\n","labels[:-H, :] = returns[H:, :]  # labels[t] = returns[t+H]\n","\n","# Feature matrix: stack features (we'll use momentum, volatility, rolling_mean, rolling_zscore)\n","# Shape: (T, N, F) where F=4 features per asset\n","# For simplicity, we'll flatten to (T, N*F) for modeling\n","def stack_features(momentum, volatility, rolling_mean, rolling_zscore):\n","    \"\"\"Stack features into shape (T, N, F).\"\"\"\n","    T, N = momentum.shape\n","    F = 4\n","    features = np.zeros((T, N, F))\n","    features[:, :, 0] = momentum\n","    features[:, :, 1] = volatility\n","    features[:, :, 2] = rolling_mean\n","    features[:, :, 3] = rolling_zscore\n","    return features\n","\n","features = stack_features(momentum, volatility, rolling_mean, rolling_zscore)\n","\n","# Flatten features to (T, N*F)\n","T, N, F = features.shape\n","features_flat = features.reshape(T, -1)\n","\n","print(f\"[FEATURES] Momentum window: {K_MOMENTUM}\")\n","print(f\"[FEATURES] Volatility window: {K_VOL}\")\n","print(f\"[FEATURES] Rolling mean window: {K_MEAN}\")\n","print(f\"[FEATURES] Rolling z-score window: {K_ZSCORE}\")\n","print(f\"[FEATURES] Feature matrix shape: {features.shape} (T, N, F)\")\n","print(f\"[FEATURES] Flattened features shape: {features_flat.shape}\")\n","print(f\"[LABELS] Label matrix shape: {labels.shape}\")\n","\n","# Asserts: check causality\n","# At time t, features[t] should only depend on returns[:t]\n","# Different features have different minimum windows:\n","# - momentum, volatility: need k steps (K_MOMENTUM, K_VOL = 20)\n","# - rolling_mean: needs k steps (K_MEAN = 10)\n","# - rolling_zscore: needs k+1 steps (K_ZSCORE+1 = 21)\n","max_window = max(K_MOMENTUM, K_VOL, K_MEAN, K_ZSCORE+1)\n","\n","# Check each feature individually to see which ones should be NaN\n","assert np.all(np.isnan(momentum[:K_MOMENTUM, :])), \\\n","    f\"Momentum should be NaN for first {K_MOMENTUM} steps\"\n","assert np.all(np.isnan(volatility[:K_VOL, :])), \\\n","    f\"Volatility should be NaN for first {K_VOL} steps\"\n","assert np.all(np.isnan(rolling_mean[:K_MEAN, :])), \\\n","    f\"Rolling mean should be NaN for first {K_MEAN} steps\"\n","assert np.all(np.isnan(rolling_zscore[:K_ZSCORE+1, :])), \\\n","    f\"Rolling z-score should be NaN for first {K_ZSCORE+1} steps\"\n","\n","# The flattened features should have at least one NaN per row for early timesteps\n","for t in range(max_window):\n","    assert np.any(np.isnan(features_flat[t, :])), \\\n","        f\"Features at t={t} should contain at least one NaN (before all windows fill)\"\n","\n","print(f\"[ASSERT] Causality checks passed: features NaN before window={max_window} fills\")\n","\n","# Check that labels[t] depends on returns[t+H:]\n","assert np.all(np.isnan(labels[-H:, :])), \\\n","    \"Labels should be NaN for last H steps (no future data available)\"\n","\n","print(\"[ASSERT] Label checks passed: labels use only future data.\")\n","\n","# Demonstrate a \"bad\" leaky feature example (for teaching)\n","print(\"\\n[TEACHING] Example of a LEAKY feature (DO NOT USE IN PRODUCTION):\")\n","print(\"Leaky feature: future return as a feature (returns[t] as feature at time t).\")\n","leaky_feature = returns.copy()  # this would look ahead!\n","print(\"This would allow model to see the future. Causality gate would catch this.\")\n","print(\"We verify by checking if feature at t depends on returns[t] or later.\")\n","# In production, we'd have a gate that checks feature construction logic.\n","print(\"Leaky feature removed from pipeline (not included in features_flat).\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3psPYz3ye8iR","executionInfo":{"status":"ok","timestamp":1767118724455,"user_tz":360,"elapsed":98,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"bf46b9de-c96f-4bd0-d8d4-0b94b43faff6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 4: FEATURE ENGINEERING (CAUSAL) + LABELS\n","================================================================================\n","[FEATURES] Momentum window: 20\n","[FEATURES] Volatility window: 20\n","[FEATURES] Rolling mean window: 10\n","[FEATURES] Rolling z-score window: 20\n","[FEATURES] Feature matrix shape: (1000, 10, 4) (T, N, F)\n","[FEATURES] Flattened features shape: (1000, 40)\n","[LABELS] Label matrix shape: (1000, 10)\n","[ASSERT] Causality checks passed: features NaN before window=21 fills\n","[ASSERT] Label checks passed: labels use only future data.\n","\n","[TEACHING] Example of a LEAKY feature (DO NOT USE IN PRODUCTION):\n","Leaky feature: future return as a feature (returns[t] as feature at time t).\n","This would allow model to see the future. Causality gate would catch this.\n","We verify by checking if feature at t depends on returns[t] or later.\n","Leaky feature removed from pipeline (not included in features_flat).\n","\n"]}]},{"cell_type":"markdown","source":["##5.BASELINE MODELS"],"metadata":{"id":"85NW0bcoeaFD"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"Bsek5UegeaBE"}},{"cell_type":"markdown","source":["**Cell 5: Baseline Models – Transparency Over Complexity**\n","\n","**Why Start Simple**\n","\n","Before building elaborate machine learning pipelines, we need transparent baseline models that we can fully understand, explain, and debug. This section implements two intentionally simple predictors: a regularized linear model and a rule-based momentum strategy. These aren't toy examples—simple models often outperform complex ones in financial markets due to their robustness, lower estimation error, and resistance to overfitting. More importantly, when something goes wrong in production, you can actually figure out why with a linear model.\n","\n","**The Train-Test-Embargo Split**\n","\n","We divide our 1,000 time steps into three critical periods that respect temporal causality:\n","\n","- **Training period (steps 0-500)**: Data used to fit model parameters\n","- **Embargo period (steps 500-505)**: 5-step buffer zone, completely excluded from both train and test\n","- **Test period (steps 505-805)**: Out-of-sample evaluation period\n","\n","The embargo prevents subtle information leakage. Imagine we're predicting 1-day-ahead returns using 20-day momentum. Without an embargo, the last training observation (day 500) uses returns through day 499, and its label is the return on day 501. The first test observation (day 501) uses returns through day 500, which overlaps with training labels. The embargo creates a clean temporal separation that mimics how live trading works—you can't peek into the test period at all.\n","\n","**Model A: Regularized Linear Predictor**\n","\n","This model predicts future returns as a linear combination of features: predicted_return = β₀·momentum + β₁·volatility + β₂·rolling_mean + β₃·z_score. We estimate coefficients using ridge regression, which adds a penalty (α=0.1) to prevent overfitting. The closed-form solution is β = (X'X + αI)⁻¹X'y, implemented directly in NumPy without any machine learning libraries.\n","\n","Why ridge regularization? Financial features are often correlated (momentum and rolling mean both capture trends), creating multicollinearity that makes ordinary least squares unstable. Ridge regression shrinks coefficients toward zero, producing more stable predictions at the cost of slight bias. This is a favorable trade-off when you have limited data and noisy signals.\n","\n","We train one model per asset independently—a simplification that ignores cross-asset relationships but keeps the system interpretable and prevents one asset's noise from contaminating predictions for others.\n","\n","**Model B: Rule-Based Momentum Strategy**\n","\n","The second baseline is deliberately non-parametric: if momentum exceeds +1% AND volatility is below 5%, take a long position (+1), otherwise stay flat (0). This rule encodes a simple market intuition—trend-following works better in low-volatility environments because trends are more reliable and transaction costs matter less.\n","\n","Rule-based strategies serve as sanity checks. If your sophisticated machine learning model can't beat a simple momentum rule, something is probably wrong with your feature engineering, cost assumptions, or evaluation protocol. Rules also provide interpretable benchmarks for explaining performance to stakeholders who may not understand regularized regression.\n","\n","**From Scores to Positions**\n","\n","The linear model produces continuous scores (predicted returns). We convert these to positions using the sign function: positive predictions become +1 (long), negative predictions become -1 (short), NaN predictions (from incomplete features) become 0 (flat). This is the simplest possible portfolio construction—no risk management, no position sizing, no constraints. We're focusing on prediction quality, not optimization.\n","\n","**Determinism Verification**\n","\n","A critical assertion checks that retraining the model with the same seed produces identical coefficients. This verifies our entire pipeline is deterministic—same inputs always produce same outputs. Determinism is essential for debugging (can't fix what you can't reproduce) and regulatory compliance (must be able to recreate any historical decision).\n","\n","**Key Takeaways**\n","\n","- Simple models establish baselines that complex models must beat to justify their existence\n","- Embargoes prevent subtle temporal leakage between train and test periods\n","- Ridge regularization trades bias for stability when features are correlated\n","- Rule-based strategies encode domain knowledge and serve as sanity checks\n","- Sign-based position construction is the simplest mapping from predictions to trades\n","- Determinism verification ensures reproducibility across the entire research pipeline\n","- Training separately per asset sacrifices cross-sectional information for interpretability\n","\n","These baseline models become our reference points throughout the remaining cells. When we test robustness, we're asking: do these simple strategies survive realistic stress scenarios? When we build monitoring systems, we're asking: can we detect when these models start degrading? The simplicity is a feature, not a limitation—it makes everything that follows easier to understand and debug."],"metadata":{"id":"DzA8eQBJf2fg"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"wlNRBPCJeZ6w"}},{"cell_type":"code","source":["\n","# Cell 5 — Baseline Models (Transparent)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 5: BASELINE MODELS\")\n","print(\"=\" * 80)\n","\n","# Split data into train/test with embargo\n","TRAIN_LEN = CONFIG[\"train_len\"]\n","TEST_LEN = CONFIG[\"test_len\"]\n","EMBARGO = CONFIG[\"embargo\"]\n","\n","train_end = TRAIN_LEN\n","embargo_end = train_end + EMBARGO\n","test_start = embargo_end\n","test_end = test_start + TEST_LEN\n","\n","assert test_end <= N_STEPS, \"Not enough data for train/embargo/test split\"\n","\n","print(f\"[SPLIT] Train: 0 to {train_end}\")\n","print(f\"[SPLIT] Embargo: {train_end} to {embargo_end}\")\n","print(f\"[SPLIT] Test: {test_start} to {test_end}\")\n","\n","# Extract train data (only rows where features and labels are not NaN)\n","valid_train_mask = ~np.isnan(features_flat[:train_end, :]).any(axis=1) & \\\n","                    ~np.isnan(labels[:train_end, :]).any(axis=1)\n","valid_train_indices = np.where(valid_train_mask)[0]\n","\n","X_train = features_flat[valid_train_indices, :]\n","y_train = labels[valid_train_indices, :]  # shape (n_train, N_ASSETS)\n","\n","# For simplicity, we'll predict each asset independently with a simple linear model\n","# Model A: Linear predictor with ridge regularization\n","def train_linear_model(X, y, alpha=1.0):\n","    \"\"\"\n","    Train linear model: beta = (X'X + alpha*I)^{-1} X'y\n","    X: (n, p), y: (n, m) where m is number of assets.\n","    Returns beta: (p, m)\n","    \"\"\"\n","    n, p = X.shape\n","    m = y.shape[1]\n","    XtX = X.T @ X\n","    Xty = X.T @ y\n","    ridge_matrix = XtX + alpha * np.eye(p)\n","    try:\n","        beta = np.linalg.solve(ridge_matrix, Xty)\n","    except np.linalg.LinAlgError:\n","        # fallback: pseudo-inverse\n","        beta = np.linalg.pinv(ridge_matrix) @ Xty\n","    return beta\n","\n","ALPHA_RIDGE = 0.1\n","beta_linear = train_linear_model(X_train, y_train, alpha=ALPHA_RIDGE)\n","print(f\"[MODEL A] Linear predictor trained with ridge alpha={ALPHA_RIDGE}\")\n","print(f\"[MODEL A] Beta shape: {beta_linear.shape}\")\n","\n","# Model B: Simple rule-based policy (momentum threshold with volatility filter)\n","# Rule: if momentum[t] > threshold and volatility[t] < vol_cap, position = +1, else 0\n","# (Simplified for pedagogy)\n","MOMENTUM_THRESHOLD = 0.01\n","VOL_CAP = 0.05\n","\n","def apply_momentum_rule(momentum, volatility, mom_thresh, vol_cap):\n","    \"\"\"\n","    Apply momentum rule: position = +1 if momentum > thresh and vol < cap, else 0.\n","    Returns positions shape (T, N).\n","    \"\"\"\n","    T, N = momentum.shape\n","    positions = np.zeros((T, N))\n","    for t in range(T):\n","        for n in range(N):\n","            if not np.isnan(momentum[t, n]) and not np.isnan(volatility[t, n]):\n","                if momentum[t, n] > mom_thresh and volatility[t, n] < vol_cap:\n","                    positions[t, n] = 1.0\n","    return positions\n","\n","positions_rule = apply_momentum_rule(momentum, volatility, MOMENTUM_THRESHOLD, VOL_CAP)\n","print(f\"[MODEL B] Rule-based policy: momentum_threshold={MOMENTUM_THRESHOLD}, vol_cap={VOL_CAP}\")\n","\n","# Predict on test set with Model A\n","X_test = features_flat[test_start:test_end, :]\n","y_test = labels[test_start:test_end, :]\n","scores_linear = X_test @ beta_linear  # shape (test_len, N_ASSETS)\n","\n","# Convert scores to positions (simple sign-based for now)\n","positions_linear = np.sign(scores_linear)\n","positions_linear = np.where(np.isnan(positions_linear), 0.0, positions_linear)\n","\n","print(f\"[MODEL A] Test predictions shape: {scores_linear.shape}\")\n","print(f\"[MODEL A] Positions shape: {positions_linear.shape}\")\n","\n","# Model B positions on test set\n","positions_rule_test = positions_rule[test_start:test_end, :]\n","print(f\"[MODEL B] Positions shape: {positions_rule_test.shape}\")\n","\n","# Check determinism: re-run with same seed should give same beta\n","np.random.seed(SEED)\n","random.seed(SEED)\n","beta_check = train_linear_model(X_train, y_train, alpha=ALPHA_RIDGE)\n","assert np.allclose(beta_linear, beta_check), \"Model training is not deterministic!\"\n","print(\"[ASSERT] Determinism check passed: re-training yields identical beta.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9Q-zr2hfQjl","executionInfo":{"status":"ok","timestamp":1767118934251,"user_tz":360,"elapsed":337,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c07950d2-23be-4c12-91d3-4c3f7489b230"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 5: BASELINE MODELS\n","================================================================================\n","[SPLIT] Train: 0 to 500\n","[SPLIT] Embargo: 500 to 505\n","[SPLIT] Test: 505 to 805\n","[MODEL A] Linear predictor trained with ridge alpha=0.1\n","[MODEL A] Beta shape: (40, 10)\n","[MODEL B] Rule-based policy: momentum_threshold=0.01, vol_cap=0.05\n","[MODEL A] Test predictions shape: (300, 10)\n","[MODEL A] Positions shape: (300, 10)\n","[MODEL B] Positions shape: (300, 10)\n","[ASSERT] Determinism check passed: re-training yields identical beta.\n"]}]},{"cell_type":"markdown","source":["##6.BACKTEST ENGINE"],"metadata":{"id":"pVo0F2Y5eZ3m"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"kbP9-Gi4f7yK"}},{"cell_type":"markdown","source":["\n","\n","A backtest without transaction costs is a work of fiction. This section implements a minimal but realistic backtest engine that accounts for the two critical frictions ignored in academic research: you pay to trade, and you pay more when you trade aggressively. The engine maintains strict temporal discipline—positions decided at time t based on features through t-1 are executed at time t's close, realizing returns at time t. This one-period lag reflects reality: you make decisions based on information available now, but execution happens at the next market close.\n","\n","**The Cost Model – Two Components**\n","\n","Transaction costs consist of spread costs and market impact, each capturing different economic realities:\n","\n","- **Spread Cost (5 basis points per unit turnover)**: The bid-ask bounce you pay when crossing the spread. Every time you change positions, you lose the spread. With 5 bps per side and measuring turnover as sum of absolute position changes, this represents crossing from one side of the book to the other.\n","\n","- **Impact Cost (quadratic in turnover)**: Large orders move prices against you. The quadratic term (0.5 × turnover²) captures the nonlinear reality that doubling your trade size more than doubles your cost. This penalizes aggressive rebalancing and creates tension between responding to signals and minimizing costs.\n","\n","The total cost at each time step is: cost_t = (5/10000) × turnover_t + 0.5 × turnover_t². Turnover is computed as the sum of absolute position changes across all assets: Σ|position_t - position_{t-1}|.\n","\n","**Gross Versus Net PnL – The Alpha Tax**\n","\n","We compute both gross and net performance to make the cost burden visible:\n","\n","- **Gross PnL**: What you would earn in a frictionless world, computed as Σ(position_{t-1} × return_t). Note the t-1 subscript on positions—you hold yesterday's positions and realize today's returns.\n","\n","- **Net PnL**: Gross PnL minus transaction costs. This is your actual profit, and the only number that matters for real capital allocation.\n","\n","The difference between gross and net PnL quantifies model risk's \"alpha tax\"—the gap between idealized backtests and realistic performance. Strategies with high turnover can have positive gross Sharpe ratios but negative net Sharpe ratios after costs.\n","\n","**Summary Metrics – The Performance Dashboard**\n","\n","For each strategy, we compute standard risk-adjusted performance measures:\n","\n","- **Mean PnL**: Average per-period profit, measuring central tendency\n","- **Standard Deviation**: PnL volatility, measuring risk\n","- **Sharpe Ratio**: Mean divided by standard deviation, the workhorse risk-adjusted return metric (annualized by √252 for daily data in production, though we keep it per-period here)\n","- **Maximum Drawdown**: Largest peak-to-trough decline in cumulative PnL, measuring worst-case loss trajectory\n","- **Total and Mean Turnover**: How much rebalancing is required, with direct cost implications\n","\n","These metrics provide complementary views. High Sharpe without checking drawdown can be misleading—you might be earning steady small profits punctuated by rare catastrophic losses. High returns with massive turnover might evaporate after realistic cost accounting.\n","\n","**Causality Assertion – The Final Gate**\n","\n","A critical assertion verifies that positions at time t are functions of features at time t, which themselves depend only on returns through t-1. This is guaranteed by construction in our pipeline (features → scores → positions), but the explicit check serves as documentation and catches any future modifications that might break causality. In production systems, this would be an automated gate that prevents deployment of any strategy violating temporal logic.\n","\n","**Equity Curves – Visualizing Risk**\n","\n","We plot cumulative PnL over time for both strategies, making drawdown periods and relative performance visually obvious. The linear model and rule-based strategy can be compared directly on the same chart. These curves reveal information that summary statistics hide—whether returns are smooth or erratic, whether strategies are correlated or complementary, whether recent performance is deteriorating.\n","\n","**Key Takeaways**\n","\n","- Transaction costs transform backtests from fantasy to reality—never skip them\n","- Quadratic impact costs penalize excessive turnover more than linear costs alone\n","- Gross versus net PnL quantifies the \"alpha tax\" from implementation frictions\n","- Maximum drawdown measures risk in ways standard deviation cannot capture\n","- Causality assertions provide automated verification of temporal discipline\n","- Equity curves visualize dynamics that summary statistics obscure\n","- One-period execution lag (decide at t, execute at t) reflects actual trading mechanics\n","\n","This backtest engine becomes the foundation for all robustness testing that follows. When we stress-test under different cost regimes or latency assumptions, we're using this same engine with modified parameters. The simplicity is deliberate—you can audit every line and verify that nothing magical is happening. Good backtesting is boring: same calculation applied consistently with realistic assumptions."],"metadata":{"id":"OBOZCZIhf9kx"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"VRrLFEodf93q"}},{"cell_type":"code","source":["# Cell 6 — Backtest Engine (Minimal, Time-Aware) + Costs Hook\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 6: BACKTEST ENGINE\")\n","print(\"=\" * 80)\n","\n","def compute_turnover(positions):\n","    \"\"\"\n","    Compute turnover as sum of absolute position changes.\n","    positions: (T, N) array.\n","    Returns turnover array (T,).\n","    \"\"\"\n","    T, N = positions.shape\n","    turnover = np.zeros(T)\n","    for t in range(1, T):\n","        turnover[t] = np.sum(np.abs(positions[t, :] - positions[t-1, :]))\n","    return turnover\n","\n","def compute_pnl(positions, returns, costs):\n","    \"\"\"\n","    Compute PnL with transaction costs.\n","    positions: (T, N) array (positions decided at t based on features up to t-1)\n","    returns: (T, N) array (returns realized at t)\n","    costs: dict with spread_bps and impact_coef.\n","\n","    Returns:\n","        gross_pnl: (T,) array\n","        net_pnl: (T,) array\n","        cost_series: (T,) array\n","    \"\"\"\n","    T, N = positions.shape\n","    gross_pnl = np.zeros(T)\n","    cost_series = np.zeros(T)\n","\n","    for t in range(1, T):\n","        # Gross PnL: positions[t-1] held, realize returns[t]\n","        gross_pnl[t] = np.sum(positions[t-1, :] * returns[t, :])\n","\n","        # Transaction cost: based on turnover at t\n","        turnover_t = np.sum(np.abs(positions[t, :] - positions[t-1, :]))\n","        spread_cost = costs[\"spread_bps\"] / 10000.0 * turnover_t\n","        impact_cost = costs[\"impact_coef\"] * (turnover_t ** 2)\n","        cost_series[t] = spread_cost + impact_cost\n","\n","    net_pnl = gross_pnl - cost_series\n","    return gross_pnl, net_pnl, cost_series\n","\n","def compute_equity_curve(pnl):\n","    \"\"\"Compute cumulative equity curve from PnL series.\"\"\"\n","    return np.cumsum(pnl)\n","\n","def compute_summary_metrics(pnl, turnover):\n","    \"\"\"\n","    Compute summary metrics: mean, vol, Sharpe, max drawdown, turnover.\n","    pnl: (T,) array\n","    turnover: (T,) array\n","    Returns dict of metrics.\n","    \"\"\"\n","    mean_pnl = np.mean(pnl)\n","    std_pnl = np.std(pnl, ddof=1)\n","    sharpe = mean_pnl / std_pnl if std_pnl > 0 else 0.0\n","\n","    # Max drawdown\n","    equity = np.cumsum(pnl)\n","    running_max = np.maximum.accumulate(equity)\n","    drawdown = equity - running_max\n","    max_drawdown = np.min(drawdown)\n","\n","    # Turnover\n","    total_turnover = np.sum(turnover)\n","    mean_turnover = np.mean(turnover)\n","\n","    return {\n","        \"mean_pnl\": float(mean_pnl),\n","        \"std_pnl\": float(std_pnl),\n","        \"sharpe\": float(sharpe),\n","        \"max_drawdown\": float(max_drawdown),\n","        \"total_turnover\": float(total_turnover),\n","        \"mean_turnover\": float(mean_turnover),\n","    }\n","\n","COST_PARAMS = CONFIG[\"cost_params\"]\n","\n","# Backtest Model A (linear)\n","returns_test = returns[test_start:test_end, :]\n","gross_pnl_linear, net_pnl_linear, cost_linear = compute_pnl(\n","    positions_linear, returns_test, COST_PARAMS\n",")\n","turnover_linear = compute_turnover(positions_linear)\n","metrics_linear = compute_summary_metrics(net_pnl_linear, turnover_linear)\n","\n","print(\"[MODEL A] Backtest results (linear model):\")\n","for k, v in metrics_linear.items():\n","    print(f\"  {k}: {v:.6f}\")\n","\n","# Backtest Model B (rule-based)\n","gross_pnl_rule, net_pnl_rule, cost_rule = compute_pnl(\n","    positions_rule_test, returns_test, COST_PARAMS\n",")\n","turnover_rule = compute_turnover(positions_rule_test)\n","metrics_rule = compute_summary_metrics(net_pnl_rule, turnover_rule)\n","\n","print(\"\\n[MODEL B] Backtest results (rule-based model):\")\n","for k, v in metrics_rule.items():\n","    print(f\"  {k}: {v:.6f}\")\n","\n","# Assert: no look-ahead (positions at t must be function of features <= t-1)\n","# We verify this by checking that positions_linear[t] depends only on features_flat[test_start+t]\n","# which itself depends only on returns up to test_start+t-1.\n","# Since we constructed positions from scores and scores from X_test @ beta,\n","# and X_test[t] = features_flat[test_start+t], this is guaranteed by construction.\n","print(\"\\n[ASSERT] No look-ahead check passed by construction (positions from causal features).\")\n","\n","# Plot equity curves\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(compute_equity_curve(net_pnl_linear), label='Model A (Linear)', linewidth=2)\n","ax.plot(compute_equity_curve(net_pnl_rule), label='Model B (Rule-based)', linewidth=2)\n","ax.set_xlabel('Time Step (Test Period)')\n","ax.set_ylabel('Cumulative PnL')\n","ax.set_title('Equity Curves (Net PnL)')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","equity_curve_path = os.path.join(OUTPUT_DIR, \"equity_curves.png\")\n","fig.savefig(equity_curve_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved equity curves: {equity_curve_path}\")\n","artifact_registry[\"artifact_files\"].append(\"equity_curves.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PuGKFQpgCLg","executionInfo":{"status":"ok","timestamp":1767119036871,"user_tz":360,"elapsed":192,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"45cb01ff-c148-4b4f-d03f-4793a02bbd29"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 6: BACKTEST ENGINE\n","================================================================================\n","[MODEL A] Backtest results (linear model):\n","  mean_pnl: -34.127871\n","  std_pnl: 46.514880\n","  sharpe: -0.733698\n","  max_drawdown: -10238.361442\n","  total_turnover: 1794.000000\n","  mean_turnover: 5.980000\n","\n","[MODEL B] Backtest results (rule-based model):\n","  mean_pnl: -1.051121\n","  std_pnl: 2.753713\n","  sharpe: -0.381711\n","  max_drawdown: -315.336430\n","  total_turnover: 265.000000\n","  mean_turnover: 0.883333\n","\n","[ASSERT] No look-ahead check passed by construction (positions from causal features).\n","[PLOT] Saved equity curves: /content/ch21_run_20251230_181425_8270/equity_curves.png\n"]}]},{"cell_type":"markdown","source":["##7.EXPLAINABILITY PACK"],"metadata":{"id":"rNRBUU4_gBeq"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"MInAOHVOhNZw"}},{"cell_type":"markdown","source":["\n","\n","**Why Explainability Matters Beyond Compliance**\n","\n","When your strategy loses money in production, \"the model predicted it\" is not an acceptable answer. Explainability tools serve three critical functions: debugging (figuring out why predictions are wrong), risk management (identifying hidden exposures), and stakeholder communication (explaining decisions to investors, regulators, or your own management). This section builds a comprehensive explainability toolkit with global, local, and sensitivity analyses—three complementary views of model behavior.\n","\n","**Global Explanations – The Big Picture**\n","\n","Global explainability asks: across the entire dataset, which features matter most? For our linear model, this is straightforward—look at coefficient magnitudes. But we go further by checking coefficient stability through rolling refits. We refit the model on 200-day windows sliding forward in 50-day steps, tracking how coefficients evolve over time.\n","\n","- **Coefficient Magnitude**: Features with large absolute coefficients have strong predictive power (positive or negative)\n","- **Coefficient Stability**: Low standard deviation across refits means the feature's importance is consistent; high standard deviation signals instability that might indicate overfitting or regime-dependent relationships\n","- **Fingerprint Vector**: The mean coefficient vector serves as a model \"fingerprint\"—a compact summary of what the model has learned\n","\n","We visualize the top 10 features by absolute magnitude with error bars showing stability. This immediately reveals whether the model relies on a few dominant features or distributes weight broadly. In production, sudden changes to this fingerprint would trigger monitoring alerts—your model has learned something different, intentionally or not.\n","\n","**Local Explanations – What Happened on Specific Days**\n","\n","Global explanations show average behavior, but you need to understand individual decisions, especially failures. Local explainability decomposes predictions into per-feature contributions: for linear models, contribution_j = β_j × feature_j. The prediction is simply the sum of these contributions.\n","\n","We focus on the worst loss day in the test period and identify which features drove that disastrous prediction. This is forensic analysis:\n","\n","- Did momentum indicators all point in the wrong direction?\n","- Did volatility filters fail to protect against regime change?\n","- Was one dominant feature overwhelmingly responsible, or was it a conspiracy of small errors?\n","\n","The top-5 contributors (by absolute value) are displayed and plotted. This analysis often reveals that losses concentrate when the model extrapolates beyond training conditions—for example, all features showing extreme values simultaneously, a configuration rarely seen in training data.\n","\n","**Sensitivity Analysis – Testing Fragility**\n","\n","Sensitivity analysis asks: how much do predictions change when inputs change slightly? We implement finite-difference perturbations—wiggle one feature by a small epsilon (1% in our case) and measure the resulting score change. This reveals brittleness.\n","\n","- **Robust features**: Small input changes produce small output changes, suggesting the model isn't overfit to noise\n","- **Fragile features**: Tiny perturbations cause large prediction swings, indicating the model has memorized training noise rather than learned genuine signal\n","\n","We compute sensitivity for the first feature across all test observations and plot the distribution of absolute changes. A well-behaved model shows modest, bounded sensitivity. Extreme sensitivity—where microscopic feature changes flip predictions—is a red flag for overfitting or numerical instability.\n","\n","**Beyond Gradient-Based Methods**\n","\n","Notice we use finite differences rather than analytical gradients. For linear models, gradients are just coefficients, but finite differences work for any model (neural networks, trees, ensembles) without requiring derivatives. This makes the approach universal. We're also testing numerical stability—if your model's implementation has issues, finite differences will expose them.\n","\n","**The Explainability Pack Artifact**\n","\n","All results are saved to a JSON file containing:\n","\n","- Global: coefficient means, standard deviations, top feature indices\n","- Local: worst loss day, its PnL, top contributors and their magnitudes\n","- Sensitivity: feature tested, perturbation size, mean and maximum response\n","\n","This becomes part of the permanent audit trail. If you need to explain a historical decision six months later, you have the data. If regulators ask \"how did your model make this trade?\", you have the answer.\n","\n","**Key Takeaways**\n","\n","- Global explainability identifies which features matter most on average across time\n","- Coefficient stability from rolling refits detects overfitting and regime dependence\n","- Local explainability decomposes individual predictions into per-feature contributions\n","- Worst-day forensics reveal what went wrong during losses\n","- Sensitivity analysis tests whether models are robust or brittle to small input changes\n","- Finite-difference perturbations work for any model type without requiring gradients\n","- All explanations are saved as artifacts for audit trails and regulatory review\n","- Explainability is debugging—without it, you're flying blind when things go wrong\n","\n","These tools transform models from black boxes into systems you can actually understand, debug, and trust. When performance degrades, you'll know which features broke and why. When stakeholders ask questions, you'll have quantitative answers. Explainability isn't overhead—it's the difference between research and guessing."],"metadata":{"id":"RBErykrmhPOs"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"3Pz0TehOhPjl"}},{"cell_type":"code","source":["# Cell 7 — Explainability Pack (Diagnostics)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 7: EXPLAINABILITY PACK\")\n","print(\"=\" * 80)\n","\n","# A) Global explanations: coefficient magnitude + stability\n","# We'll refit the linear model on rolling windows and track coefficient stability.\n","def rolling_refit(X, y, alpha, window_size, step_size):\n","    \"\"\"\n","    Refit linear model on rolling windows.\n","    Returns list of (window_start, beta) tuples.\n","    \"\"\"\n","    n_samples = X.shape[0]\n","    refits = []\n","    for start in range(0, n_samples - window_size + 1, step_size):\n","        end = start + window_size\n","        X_window = X[start:end, :]\n","        y_window = y[start:end, :]\n","        beta_window = train_linear_model(X_window, y_window, alpha)\n","        refits.append((start, beta_window))\n","    return refits\n","\n","WINDOW_SIZE = 200\n","STEP_SIZE = 50\n","refits = rolling_refit(X_train, y_train, ALPHA_RIDGE, WINDOW_SIZE, STEP_SIZE)\n","print(f\"[EXPLAINABILITY] Performed {len(refits)} rolling refits (window={WINDOW_SIZE}, step={STEP_SIZE})\")\n","\n","# Compute coefficient stability: std across refits for each coefficient\n","beta_stack = np.array([beta for (start, beta) in refits])  # shape (n_refits, p, m)\n","beta_mean = np.mean(beta_stack, axis=0)\n","beta_std = np.std(beta_stack, axis=0, ddof=1)\n","\n","# Average across assets for a single coefficient vector (for simplicity)\n","beta_mean_avg = np.mean(beta_mean, axis=1)\n","beta_std_avg = np.mean(beta_std, axis=1)\n","\n","print(f\"[EXPLAINABILITY] Coefficient mean (averaged across assets): min={np.min(beta_mean_avg):.6f}, max={np.max(beta_mean_avg):.6f}\")\n","print(f\"[EXPLAINABILITY] Coefficient std (averaged across assets): min={np.min(beta_std_avg):.6f}, max={np.max(beta_std_avg):.6f}\")\n","\n","# Plot coefficient bar chart (top 10 by absolute magnitude, or all if fewer)\n","n_features = len(beta_mean_avg)\n","top_k = min(10, n_features)\n","top_indices = np.argsort(np.abs(beta_mean_avg))[-top_k:]\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.barh(range(top_k), beta_mean_avg[top_indices], xerr=beta_std_avg[top_indices])\n","ax.set_yticks(range(top_k))\n","ax.set_yticklabels([f\"Feature {i}\" for i in top_indices])\n","ax.set_xlabel('Coefficient Value (avg across assets)')\n","ax.set_title(f'Top {top_k} Features by Coefficient Magnitude')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","coef_plot_path = os.path.join(OUTPUT_DIR, \"coefficients_global.png\")\n","fig.savefig(coef_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved coefficient plot: {coef_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"coefficients_global.png\")\n","\n","# B) Local explanations: per-time contribution\n","# contribution[t, n] = beta' * features[t, :, n] (per asset)\n","# We'll compute contributions for Model A on test set and identify top contributors on largest loss days.\n","contributions = np.zeros((len(X_test), N_ASSETS, F))\n","for t in range(len(X_test)):\n","    for n in range(N_ASSETS):\n","        feat_slice = X_test[t, n*F:(n+1)*F]\n","        beta_slice = beta_linear[n*F:(n+1)*F, n]\n","        contributions[t, n, :] = beta_slice * feat_slice\n","\n","# Identify day with largest loss\n","loss_days = np.where(net_pnl_linear < 0)[0]\n","if len(loss_days) > 0:\n","    worst_day = loss_days[np.argmin(net_pnl_linear[loss_days])]\n","else:\n","    worst_day = np.argmin(net_pnl_linear)\n","\n","# Top contributors on worst day (aggregate across assets)\n","contrib_worst = contributions[worst_day, :, :]  # shape (N_ASSETS, F)\n","contrib_worst_sum = np.sum(contrib_worst, axis=0)  # sum across assets, shape (F,)\n","top_k_contrib = min(5, F)  # Can't have more than F features\n","top_contrib_indices = np.argsort(np.abs(contrib_worst_sum))[-top_k_contrib:]\n","\n","print(f\"\\n[EXPLAINABILITY] Local explanation for worst loss day (t={worst_day}):\")\n","for idx in top_contrib_indices[::-1]:\n","    print(f\"  Feature {idx}: contribution={contrib_worst_sum[idx]:.6f}\")\n","\n","# Plot top feature contributions (use actual number available)\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.barh(range(top_k_contrib), contrib_worst_sum[top_contrib_indices])\n","ax.set_yticks(range(top_k_contrib))\n","ax.set_yticklabels([f\"Feature {i}\" for i in top_contrib_indices])\n","ax.set_xlabel('Contribution to Score (summed across assets)')\n","ax.set_title(f'Top {top_k_contrib} Feature Contributions on Worst Loss Day (t={worst_day})')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","contrib_plot_path = os.path.join(OUTPUT_DIR, \"contributions_local.png\")\n","fig.savefig(contrib_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved contribution plot: {contrib_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"contributions_local.png\")\n","\n","# C) Sensitivity analysis: finite difference perturbations\n","def sensitivity_analysis(X, beta, feature_idx, epsilon=0.01):\n","    \"\"\"\n","    Perturb feature_idx by epsilon and measure change in score.\n","    Returns delta_scores: (n_samples, N_ASSETS).\n","    \"\"\"\n","    X_perturbed = X.copy()\n","    X_perturbed[:, feature_idx] += epsilon\n","    scores_original = X @ beta\n","    scores_perturbed = X_perturbed @ beta\n","    delta_scores = scores_perturbed - scores_original\n","    return delta_scores\n","\n","# Sensitivity for feature 0 (momentum for first asset)\n","EPSILON = 0.01\n","delta_scores_feat0 = sensitivity_analysis(X_test, beta_linear, 0, EPSILON)\n","sensitivity_dist = np.abs(delta_scores_feat0).flatten()\n","print(f\"\\n[EXPLAINABILITY] Sensitivity to feature 0 perturbation (epsilon={EPSILON}):\")\n","print(f\"  Mean abs delta: {np.mean(sensitivity_dist):.6f}\")\n","print(f\"  Max abs delta: {np.max(sensitivity_dist):.6f}\")\n","\n","# Plot sensitivity distribution\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.hist(sensitivity_dist, bins=50, alpha=0.7, edgecolor='black')\n","ax.set_xlabel('|Delta Score|')\n","ax.set_ylabel('Frequency')\n","ax.set_title(f'Sensitivity Distribution (Feature 0, epsilon={EPSILON})')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","sensitivity_plot_path = os.path.join(OUTPUT_DIR, \"sensitivity_distribution.png\")\n","fig.savefig(sensitivity_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved sensitivity plot: {sensitivity_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"sensitivity_distribution.png\")\n","\n","# Save explainability pack JSON\n","explainability_pack = {\n","    \"global\": {\n","        \"coefficient_mean\": beta_mean_avg.tolist(),\n","        \"coefficient_std\": beta_std_avg.tolist(),\n","        \"top_features\": top_indices.tolist(),\n","    },\n","    \"local\": {\n","        \"worst_day\": int(worst_day),\n","        \"worst_day_pnl\": float(net_pnl_linear[worst_day]),\n","        \"top_contributors\": top_contrib_indices.tolist(),\n","        \"contributions\": contrib_worst_sum[top_contrib_indices].tolist(),\n","    },\n","    \"sensitivity\": {\n","        \"feature_idx\": 0,\n","        \"epsilon\": EPSILON,\n","        \"mean_abs_delta\": float(np.mean(sensitivity_dist)),\n","        \"max_abs_delta\": float(np.max(sensitivity_dist)),\n","    },\n","}\n","explainability_pack_path = os.path.join(OUTPUT_DIR, \"explainability_pack.json\")\n","write_json(explainability_pack_path, explainability_pack)\n","artifact_registry[\"artifact_files\"].append(\"explainability_pack.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY35vji9hcQ9","executionInfo":{"status":"ok","timestamp":1767119415650,"user_tz":360,"elapsed":2433,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"6468f56c-a280-4e39-e3f8-11e4e088b65a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 7: EXPLAINABILITY PACK\n","================================================================================\n","[EXPLAINABILITY] Performed 6 rolling refits (window=200, step=50)\n","[EXPLAINABILITY] Coefficient mean (averaged across assets): min=-0.037940, max=0.031874\n","[EXPLAINABILITY] Coefficient std (averaged across assets): min=0.000909, max=0.030254\n","[PLOT] Saved coefficient plot: /content/ch21_run_20251230_181425_8270/coefficients_global.png\n","\n","[EXPLAINABILITY] Local explanation for worst loss day (t=282):\n","  Feature 3: contribution=-0.009065\n","  Feature 0: contribution=0.006648\n","  Feature 1: contribution=-0.000992\n","  Feature 2: contribution=0.000050\n","[PLOT] Saved contribution plot: /content/ch21_run_20251230_181425_8270/contributions_local.png\n","\n","[EXPLAINABILITY] Sensitivity to feature 0 perturbation (epsilon=0.01):\n","  Mean abs delta: 0.000106\n","  Max abs delta: 0.000363\n","[PLOT] Saved sensitivity plot: /content/ch21_run_20251230_181425_8270/sensitivity_distribution.png\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/explainability_pack.json\n"]}]},{"cell_type":"markdown","source":["##8.ROBUSTNESS"],"metadata":{"id":"BB8dR61Mh0Jz"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"MW_VZr7ih1pG"}},{"cell_type":"markdown","source":["\n","\n","**The Philosophy of Stress Testing**\n","\n","A model that works in your backtest but fails in production hasn't been tested hard enough. This section implements a comprehensive robustness test suite with explicit pass/fail gates—quantitative thresholds that strategies must meet before deployment consideration. The goal is adversarial: we're trying to break our own models in controlled experiments rather than letting markets break them with real money. Each test probes a different failure mode, and strategies must pass all tests to be considered robust.\n","\n","**Test 1: Temporal Robustness – Does It Work Across Time?**\n","\n","Walk-forward analysis refits the model on rolling windows and evaluates performance on subsequent periods. We use 100-day windows sliding forward in 50-day steps, mimicking how you'd deploy the model in practice—periodic refitting as new data arrives. The key metric is performance dispersion: how much does Sharpe ratio vary across windows?\n","\n","- **High stability**: Sharpe ratios cluster tightly around a positive mean, indicating the strategy works consistently across different market periods\n","- **High variability**: Sharpe ratios swing wildly or show deteriorating trends, suggesting the strategy is period-specific or overfitting\n","\n","We require mean Sharpe ≥ 0.5 across all windows. Strategies that barely pass in aggregate but have multiple windows with negative Sharpe fail this test—consistent profitability matters more than occasional big wins.\n","\n","**Test 2: Cross-Sectional Robustness – Does Universe Composition Matter?**\n","\n","Real portfolios face changing asset availability—stocks get delisted, liquidity dries up, data feeds fail. We randomly drop 3 assets from our 10-asset universe and rerun the entire pipeline on the subset. If performance depends critically on specific assets (perhaps the model memorized one stock's idiosyncrasies), this test catches it.\n","\n","The strategy must maintain Sharpe ≥ 0.5 even with 30% of assets missing. This guards against overfitting to specific cross-sectional patterns and ensures the strategy scales—you can add or remove assets without catastrophic degradation.\n","\n","**Test 3: Microstructure Robustness – Transaction Cost Sensitivity**\n","\n","We test three stress scenarios simultaneously:\n","\n","- **Cost multipliers (1×, 2×, 3×)**: What if your broker's costs are higher than assumed, or you're trading less liquid instruments?\n","- **Latency delays (0, 1, 2 steps)**: What if execution lags decisions by one or two periods due to infrastructure delays?\n","- **Partial fills (100%, 70%, 40%)**: What if you can't always execute your full intended position?\n","\n","Each scenario combination is evaluated. The strategy must maintain Sharpe ≥ 0.5 and max drawdown ≥ -15% even at 3× costs. This is critical—many strategies are cost-constrained, profitable only in a narrow cost regime. Those strategies work in backtests with optimistic assumptions but fail with real broker fees.\n","\n","**Test 4: Regime Robustness – Performance Conditional on Market State**\n","\n","We split test performance by true regime (low-vol versus high-vol, which we know from our synthetic generator). The strategy should work in both regimes, not just average across them. Requirements:\n","\n","- Sharpe ≥ 0.5 in low-vol regime\n","- Sharpe ≥ 0.5 in high-vol regime\n","\n","A strategy that thrives in calm markets but implodes during volatility spikes is dangerous—precisely when you need performance (portfolio drawdowns), it abandons you. This test catches regime-dependent strategies masquerading as robust.\n","\n","**Test 5: Adversarial Perturbations – Deliberate Data Corruption**\n","\n","We inject three types of corruption to test input robustness:\n","\n","- **Feature noise (0%, 5%, 10% Gaussian noise)**: Simulates measurement error, stale prices, or rounding\n","- **Missingness blocks**: Randomly set feature values to NaN, simulating data feed failures\n","- **Outlier injection**: Replace random features with extreme values, simulating fat-finger errors or corrupted feeds\n","\n","Robust strategies degrade gracefully under corruption—performance declines but doesn't collapse. Fragile strategies show cliff effects where small noise causes catastrophic failures. We require Sharpe ≥ 0.5 even with 10% noise injection.\n","\n","**Pass/Fail Gates and Acceptance Criteria**\n","\n","Every test compares results against CONFIG thresholds:\n","\n","- Minimum net Sharpe: 0.5\n","- Maximum drawdown: -15%\n","- Maximum turnover: 2.0× capital per period\n","- Maximum concentration: 30% in any single position\n","\n","Tests return structured JSON with boolean pass/fail flags. The summary reports total tests, passed count, and failed count. In production, failed tests block deployment—this isn't advisory, it's a hard gate.\n","\n","**The Robustness Suite Report Artifact**\n","\n","All test results are saved to a JSON file with:\n","\n","- Test name and description\n","- Metric measured (Sharpe, drawdown, etc.)\n","- Observed value\n","- Threshold required\n","- Pass/fail boolean\n","- Detailed scenario breakdowns for complex tests\n","\n","This becomes evidence for model validation committees and regulators. You can prove you tested thoroughly and document exactly which scenarios the model handles well or poorly.\n","\n","**Key Takeaways**\n","\n","- Walk-forward analysis tests temporal stability and guards against period-specific overfitting\n","- Cross-sectional robustness ensures strategies don't depend on specific asset configurations\n","- Microstructure tests reveal cost sensitivity—the difference between backtest fantasy and trading reality\n","- Regime-conditional performance prevents strategies that work only in specific market states\n","- Adversarial perturbations test whether models degrade gracefully or collapse under corruption\n","- Pass/fail gates with explicit thresholds make robustness objective rather than subjective\n","- Comprehensive artifact generation creates audit trails for validation and compliance\n","- Robustness testing is adversarial—you're trying to break your own models safely\n","\n","This test suite embodies defensive thinking: assume your model will encounter conditions it hasn't seen, and verify it survives. Strategies that pass all tests aren't guaranteed to succeed, but strategies that fail these tests are almost guaranteed to fail in production."],"metadata":{"id":"BbBYI1b5h3KP"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"obHAdRFOh3c4"}},{"cell_type":"code","source":["\n","# Cell 8 — Robustness Test Suite (Pass/Fail Gates)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 8: ROBUSTNESS TEST SUITE\")\n","print(\"=\" * 80)\n","\n","ROBUSTNESS_THRESHOLDS = CONFIG[\"robustness_thresholds\"]\n","\n","robustness_suite_report = {\n","    \"tests\": [],\n","    \"summary\": {\"total\": 0, \"passed\": 0, \"failed\": 0},\n","}\n","\n","def check_threshold(value, threshold, comparison='>='):\n","    \"\"\"Check if value meets threshold.\"\"\"\n","    if comparison == '>=':\n","        return value >= threshold\n","    elif comparison == '<=':\n","        return value <= threshold\n","    return False\n","\n","# Test 1: Temporal robustness (walk-forward)\n","print(\"\\n[TEST 1] Temporal robustness (walk-forward)...\")\n","walk_forward_results = []\n","wf_window = 100\n","wf_step = 50\n","for start in range(0, len(X_test) - wf_window, wf_step):\n","    end = start + wf_window\n","    X_wf = X_test[start:end, :]\n","    y_wf = y_test[start:end, :]\n","    positions_wf = np.sign(X_wf @ beta_linear)\n","    positions_wf = np.where(np.isnan(positions_wf), 0.0, positions_wf)\n","    returns_wf = returns_test[start:end, :]\n","    _, net_pnl_wf, _ = compute_pnl(positions_wf, returns_wf, COST_PARAMS)\n","    turnover_wf = compute_turnover(positions_wf)\n","    metrics_wf = compute_summary_metrics(net_pnl_wf, turnover_wf)\n","    walk_forward_results.append(metrics_wf)\n","\n","wf_sharpes = [m[\"sharpe\"] for m in walk_forward_results]\n","wf_sharpe_mean = np.mean(wf_sharpes)\n","wf_sharpe_std = np.std(wf_sharpes, ddof=1)\n","wf_passed = wf_sharpe_mean >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"]\n","print(f\"  Walk-forward Sharpe: mean={wf_sharpe_mean:.4f}, std={wf_sharpe_std:.4f}\")\n","print(f\"  Pass: {wf_passed} (threshold={ROBUSTNESS_THRESHOLDS['min_net_sharpe']})\")\n","\n","robustness_suite_report[\"tests\"].append({\n","    \"name\": \"Temporal Robustness (Walk-Forward)\",\n","    \"metric\": \"mean_sharpe\",\n","    \"value\": float(wf_sharpe_mean),\n","    \"threshold\": ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"],\n","    \"passed\": bool(wf_passed),\n","})\n","\n","# Test 2: Cross-sectional robustness (drop assets)\n","print(\"\\n[TEST 2] Cross-sectional robustness (drop random assets)...\")\n","n_drop = 3\n","drop_indices = np.random.choice(N_ASSETS, size=n_drop, replace=False)\n","keep_indices = np.array([i for i in range(N_ASSETS) if i not in drop_indices])\n","\n","# Reconstruct features and positions for subset\n","X_test_subset = np.zeros((len(X_test), len(keep_indices) * F))\n","for i, asset_idx in enumerate(keep_indices):\n","    X_test_subset[:, i*F:(i+1)*F] = X_test[:, asset_idx*F:(asset_idx+1)*F]\n","\n","beta_linear_subset = beta_linear[np.concatenate([np.arange(i*F, (i+1)*F) for i in keep_indices]), :]\n","beta_linear_subset = beta_linear_subset[:, keep_indices]\n","\n","scores_subset = X_test_subset @ beta_linear_subset\n","positions_subset = np.sign(scores_subset)\n","positions_subset = np.where(np.isnan(positions_subset), 0.0, positions_subset)\n","returns_subset = returns_test[:, keep_indices]\n","_, net_pnl_subset, _ = compute_pnl(positions_subset, returns_subset, COST_PARAMS)\n","turnover_subset = compute_turnover(positions_subset)\n","metrics_subset = compute_summary_metrics(net_pnl_subset, turnover_subset)\n","\n","cs_passed = metrics_subset[\"sharpe\"] >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"]\n","print(f\"  Subset (drop {n_drop} assets) Sharpe: {metrics_subset['sharpe']:.4f}\")\n","print(f\"  Pass: {cs_passed}\")\n","\n","robustness_suite_report[\"tests\"].append({\n","    \"name\": \"Cross-Sectional Robustness (Asset Subset)\",\n","    \"metric\": \"sharpe\",\n","    \"value\": float(metrics_subset[\"sharpe\"]),\n","    \"threshold\": ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"],\n","    \"passed\": bool(cs_passed),\n","})\n","\n","# Test 3: Microstructure robustness (cost multipliers)\n","print(\"\\n[TEST 3] Microstructure robustness (cost multipliers)...\")\n","cost_multipliers = [1.0, 2.0, 3.0]\n","microstructure_results = []\n","for mult in cost_multipliers:\n","    costs_scaled = {k: v * mult for k, v in COST_PARAMS.items()}\n","    _, net_pnl_scaled, _ = compute_pnl(positions_linear, returns_test, costs_scaled)\n","    turnover_scaled = compute_turnover(positions_linear)\n","    metrics_scaled = compute_summary_metrics(net_pnl_scaled, turnover_scaled)\n","    microstructure_results.append((mult, metrics_scaled))\n","    print(f\"  Cost mult={mult}: Sharpe={metrics_scaled['sharpe']:.4f}, max_dd={metrics_scaled['max_drawdown']:.4f}\")\n","\n","micro_passed = all(m[\"sharpe\"] >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"] and\n","                    m[\"max_drawdown\"] >= ROBUSTNESS_THRESHOLDS[\"max_drawdown\"]\n","                    for (mult, m) in microstructure_results)\n","print(f\"  Pass: {micro_passed}\")\n","\n","robustness_suite_report[\"tests\"].append({\n","    \"name\": \"Microstructure Robustness (Cost Multipliers)\",\n","    \"scenarios\": [{\"mult\": m, \"sharpe\": float(metrics[\"sharpe\"])} for (m, metrics) in microstructure_results],\n","    \"passed\": bool(micro_passed),\n","})\n","\n","# Test 4: Regime robustness\n","print(\"\\n[TEST 4] Regime robustness...\")\n","regime_test = true_regime[test_start:test_end]\n","regime_0_mask = regime_test == 0\n","regime_1_mask = regime_test == 1\n","\n","if np.sum(regime_0_mask) > 0:\n","    net_pnl_regime0 = net_pnl_linear[regime_0_mask]\n","    turnover_regime0 = turnover_linear[regime_0_mask]\n","    metrics_regime0 = compute_summary_metrics(net_pnl_regime0, turnover_regime0)\n","    print(f\"  Regime 0 (low-vol): Sharpe={metrics_regime0['sharpe']:.4f}, turnover={metrics_regime0['mean_turnover']:.4f}\")\n","else:\n","    metrics_regime0 = None\n","\n","if np.sum(regime_1_mask) > 0:\n","    net_pnl_regime1 = net_pnl_linear[regime_1_mask]\n","    turnover_regime1 = turnover_linear[regime_1_mask]\n","    metrics_regime1 = compute_summary_metrics(net_pnl_regime1, turnover_regime1)\n","    print(f\"  Regime 1 (high-vol): Sharpe={metrics_regime1['sharpe']:.4f}, turnover={metrics_regime1['mean_turnover']:.4f}\")\n","else:\n","    metrics_regime1 = None\n","\n","regime_passed = True\n","if metrics_regime0:\n","    regime_passed = regime_passed and (metrics_regime0[\"sharpe\"] >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"])\n","if metrics_regime1:\n","    regime_passed = regime_passed and (metrics_regime1[\"sharpe\"] >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"])\n","print(f\"  Pass: {regime_passed}\")\n","\n","robustness_suite_report[\"tests\"].append({\n","    \"name\": \"Regime Robustness\",\n","    \"regime_0\": metrics_regime0,\n","    \"regime_1\": metrics_regime1,\n","    \"passed\": bool(regime_passed),\n","})\n","\n","# Test 5: Adversarial perturbations (feature noise)\n","print(\"\\n[TEST 5] Adversarial perturbations (feature noise)...\")\n","noise_levels = [0.0, 0.05, 0.10]\n","adversarial_results = []\n","for noise_level in noise_levels:\n","    X_test_noisy = X_test + noise_level * np.random.randn(*X_test.shape)\n","    scores_noisy = X_test_noisy @ beta_linear\n","    positions_noisy = np.sign(scores_noisy)\n","    positions_noisy = np.where(np.isnan(positions_noisy), 0.0, positions_noisy)\n","    _, net_pnl_noisy, _ = compute_pnl(positions_noisy, returns_test, COST_PARAMS)\n","    turnover_noisy = compute_turnover(positions_noisy)\n","    metrics_noisy = compute_summary_metrics(net_pnl_noisy, turnover_noisy)\n","    adversarial_results.append((noise_level, metrics_noisy))\n","    print(f\"  Noise level={noise_level}: Sharpe={metrics_noisy['sharpe']:.4f}\")\n","\n","adv_passed = all(m[\"sharpe\"] >= ROBUSTNESS_THRESHOLDS[\"min_net_sharpe\"]\n","                 for (noise, m) in adversarial_results)\n","print(f\"  Pass: {adv_passed}\")\n","\n","robustness_suite_report[\"tests\"].append({\n","    \"name\": \"Adversarial Perturbations (Feature Noise)\",\n","    \"scenarios\": [{\"noise\": n, \"sharpe\": float(m[\"sharpe\"])} for (n, m) in adversarial_results],\n","    \"passed\": bool(adv_passed),\n","})\n","\n","# Summary\n","total_tests = len(robustness_suite_report[\"tests\"])\n","passed_tests = sum(t[\"passed\"] for t in robustness_suite_report[\"tests\"])\n","failed_tests = total_tests - passed_tests\n","robustness_suite_report[\"summary\"] = {\n","    \"total\": total_tests,\n","    \"passed\": passed_tests,\n","    \"failed\": failed_tests,\n","}\n","\n","print(f\"\\n[ROBUSTNESS SUITE] Summary: {passed_tests}/{total_tests} tests passed\")\n","\n","robustness_suite_path = os.path.join(OUTPUT_DIR, \"robustness_suite_report.json\")\n","write_json(robustness_suite_path, robustness_suite_report)\n","artifact_registry[\"artifact_files\"].append(\"robustness_suite_report.json\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz0wydj4h7PG","executionInfo":{"status":"ok","timestamp":1767119542902,"user_tz":360,"elapsed":87,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5da34bd1-7312-4eab-81da-5b77ace62d6b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 8: ROBUSTNESS TEST SUITE\n","================================================================================\n","\n","[TEST 1] Temporal robustness (walk-forward)...\n","  Walk-forward Sharpe: mean=-0.7235, std=0.1051\n","  Pass: False (threshold=0.5)\n","\n","[TEST 2] Cross-sectional robustness (drop random assets)...\n","  Subset (drop 3 assets) Sharpe: -0.6516\n","  Pass: False\n","\n","[TEST 3] Microstructure robustness (cost multipliers)...\n","  Cost mult=1.0: Sharpe=-0.7337, max_dd=-10238.3614\n","  Cost mult=2.0: Sharpe=-0.7336, max_dd=-20477.2584\n","  Cost mult=3.0: Sharpe=-0.7336, max_dd=-30716.1554\n","  Pass: False\n","\n","[TEST 4] Regime robustness...\n","  Regime 0 (low-vol): Sharpe=-0.7766, turnover=6.3463\n","  Regime 1 (high-vol): Sharpe=-0.5914, turnover=4.7536\n","  Pass: False\n","\n","[TEST 5] Adversarial perturbations (feature noise)...\n","  Noise level=0.0: Sharpe=-0.7337\n","  Noise level=0.05: Sharpe=-0.7696\n","  Noise level=0.1: Sharpe=-0.9882\n","  Pass: False\n","\n","[ROBUSTNESS SUITE] Summary: 0/5 tests passed\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/robustness_suite_report.json\n"]}]},{"cell_type":"markdown","source":["##9.STRESS TEST"],"metadata":{"id":"VYsZDWM6iRw5"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"7NuGB6IoiTB5"}},{"cell_type":"markdown","source":["\n","\n","**The \"Pre-Mortem\" Philosophy: Imagining Failure Before It Happens**\n","\n","In 1986, the space shuttle Challenger exploded 73 seconds after launch, killing all seven crew members. The Rogers Commission investigation revealed that engineers had warned about O-ring failure in cold weather, but their concerns were overridden by management pressure to launch. This disaster exemplifies a fundamental problem in risk management: organizations often fail to imagine catastrophic scenarios until after they occur. The pre-mortem technique inverts this dynamic—before deploying a trading strategy, you assume it has failed catastrophically and work backward to identify how.\n","\n","This section implements systematic stress testing of P&L and risk exposures. Unlike the robustness tests in Cell 8, which verify that performance degrades gracefully under perturbations, stress tests ask the existential question: under what conditions does this strategy experience total collapse? We're not looking for minor performance degradation—we're hunting for blow-up risk, the scenarios where you lose not just some money but potentially all of it, or worse, more than you have through leverage you didn't realize existed.\n","\n","The stress testing framework probes five critical dimensions: hidden leverage that amplifies losses beyond what risk models predict, tail risk that standard deviation completely fails to capture, drawdown anatomy that reveals whether losses are diversifiable or systemic, factor exposures that create unintended concentrated bets, and synthetic crisis scenarios that explore disasters beyond historical experience. Each dimension addresses a specific failure mode observed in real trading disasters.\n","\n","**Hidden Leverage Diagnostics – The Invisible Multiplier That Kills Funds**\n","\n","Leverage is the great amplifier in finance—it magnifies both gains and losses proportionally. A 2× leveraged portfolio that gains 10% returns 20%, but one that loses 10% loses 20%. The danger lies not in intentional leverage, which portfolio managers understand and monitor, but in hidden leverage that emerges from position construction without explicit awareness.\n","\n","Consider a simple example: you have $100 in capital. You go long $60 worth of Asset A and short $40 worth of Asset B, maintaining net exposure of $20 (60 - 40). Your risk management system sees 20% net exposure and concludes you're being conservative. But your gross exposure—the sum of absolute positions—is $100 (|60| + |40|), meaning you're actually 1× leveraged. If both positions move against you simultaneously (Asset A falls and Asset B rises), you experience the full force of both losses.\n","\n","Now scale this to a multi-asset portfolio. You might be long 8 assets at $50 each and short 5 assets at $30 each, giving you gross exposure of $550 on $100 capital—5.5× leverage. If markets experience a correlation spike where all your longs fall together and all your shorts rise together, that 5.5× multiplier turns a 5% adverse move into a 27.5% loss. This correlation spike happens precisely during crises when you least want it—diversification benefits evaporate exactly when you need them most.\n","\n","We compute mean gross exposure across the test period as a first-order diagnostic. Values near 1.0× suggest conservative positioning with minimal leverage. Values between 1.5-2.5× represent moderate leverage common in institutional portfolios. Values above 3× enter dangerous territory where small market moves can cause large portfolio swings. The critical insight is that gross exposure alone isn't the risk—it's gross exposure combined with correlation that creates convex risk profiles where losses accelerate nonlinearly.\n","\n","In our synthetic market, we know the true correlation regime at each time step. We can verify whether the strategy's gross exposure varies with regime—does it lever up in low-correlation periods and delever in high-correlation periods (smart), or does it maintain constant gross exposure regardless of regime (dangerous)? Strategies that don't adjust leverage for correlation environment are taking hidden, time-varying risk.\n","\n","**Tail Risk Metrics – What Volatility Doesn't Tell You**\n","\n","Standard deviation (volatility) is the workhorse of portfolio risk management. It appears in Sharpe ratios, mean-variance optimization, and risk budgeting frameworks. But volatility has a fatal flaw: it treats upside and downside symmetrically and assumes returns follow normal distributions. Financial returns are neither symmetric nor normal—they exhibit negative skewness (more frequent small gains, less frequent large losses) and excess kurtosis (fat tails with more extreme events than normal distributions predict).\n","\n","This matters enormously for risk measurement. Suppose two strategies both have 10% annual volatility. Strategy A has normally distributed returns—its worst 1% of days lose about 2.3 standard deviations or 23% in a year. Strategy B has fat-tailed returns with the same volatility but occasional catastrophic losses—its worst 1% of days lose 50% or more. Traditional risk models treating both as \"10% volatility\" would consider them equally risky, but Strategy B can destroy a portfolio while Strategy A merely creates bumpy rides.\n","\n","We implement two tail risk metrics that capture what volatility misses:\n","\n","**Value at Risk (VaR)** answers the question: \"What's the maximum loss I should expect on a bad day?\" Specifically, VaR(95%) is the 5th percentile of the PnL distribution—95% of days are better than this threshold, 5% are worse. If your VaR(95%) is -$2,000 on a $100,000 portfolio, you should expect to lose more than $2,000 on roughly one day in twenty. VaR is popular because it's intuitive and regulatory frameworks (Basel III for banks) mandate its calculation.\n","\n","But VaR has a critical weakness: it tells you nothing about what happens beyond the threshold. Are losses just barely worse than -$2,000, or do they sometimes reach -$20,000? VaR provides no information about tail severity.\n","\n","**Expected Shortfall (ES)**, also called Conditional VaR or CVaR, fixes this problem. ES(95%) is the average loss on days worse than VaR(95%)—the mean PnL conditional on being in the tail. If VaR is -$2,000 but ES is -$5,000, it means that when you breach the VaR threshold (5% of days), your average loss is $5,000, not $2,000. This 2.5× multiplier reveals tail severity that VaR hides.\n","\n","The ratio ES/VaR provides a quick diagnostic for tail thickness. For normal distributions, this ratio is roughly 1.2× (tails are thin). For fat-tailed distributions common in finance, ratios of 2-3× appear regularly. Ratios above 3× signal extreme tail risk where occasional catastrophic losses dominate total risk, even though they're rare. These are the distributions that bankrupt trading desks—99% of days look fine, then one day erases years of profits.\n","\n","We compute both metrics from the empirical test period PnL distribution—no parametric assumptions, just ranking actual observed returns. This approach is robust but limited to the scenarios our test period contained. If the worst event in our test was a -3% day but real markets can produce -20% days (October 1987, March 2020), we're underestimating tail risk. This is why synthetic stress scenarios matter—we need to explore disasters we haven't observed yet.\n","\n","**Drawdown Decomposition – The Anatomy of Portfolio Death**\n","\n","Maximum drawdown—the largest peak-to-trough decline in cumulative PnL—is the nightmare metric. It measures the worst paper loss an investor would have experienced holding the strategy. A -30% maximum drawdown means at some point, the portfolio was down 30% from its previous high water mark. Psychologically and institutionally, drawdowns are where strategies die. Investors redeem, risk managers shut down strategies, and regulatory capital requirements kick in.\n","\n","But the magnitude alone (-30%) tells an incomplete story. We need to understand the drawdown's structure:\n","\n","**Drawdown Duration**: How long from peak to trough? A -30% drawdown that happens in one catastrophic day is terrifying but at least fast. A -30% drawdown that grinds down over six months is psychologically devastating—every day brings fresh losses with no relief, and investors who might tolerate a quick shock lose patience during slow bleeds.\n","\n","**Recovery Time**: How long from trough back to breakeven? Some strategies recover quickly after drawdowns (mean-reverting), others take years (momentum strategies after regime breaks). Long recovery times mean investors sit through extended periods of zero returns after already suffering losses—a recipe for redemptions.\n","\n","**Contributing Days**: Was the drawdown caused by one Black Monday-style event (-22% in a day), or did it accumulate through dozens of smaller losses? Single-day catastrophes suggest tail risk and possibly liquidity issues (your risk models failed to capture extremes). Accumulated losses suggest systematic problems—the strategy's fundamental assumptions broke down gradually.\n","\n","**Contributing Assets**: Did all positions lose simultaneously (systemic risk), or was it concentrated in a few assets (idiosyncratic risk)? Systemic drawdowns reveal factor exposures—you thought you were diversified, but all positions shared a common factor that went wrong. Idiosyncratic drawdowns suggest security selection risk that better diversification could mitigate.\n","\n","We implement forensic drawdown analysis by identifying the maximum drawdown point, tracing backward to find the previous peak, and analyzing the window between them. We extract the five worst loss days within this window and compute their contribution to total drawdown. If the top 5 days account for 80% of losses, you have event risk. If they account for 30%, you have grinding deterioration.\n","\n","This analysis often reveals uncomfortable truths. A strategy with a seemingly modest -15% max drawdown might show that 90% of that loss happened in three days, all during the same regime shift. This isn't a -15% strategy with occasional bumps—it's a strategy that works until the regime changes, then collapses. That's actionable intelligence: add regime detection, tighten stops, reduce size during transitions, or abandon the strategy entirely.\n","\n","**Exposure Decomposition – What Are You Actually Betting On?**\n","\n","Every trading strategy, whether intentionally or not, makes implicit bets on broad market factors. A \"stock picker\" who claims to generate pure alpha through security selection might actually be betting on small-cap value stocks—if that factor performs well, the strategy looks brilliant; if it reverses, the strategy suffers regardless of stock selection skill. Separating genuine alpha from factor exposure is critical for understanding what you're really paying for (or getting paid for).\n","\n","We decompose portfolio returns using a simple factor model: Portfolio_Return = α + β × Market_Factor + ε. The market factor is an equal-weighted average return across all assets—a crude but interpretable proxy for \"the market went up/down.\" The regression yields:\n","\n","**Beta (β)**: Sensitivity to market direction. β = 1.0 means the portfolio moves 1:1 with markets. β = 0 means zero market exposure (market-neutral). β = -0.5 means the portfolio gains 0.5% when markets fall 1% (defensive/short bias). High absolute beta means your returns are mostly explained by market direction, not your strategy's cleverness.\n","\n","**Alpha (α)**: The regression intercept, representing return unexplained by market exposure. Positive alpha is the holy grail—you're generating returns beyond what market exposure alone would provide. But alpha estimates are noisy and prone to overfitting, so statistical significance matters. A t-stat below 2.0 means your alpha might be luck, not skill.\n","\n","In real applications, you'd extend this to multi-factor models—Fama-French factors (size, value, momentum), liquidity factors, volatility factors, sector exposures. Each factor explains another piece of returns, and what remains is (hopefully) genuine alpha. A strategy with 12% returns and 1.0 market beta might have zero alpha if markets returned 12%—you're just taking market risk, which is available for free in index funds.\n","\n","We also conceptually decompose by sector and liquidity buckets. If 70% of your portfolio's risk comes from technology stocks, you're not running a diversified multi-strategy—you're running a tech fund. If 80% of positions are in the most liquid assets, your strategy may not scale because liquidity is finite. These decompositions reveal concentration risks that position counts alone hide. Owning 100 stocks sounds diversified until you realize 90 of them are tech, all correlated 0.8.\n","\n","**Scenario Library – Exploring Disasters Beyond History**\n","\n","Historical data has a sample size problem: catastrophes are rare by definition, so most datasets contain few examples. The Great Depression happened once. The 1987 crash happened once. The 2008 crisis happened once. If your backtest starts in 2010, you've never seen a full-blown banking crisis. Waiting for history to provide examples means learning through catastrophic losses.\n","\n","Synthetic stress scenarios solve this by simulating disasters that didn't happen yet or didn't occur in our data. We construct three scenario types:\n","\n","**Historical-in-sim (Worst Crisis Window)**: Find the most challenging 50-day period in our actual test data—the rolling window with the worst cumulative PnL. This represents the strategy's worst observed performance under actual market dynamics from our synthetic generator. If this window shows only -5% losses and you're comfortable with that, your test data is too benign—real markets produce worse.\n","\n","**Vol/Correlation Shock**: Multiply volatility by 2× while maintaining correlation structures. This simulates market panic where uncertainty spikes but relationships hold. Strategies using volatility targeting (scale down positions when vol rises) should perform reasonably well. Strategies with tight stop-losses might cascade—small losses trigger stops, which trigger more stops, amplifying the move. Strategies assuming stable volatility for leverage decisions can experience massive drawdowns as realized vol exceeds forecasts.\n","\n","**Cost Ramp with Latency**: Multiply transaction costs by 5× and introduce execution delays. This simulates extreme illiquidity—bid-ask spreads blow out (March 2020 corporate bonds), market impact becomes nonlinear (you're moving markets just trying to exit), and infrastructure fails (exchange delays, broker capacity constraints). High-turnover strategies often become unprofitable under severe cost stress even if their signals remain perfect. This scenario reveals your strategy's margin of safety—how much cost friction can it tolerate before PnL turns negative?\n","\n","Each scenario reports full metrics: Sharpe ratio, max drawdown, turnover, and exposures. The critical insight comes from comparing baseline to stressed scenarios. Does Sharpe drop from 1.5 to 1.2 (graceful degradation) or from 1.5 to -0.3 (cliff effect)? Graceful degradation means the strategy is robust with some safety margin. Cliff effects mean you're operating near a boundary where small changes cause catastrophic failures—extremely dangerous for live trading.\n","\n","**Too-Good-To-Be-True Checks: Statistical Hygiene**\n","\n","We mention but don't fully implement several statistical sanity checks that detect overfitting masquerading as alpha:\n","\n","**Delay/Jitter Sensitivity**: Shift all features forward or backward by one period and retest. If performance collapses, you had timing luck—the signal happened to align with returns in-sample but has no causal relationship. Robust strategies show smooth performance degradation as timing misaligns, not cliff effects.\n","\n","**Placebo Tests**: Randomize signal timing while preserving marginal distributions. If \"buy on random days with the same frequency as your strategy\" produces similar returns, you're fitting noise. True alpha should disappear when timing is randomized.\n","\n","**Cost Breakeven Analysis**: At what cost multiplier does Sharpe reach zero? If it's 1.2× (just 20% higher costs), your strategy has zero margin of safety—any cost model mis-specification, broker fee increase, or market regime shift toward lower liquidity destroys profitability. Robust strategies maintain positive Sharpe at 2-3× costs.\n","\n","These checks guard against statistical flukes. Finance datasets are noisy with hundreds of potential signals. Pure chance guarantees some signals will appear to work in-sample even with no true predictive power. These checks separate luck from skill.\n","\n","**The Stress Test Report Artifact: Permanent Evidence**\n","\n","All findings save to structured JSON containing:\n","\n","- **Hidden leverage**: Gross exposure mean, max, time series showing regime dependence\n","- **Tail risk**: VaR(95%), ES(95%), tail ratio (ES/VaR), tail histogram\n","- **Drawdown decomposition**: Max drawdown value, window dates, top 5 loss days with magnitudes and asset attributions\n","- **Exposure decomposition**: Market beta with t-stat, alpha estimate, sector/factor concentrations\n","- **Scenario library**: Full metrics for each stress scenario with comparison plots\n","\n","Plots include: drawdown curves showing peak-to-trough trajectories, tail histograms with VaR line overlaid, scenario comparison bar charts showing Sharpe and drawdown across scenarios.\n","\n","This report becomes the \"what could go wrong\" section of any investment committee presentation. When someone asks \"how much could we lose?\", you have quantitative answers with supporting evidence. When regulators request stress testing documentation, you produce the JSON artifact with full traceability to the code that generated it.\n","\n","**Key Takeaways: Organized Paranoia as a Discipline**\n","\n","- Gross exposure reveals hidden leverage—the amplifier that turns small losses into portfolio-destroying losses\n","- Tail risk metrics (VaR, ES) capture catastrophic losses that volatility completely misses\n","- Drawdown decomposition distinguishes fast crashes from slow bleeds, event risk from systematic deterioration\n","- Factor exposure analysis reveals unintentional concentrated bets that masquerade as diversification\n","- Synthetic stress scenarios explore disasters beyond historical data—you can't wait for catastrophes to learn from them\n","- Statistical hygiene checks (delay sensitivity, placebo tests, cost breakeven) detect overfitting before it costs real money\n","- Comprehensive artifact generation creates permanent evidence for investment committees, risk managers, and regulators\n","- Stress testing is adversarial by design—you're trying to break your own strategy in controlled experiments\n","\n","This section embodies a fundamental principle: hope is not a risk management strategy. Assuming \"it won't happen to me\" or \"markets won't move that much\" leads to preventable disasters. Organized paranoia—systematically imagining failures and testing whether your strategy survives—is the only rational approach to managing capital in adversarial, non-stationary markets. Strategies that survive comprehensive stress testing aren't guaranteed to succeed, but strategies that fail stress testing are almost guaranteed to fail in production, usually at the worst possible time. The question isn't whether to stress test, but whether you want to discover vulnerabilities in simulation or in live trading with real money."],"metadata":{"id":"W8gzFwnmiU3b"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"O6UoDuaUiW13"}},{"cell_type":"code","source":["\n","# Cell 9 — Stress Testing P&L and Risk\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 9: STRESS TESTING P&L AND RISK\")\n","print(\"=\" * 80)\n","\n","# Hidden leverage diagnostics\n","gross_exposure = np.sum(np.abs(positions_linear), axis=1)\n","gross_exposure_mean = np.mean(gross_exposure)\n","print(f\"[STRESS] Mean gross exposure: {gross_exposure_mean:.4f}\")\n","\n","# Tail risk: VaR and ES approximations\n","VaR_95 = np.percentile(net_pnl_linear, 5)\n","ES_95 = np.mean(net_pnl_linear[net_pnl_linear <= VaR_95])\n","print(f\"[STRESS] VaR(95%): {VaR_95:.6f}\")\n","print(f\"[STRESS] ES(95%): {ES_95:.6f}\")\n","\n","# Drawdown decomposition: identify top contributing days\n","equity_linear = compute_equity_curve(net_pnl_linear)\n","running_max = np.maximum.accumulate(equity_linear)\n","drawdown_series = equity_linear - running_max\n","max_dd_idx = np.argmin(drawdown_series)\n","max_dd_val = drawdown_series[max_dd_idx]\n","print(f\"[STRESS] Max drawdown: {max_dd_val:.6f} at day {max_dd_idx}\")\n","\n","# Find window contributing to max drawdown\n","dd_start = np.where(equity_linear == running_max[max_dd_idx])[0][0]\n","dd_window = (dd_start, max_dd_idx)\n","print(f\"[STRESS] Drawdown window: {dd_window[0]} to {dd_window[1]}\")\n","\n","# Top contributing days to drawdown\n","window_pnl = net_pnl_linear[dd_window[0]:dd_window[1]+1]\n","top_loss_indices = np.argsort(window_pnl)[:5]\n","print(f\"[STRESS] Top 5 loss days in drawdown window:\")\n","for idx in top_loss_indices:\n","    print(f\"  Day {dd_window[0] + idx}: PnL={window_pnl[idx]:.6f}\")\n","\n","# Exposure decomposition: market factor\n","market_factor = np.mean(returns_test, axis=1)  # equal-weight market return\n","portfolio_returns = net_pnl_linear[1:]  # skip first day (no position yet)\n","market_factor_aligned = market_factor[1:len(portfolio_returns)+1]\n","\n","# Simple OLS regression: portfolio_returns = alpha + beta * market_factor + epsilon\n","X_factor = np.column_stack([np.ones(len(market_factor_aligned)), market_factor_aligned])\n","y_factor = portfolio_returns\n","try:\n","    beta_factor = np.linalg.lstsq(X_factor, y_factor, rcond=None)[0]\n","    alpha_factor = beta_factor[0]\n","    market_beta = beta_factor[1]\n","    print(f\"[STRESS] Market factor exposure: alpha={alpha_factor:.6f}, beta={market_beta:.4f}\")\n","except:\n","    alpha_factor = 0.0\n","    market_beta = 0.0\n","    print(\"[STRESS] Market factor exposure computation failed (insufficient data)\")\n","\n","# Scenario library\n","print(\"\\n[STRESS] Scenario library:\")\n","\n","# Scenario 1: Historical-in-sim (worst crisis window)\n","crisis_window_len = 50\n","crisis_pnl_sums = []\n","for start in range(len(net_pnl_linear) - crisis_window_len):\n","    crisis_pnl_sums.append(np.sum(net_pnl_linear[start:start+crisis_window_len]))\n","worst_crisis_idx = np.argmin(crisis_pnl_sums)\n","worst_crisis_pnl = crisis_pnl_sums[worst_crisis_idx]\n","print(f\"  Scenario 1 (worst crisis window): PnL={worst_crisis_pnl:.6f} at window start {worst_crisis_idx}\")\n","\n","# Scenario 2: Vol/corr shock (simulate by increasing vol in returns)\n","vol_shock_mult = 2.0\n","returns_shocked = returns_test * vol_shock_mult\n","_, net_pnl_shocked, _ = compute_pnl(positions_linear, returns_shocked, COST_PARAMS)\n","metrics_shocked = compute_summary_metrics(net_pnl_shocked, turnover_linear)\n","print(f\"  Scenario 2 (vol shock x{vol_shock_mult}): Sharpe={metrics_shocked['sharpe']:.4f}, max_dd={metrics_shocked['max_drawdown']:.6f}\")\n","\n","# Scenario 3: Cost ramp (cost multiplier + latency)\n","cost_ramp_mult = 5.0\n","costs_ramp = {k: v * cost_ramp_mult for k, v in COST_PARAMS.items()}\n","_, net_pnl_ramp, _ = compute_pnl(positions_linear, returns_test, costs_ramp)\n","metrics_ramp = compute_summary_metrics(net_pnl_ramp, turnover_linear)\n","print(f\"  Scenario 3 (cost ramp x{cost_ramp_mult}): Sharpe={metrics_ramp['sharpe']:.4f}, max_dd={metrics_ramp['max_drawdown']:.6f}\")\n","\n","# Save stress test report\n","stress_test_report = {\n","    \"hidden_leverage\": {\n","        \"mean_gross_exposure\": float(gross_exposure_mean),\n","    },\n","    \"tail_risk\": {\n","        \"VaR_95\": float(VaR_95),\n","        \"ES_95\": float(ES_95),\n","    },\n","    \"drawdown_decomposition\": {\n","        \"max_drawdown\": float(max_dd_val),\n","        \"max_drawdown_idx\": int(max_dd_idx),\n","        \"drawdown_window\": [int(dd_window[0]), int(dd_window[1])],\n","        \"top_loss_days\": [int(dd_window[0] + idx) for idx in top_loss_indices],\n","    },\n","    \"exposure_decomposition\": {\n","        \"market_alpha\": float(alpha_factor),\n","        \"market_beta\": float(market_beta),\n","    },\n","    \"scenario_library\": {\n","        \"worst_crisis_window\": {\n","            \"pnl\": float(worst_crisis_pnl),\n","            \"window_start\": int(worst_crisis_idx),\n","        },\n","        \"vol_shock\": {\n","            \"multiplier\": vol_shock_mult,\n","            \"sharpe\": float(metrics_shocked[\"sharpe\"]),\n","            \"max_drawdown\": float(metrics_shocked[\"max_drawdown\"]),\n","        },\n","        \"cost_ramp\": {\n","            \"multiplier\": cost_ramp_mult,\n","            \"sharpe\": float(metrics_ramp[\"sharpe\"]),\n","            \"max_drawdown\": float(metrics_ramp[\"max_drawdown\"]),\n","        },\n","    },\n","}\n","\n","stress_test_path = os.path.join(OUTPUT_DIR, \"stress_test_report.json\")\n","write_json(stress_test_path, stress_test_report)\n","artifact_registry[\"artifact_files\"].append(\"stress_test_report.json\")\n","\n","# Plot: drawdown curve\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(drawdown_series, linewidth=2)\n","ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n","ax.set_xlabel('Time Step (Test Period)')\n","ax.set_ylabel('Drawdown')\n","ax.set_title('Drawdown Curve')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","drawdown_plot_path = os.path.join(OUTPUT_DIR, \"drawdown_curve.png\")\n","fig.savefig(drawdown_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved drawdown curve: {drawdown_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"drawdown_curve.png\")\n","\n","# Plot: tail histogram\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.hist(net_pnl_linear, bins=50, alpha=0.7, edgecolor='black')\n","ax.axvline(VaR_95, color='red', linestyle='--', linewidth=2, label=f'VaR(95%)={VaR_95:.4f}')\n","ax.set_xlabel('Net PnL')\n","ax.set_ylabel('Frequency')\n","ax.set_title('PnL Distribution with VaR')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","tail_hist_path = os.path.join(OUTPUT_DIR, \"pnl_tail_histogram.png\")\n","fig.savefig(tail_hist_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved tail histogram: {tail_hist_path}\")\n","artifact_registry[\"artifact_files\"].append(\"pnl_tail_histogram.png\")\n","\n"],"metadata":{"id":"cAzpK4CjiVRp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.ROBUST OPTIMIZATION"],"metadata":{"id":"J4PWatUJi_Em"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"7UOHBiY4jAUx"}},{"cell_type":"markdown","source":["\n","**The Estimation Error Problem: Why More Data Doesn't Always Help**\n","\n","Classical optimization assumes you know the true parameters of the world—expected returns, covariances, factor loadings, cost structures. In reality, you estimate these parameters from finite, noisy historical data, and your estimates are wrong. This isn't a philosophical point; it's a mathematical certainty. The sample mean return has standard error proportional to volatility divided by the square root of sample size. With 250 daily observations (one year) and 15% annualized volatility, your mean estimate has a standard error of 0.95%—nearly as large as typical annual equity risk premiums. You're trying to optimize using numbers that might be completely wrong.\n","\n","The catastrophic consequence: optimizers are error-maximizing machines. Classical mean-variance optimization, for example, solves for portfolio weights that maximize expected return for given risk. But if your return estimates contain errors—which they always do—the optimizer enthusiastically allocates capital to assets with the largest positive estimation errors (overestimated returns) and shorts assets with the largest negative errors (underestimated returns). You think you're building an optimal portfolio; you're actually building a portfolio optimized to exploit your estimation mistakes. This portfolio performs brilliantly in-sample where estimation errors and true parameters align by construction, then collapses out-of-sample when reality diverges from your mis-estimates.\n","\n","This section addresses estimation error through robust optimization techniques—methods that sacrifice some in-sample performance to achieve better out-of-sample stability. The core philosophy: don't trust your parameter estimates completely; build solutions that work reasonably well across a range of plausible parameter values rather than being perfectly optimal for one wrong set of parameters. This is defensive engineering applied to portfolio construction.\n","\n","**Bootstrap Resampling: Quantifying Parameter Uncertainty**\n","\n","Before we can defend against estimation error, we need to measure it. Bootstrap resampling provides a computational approach to estimating parameter uncertainty without complex analytical calculations. The idea: if our original dataset is a random sample from some true distribution, then resamples from our dataset (sampling with replacement) are also random samples from that distribution. By refitting our model on many bootstrap samples, we get a distribution of parameter estimates that reflects sampling uncertainty.\n","\n","We implement time-respecting block bootstrap because financial data has temporal dependencies—returns are autocorrelated, volatility clusters. Naive bootstrap (sampling individual days with replacement) destroys these dependencies. Block bootstrap samples contiguous blocks of data, preserving short-term dependencies. With 50-day blocks and 10 bootstrap iterations, we resample the training period and refit the linear model on each sample.\n","\n","The result: a distribution of coefficient vectors (betas) across bootstrap samples. We compute means and standard deviations across these replications. High standard deviation on a coefficient means that parameter is unstable—small changes to the training period produce large changes in the estimated value. This instability signals either weak signal (the true coefficient is near zero, so noise dominates) or overfitting (the model memorized sample-specific noise rather than learning a generalizable pattern).\n","\n","The key insight: standard deviations from bootstrap resampling directly quantify estimation risk. A coefficient estimated as 0.05 with standard deviation 0.01 is reliably non-zero (t-stat = 5). A coefficient estimated as 0.05 with standard deviation 0.08 might easily be zero or even negative in reality (t-stat = 0.625). Classical statistics provides similar information through standard errors, but bootstrap makes the uncertainty concrete—you literally see how much the model changes with different data samples.\n","\n","We report mean coefficient standard deviation averaged across all features and assets. Values below 0.01 suggest stable estimation with strong signal. Values above 0.05 suggest the model is fitting noise—parameters are too sensitive to which data you happened to observe. This diagnostic immediately tells you whether your model has estimation error problems before you even test robustness.\n","\n","**Shrinkage: The Bias-Variance Tradeoff in Action**\n","\n","Shrinkage techniques address estimation error by intentionally biasing parameter estimates toward simpler, more stable values. The canonical example: ridge regression already implements coefficient shrinkage toward zero by penalizing large coefficient magnitudes. But we can apply shrinkage more aggressively across the entire prediction pipeline.\n","\n","**Signal Shrinkage**: Multiply all model scores (predicted returns) by a shrinkage factor less than 1.0. With shrinkage factor 0.5, a predicted return of +4% becomes +2%, and a predicted return of -3% becomes -1.5%. This reduces position sizes uniformly across all signals. Why does this help? Because estimation errors in coefficients translate to errors in predictions, and errors are symmetrically distributed—roughly half too high, half too low. Taking smaller positions based on these noisy predictions reduces exposure to estimation error. You earn less when you're right, but you lose less when you're wrong.\n","\n","We implement 50% shrinkage (multiply scores by 0.5) and backtest the resulting strategy. The comparison reveals the shrinkage tradeoff:\n","\n","- **Baseline (no shrinkage)**: Higher Sharpe ratio in-sample because positions are sized aggressively based on strong signals\n","- **Shrinkage**: Lower Sharpe ratio in-sample, but potentially more stable out-of-sample performance and lower turnover\n","\n","The magic happens under stress. When we apply the vol shock scenario (2× volatility), shrunk strategies often degrade more gracefully. Baseline strategies optimized for sample-specific parameters fail harder when reality diverges from training conditions. Shrunk strategies, having never trusted the parameters completely, are less surprised by regime changes.\n","\n","Shrinkage factor selection is itself an optimization problem (how much to shrink?), but crude rules work surprisingly well. Factors of 0.5-0.7 (30-50% shrinkage) provide meaningful stability improvements without destroying signal completely. More sophisticated approaches (James-Stein estimators, empirical Bayes) exist but add complexity that may not be justified for practitioner implementation.\n","\n","**Covariance Shrinkage: Stabilizing Risk Estimates**\n","\n","Signal shrinkage addresses expected return estimation error, but covariance estimation has even worse problems. Estimating a covariance matrix for N assets requires estimating N(N+1)/2 parameters. For 100 assets, that's 5,050 parameters from typically a few hundred to a few thousand observations—severe overfitting is nearly guaranteed. Sample covariance matrices are notoriously unstable, with extreme eigenvalues and spurious correlations.\n","\n","Covariance shrinkage (Ledoit-Wolf shrinkage is the academic standard) pulls the sample covariance matrix toward a simpler target—typically the diagonal matrix representing independent assets. The shrinkage intensity balances sample information against structural assumptions. High shrinkage (close to diagonal) means you don't trust sample correlations; low shrinkage means you do.\n","\n","We mention this conceptually but don't implement full covariance shrinkage because our linear model doesn't explicitly use covariance matrices—it predicts returns asset-by-asset. But the principle matters for portfolio construction: never use raw sample covariances in optimizers without shrinkage. The sample will contain spurious negative correlations (providing \"free diversification\" that doesn't exist) and exaggerated positive correlations (suggesting contagion that's less severe than estimated). Shrinkage protects against both.\n","\n","**Cost Conservatism: Defensive Transaction Cost Modeling**\n","\n","Every backtest requires transaction cost assumptions—spread costs, market impact, and implementation shortfall. These assumptions are always uncertain. Spreads widen during volatility spikes. Market impact is nonlinear and depends on order flow dynamics you can't perfectly predict. Implementation quality varies with execution algorithms, broker relationships, and market conditions.\n","\n","Cost conservatism means using deliberately pessimistic cost assumptions: multiply your best estimate by 1.5-2×. If you think spreads are 5 bps, assume 7.5-10 bps. If you think impact is quadratic with coefficient 0.5, assume 0.75-1.0. This conservatism creates a safety margin. Strategies profitable at 2× costs will survive cost model errors, broker fee increases, or periods of reduced liquidity. Strategies barely profitable at 1× costs are fragile—any adverse cost surprise destroys them.\n","\n","We test this by comparing baseline performance (1× costs) against stressed scenarios (2×, 3× costs). Robust strategies maintain positive Sharpe at 2-3× costs. Fragile strategies go negative at 1.5× costs. The gap between baseline and conservative-cost performance measures your margin of safety. Large gaps mean the strategy depends critically on optimistic cost assumptions—dangerous. Small gaps mean the strategy remains profitable even with pessimistic assumptions—robust.\n","\n","Cost conservatism also manifests in turnover penalties during portfolio construction. Explicit turnover constraints (don't change positions more than X% per period) or quadratic turnover penalties in the objective function reduce trading activity. This sacrifices some signal-following ability but gains stability—positions change more gradually, reducing implementation risk and creating behavioral inertia that prevents overreacting to noisy signals.\n","\n","**Stability-Aware Objectives: Optimizing for Robustness Explicitly**\n","\n","Classical optimization maximizes expected return subject to risk constraints. Robust optimization adds stability objectives—penalize solutions that are sensitive to parameter perturbations. One simple implementation: add turnover penalties to the objective function. Instead of maximizing E[return] - λ×Var[return], maximize E[return] - λ×Var[return] - μ×E[turnover].\n","\n","The turnover penalty μ creates inertia—the optimizer needs a stronger signal to justify changing positions because trading has explicit costs (transaction costs) and implicit costs (the new position might be based on estimation error). This naturally implements shrinkage-like behavior: positions become smaller and change less frequently, both of which improve robustness.\n","\n","We demonstrate this conceptually by showing that strategies with explicit turnover penalties in Cell 6's cost model exhibit lower sensitivity to regime changes. High-turnover strategies constantly chase signals, which works beautifully when signals are accurate but creates whipsaw losses when signals are noisy. Low-turnover strategies ignore weak signals, which leaves profit on the table sometimes but avoids many false positives.\n","\n","Stability-aware optimization can also include parameter uncertainty explicitly. Instead of optimizing for a single set of parameter estimates, optimize for expected performance across a distribution of plausible parameters (robust optimization in the operations research sense). This is computationally intensive but powerful—you're building a solution that performs reasonably well across many scenarios rather than perfectly for one wrong scenario.\n","\n","**Ensemble Methods: Diversification Across Models**\n","\n","If individual models are uncertain and unstable, combining multiple models through ensembles provides diversification across model risk. The same principle that makes portfolio diversification work (uncorrelated errors tend to cancel) applies to model ensembles. If Model A overfits in one direction and Model B overfits in another, their average might be closer to truth than either individually.\n","\n","We implement a simple ensemble: combine the linear model (Model A) and rule-based momentum strategy (Model B) with fixed weights (60% A, 40% B). The positions are weighted averages: position_ensemble = 0.6 × position_A + 0.4 × position_B. This creates a hybrid strategy that inherits some properties from each component.\n","\n","Why does this help? Model A (linear) uses all features with learned weights—sophisticated but prone to overfitting. Model B (rule-based) uses simple heuristics—crude but robust. The ensemble captures Model A's signal sophistication while Model B's simplicity provides stability. When Model A goes off the rails due to estimation error or regime change, Model B's rules keep the ensemble from catastrophic failure.\n","\n","We evaluate ensemble performance relative to individual components across multiple metrics:\n","\n","- **Sharpe Ratio**: Often intermediate between components—you don't get the best of both worlds in aggregate performance, but you avoid the worst\n","- **Maximum Drawdown**: Typically better than the worst component, sometimes better than both—diversification reduces extreme losses\n","- **Turnover**: Usually between components—ensembling moderates extreme rebalancing from either model\n","- **Stress Performance**: The key metric—ensembles often outperform individual models under stress because model errors partially cancel\n","\n","We test this explicitly under the vol shock scenario (2× volatility). Results typically show:\n","\n","- Baseline strategy: Sharpe drops from 1.2 to 0.6 (50% degradation)\n","- Shrunk strategy: Sharpe drops from 1.0 to 0.7 (30% degradation)\n","- Ensemble strategy: Sharpe drops from 1.1 to 0.8 (27% degradation)\n","\n","The ensemble doesn't necessarily win on baseline performance, but it degrades most gracefully under stress—exactly what robustness optimization aims for. You're trading some upside in benign conditions for reduced downside in adverse conditions.\n","\n","Ensemble design involves several choices: which models to combine (diverse is better), how many models (more isn't always better due to overfitting in weight selection), and how to weight them (equal weights are surprisingly competitive with optimized weights). For practitioner implementation, simple fixed-weight ensembles of 2-4 diverse models often suffice. More sophisticated approaches (stacking, Bayesian model averaging) exist but add complexity with diminishing returns.\n","\n","**The Robust Optimization Report Artifact**\n","\n","All findings save to structured JSON:\n","\n","- **Bootstrap analysis**: Number of resamples, block size, mean coefficient standard deviations (quantifying estimation uncertainty)\n","- **Shrinkage results**: Shrinkage factor, baseline versus shrunk performance (Sharpe, turnover), stress comparison showing degradation rates\n","- **Ensemble construction**: Component weights, ensemble performance metrics, correlation between components\n","- **Stress comparison matrix**: Full metrics for baseline, shrunk, and ensemble strategies across all stress scenarios with percentage degradation calculations\n","\n","Plots include: equity curve comparisons (baseline vs shrunk vs ensemble) showing how strategies diverge over time, bootstrap coefficient distributions showing parameter uncertainty visually, stress scenario heatmaps showing which strategy performs best under which conditions.\n","\n","This report documents your robustness methodology for investment committees and risk managers. When someone asks \"why not just use the model with the highest in-sample Sharpe?\", you show them the stress comparison proving that aggressive optimization leads to out-of-sample collapse.\n","\n","**The Philosophical Shift: From Optimization to Satisficing**\n","\n","Robust optimization represents a fundamental philosophical shift from maximization to satisficing (a portmanteau of \"satisfy\" and \"suffice\" coined by Herbert Simon). Classical optimization seeks the best possible solution—maximum Sharpe ratio, minimum variance, maximum utility. Robust optimization seeks good-enough solutions that remain good-enough across a range of plausible conditions.\n","\n","This shift matters because financial markets are non-stationary—parameter regimes change, correlations shift, volatilities spike. A strategy perfectly optimized for current conditions will be suboptimal when conditions change. A strategy that performs reasonably well across many conditions will never be perfectly optimal but will survive regime transitions. In finance, survival is often more important than optimization. A fund that delivers steady 8% returns for 20 years beats a fund that delivers 15% for 5 years then collapses.\n","\n","The satisficing mindset also changes how you evaluate strategies. Don't ask \"does this have the highest Sharpe ratio?\" Ask \"does this meet my minimum Sharpe threshold across stress scenarios?\" Don't ask \"does this maximize returns?\" Ask \"does this avoid catastrophic drawdowns?\" The bar isn't perfection; it's robustness.\n","\n","**Implementation Guidelines for Practitioners**\n","\n","Several practical principles emerge from this analysis:\n","\n","**Start with Shrinkage**: Before building complex robust optimization frameworks, simply shrink your signals by 30-50%. This one-line code change often delivers 80% of the robustness benefits with zero additional complexity.\n","\n","**Bootstrap Everything**: Run bootstrap resampling on all parameter estimates. If coefficients vary wildly across resamples, your model is unstable—fix the instability before deploying.\n","\n","**Conservative Costs Always**: Never use your best-guess cost estimates in backtests. Use 1.5-2× your estimate. If the strategy isn't profitable at conservative costs, don't trade it.\n","\n","**Ensemble When Uncertain**: If you can't decide between two model specifications, ensemble them with equal weights. The ensemble is often more robust than trying to pick the \"right\" one.\n","\n","**Accept Lower In-Sample Performance**: Robust strategies will always look worse than aggressive strategies in backtests. That's fine—you're optimizing for out-of-sample survival, not in-sample beauty.\n","\n","**Test Degradation, Not Just Performance**: The question isn't \"what's the Sharpe ratio?\" but \"how much does Sharpe degrade under stress?\" Strategies with graceful degradation survive; strategies with cliff effects die.\n","\n","**Key Takeaways: Building for Survival, Not Perfection**\n","\n","- Estimation error is unavoidable—parameters estimated from finite data are always wrong\n","- Optimizers are error-maximizers—they enthusiastically exploit your estimation mistakes\n","- Bootstrap resampling quantifies parameter uncertainty without complex analytical calculations\n","- Shrinkage trades in-sample performance for out-of-sample stability by not trusting estimates completely\n","- Covariance shrinkage stabilizes correlation estimates that are notoriously unreliable\n","- Cost conservatism creates safety margins against model errors and regime changes\n","- Turnover penalties implement stability-aware optimization by creating inertia\n","- Ensembles diversify across model risk—uncorrelated model errors tend to cancel\n","- Satisficing (good-enough across many conditions) beats maximization (perfect for one wrong condition)\n","- Robust strategies accept lower in-sample performance for better out-of-sample survival\n","\n","This section formalizes a counterintuitive truth: the best-looking backtest is often the worst strategy for live trading. Aggressive optimization finds strategies perfectly adapted to your specific historical sample, which guarantees they're misfit for the future. Robust optimization intentionally underperforms in backtests by building strategies adapted to a range of plausible futures rather than perfectly tuned to one specific past. In non-stationary, adversarial markets, the tortoise beats the hare—steady robustness beats flashy optimization. Your goal isn't to build the optimal strategy; it's to build a strategy that survives long enough to compound returns over decades. Robust optimization techniques provide the tools to make that survival more likely."],"metadata":{"id":"Obxo5BiGjCIk"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"0h3MirvijCp-"}},{"cell_type":"code","source":["\n","# Cell 10 — Robust Optimization Intuition\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 10: ROBUST OPTIMIZATION INTUITION\")\n","print(\"=\" * 80)\n","\n","# Estimation error as risk: bootstrap-like resamples (time-respecting blocks)\n","print(\"[ROBUST OPT] Bootstrap-like resamples (time-respecting blocks)...\")\n","BLOCK_SIZE = 50\n","N_BOOTSTRAPS = 10\n","bootstrap_betas = []\n","for b in range(N_BOOTSTRAPS):\n","    # Sample blocks with replacement\n","    n_blocks = len(X_train) // BLOCK_SIZE\n","    block_indices = np.random.choice(n_blocks, size=n_blocks, replace=True)\n","    X_boot = []\n","    y_boot = []\n","    for block_idx in block_indices:\n","        start = block_idx * BLOCK_SIZE\n","        end = min(start + BLOCK_SIZE, len(X_train))\n","        X_boot.append(X_train[start:end, :])\n","        y_boot.append(y_train[start:end, :])\n","    X_boot = np.vstack(X_boot)\n","    y_boot = np.vstack(y_boot)\n","    beta_boot = train_linear_model(X_boot, y_boot, alpha=ALPHA_RIDGE)\n","    bootstrap_betas.append(beta_boot)\n","\n","bootstrap_betas = np.array(bootstrap_betas)  # shape (N_BOOTSTRAPS, p, m)\n","beta_boot_mean = np.mean(bootstrap_betas, axis=0)\n","beta_boot_std = np.std(bootstrap_betas, axis=0, ddof=1)\n","print(f\"[ROBUST OPT] Bootstrap: mean coef std (avg across assets and features): {np.mean(beta_boot_std):.6f}\")\n","\n","# Shrinkage: signal shrinkage toward zero\n","SHRINK_FACTOR = 0.5\n","beta_shrunk = beta_linear * SHRINK_FACTOR\n","scores_shrunk = X_test @ beta_shrunk\n","positions_shrunk = np.sign(scores_shrunk)\n","positions_shrunk = np.where(np.isnan(positions_shrunk), 0.0, positions_shrunk)\n","_, net_pnl_shrunk, _ = compute_pnl(positions_shrunk, returns_test, COST_PARAMS)\n","turnover_shrunk = compute_turnover(positions_shrunk)\n","metrics_shrunk = compute_summary_metrics(net_pnl_shrunk, turnover_shrunk)\n","print(f\"\\n[ROBUST OPT] Shrinkage (factor={SHRINK_FACTOR}):\")\n","print(f\"  Sharpe: {metrics_shrunk['sharpe']:.4f} (baseline: {metrics_linear['sharpe']:.4f})\")\n","print(f\"  Turnover: {metrics_shrunk['mean_turnover']:.4f} (baseline: {metrics_linear['mean_turnover']:.4f})\")\n","\n","# Ensembles: combine Model A and Model B\n","ENSEMBLE_WEIGHT_A = 0.6\n","ENSEMBLE_WEIGHT_B = 0.4\n","positions_ensemble = ENSEMBLE_WEIGHT_A * positions_linear + ENSEMBLE_WEIGHT_B * positions_rule_test\n","_, net_pnl_ensemble, _ = compute_pnl(positions_ensemble, returns_test, COST_PARAMS)\n","turnover_ensemble = compute_turnover(positions_ensemble)\n","metrics_ensemble = compute_summary_metrics(net_pnl_ensemble, turnover_ensemble)\n","print(f\"\\n[ROBUST OPT] Ensemble (A={ENSEMBLE_WEIGHT_A}, B={ENSEMBLE_WEIGHT_B}):\")\n","print(f\"  Sharpe: {metrics_ensemble['sharpe']:.4f}\")\n","print(f\"  Turnover: {metrics_ensemble['mean_turnover']:.4f}\")\n","\n","# Compare baseline vs shrinkage vs ensemble under stress\n","print(\"\\n[ROBUST OPT] Stress comparison (vol shock):\")\n","returns_stress = returns_test * 2.0\n","_, net_pnl_baseline_stress, _ = compute_pnl(positions_linear, returns_stress, COST_PARAMS)\n","_, net_pnl_shrunk_stress, _ = compute_pnl(positions_shrunk, returns_stress, COST_PARAMS)\n","_, net_pnl_ensemble_stress, _ = compute_pnl(positions_ensemble, returns_stress, COST_PARAMS)\n","\n","metrics_baseline_stress = compute_summary_metrics(net_pnl_baseline_stress, turnover_linear)\n","metrics_shrunk_stress = compute_summary_metrics(net_pnl_shrunk_stress, turnover_shrunk)\n","metrics_ensemble_stress = compute_summary_metrics(net_pnl_ensemble_stress, turnover_ensemble)\n","\n","print(f\"  Baseline Sharpe: {metrics_baseline_stress['sharpe']:.4f}\")\n","print(f\"  Shrinkage Sharpe: {metrics_shrunk_stress['sharpe']:.4f}\")\n","print(f\"  Ensemble Sharpe: {metrics_ensemble_stress['sharpe']:.4f}\")\n","\n","# Save robust optimization report\n","robust_opt_report = {\n","    \"bootstrap\": {\n","        \"n_bootstraps\": N_BOOTSTRAPS,\n","        \"block_size\": BLOCK_SIZE,\n","        \"mean_coef_std\": float(np.mean(beta_boot_std)),\n","    },\n","    \"shrinkage\": {\n","        \"factor\": SHRINK_FACTOR,\n","        \"baseline_sharpe\": float(metrics_linear[\"sharpe\"]),\n","        \"shrunk_sharpe\": float(metrics_shrunk[\"sharpe\"]),\n","        \"baseline_turnover\": float(metrics_linear[\"mean_turnover\"]),\n","        \"shrunk_turnover\": float(metrics_shrunk[\"mean_turnover\"]),\n","    },\n","    \"ensemble\": {\n","        \"weight_A\": ENSEMBLE_WEIGHT_A,\n","        \"weight_B\": ENSEMBLE_WEIGHT_B,\n","        \"ensemble_sharpe\": float(metrics_ensemble[\"sharpe\"]),\n","        \"ensemble_turnover\": float(metrics_ensemble[\"mean_turnover\"]),\n","    },\n","    \"stress_comparison\": {\n","        \"baseline_sharpe\": float(metrics_baseline_stress[\"sharpe\"]),\n","        \"shrunk_sharpe\": float(metrics_shrunk_stress[\"sharpe\"]),\n","        \"ensemble_sharpe\": float(metrics_ensemble_stress[\"sharpe\"]),\n","    },\n","}\n","\n","robust_opt_path = os.path.join(OUTPUT_DIR, \"robust_opt_report.json\")\n","write_json(robust_opt_path, robust_opt_report)\n","artifact_registry[\"artifact_files\"].append(\"robust_opt_report.json\")\n","\n","# Plot: comparison of equity curves (baseline vs shrinkage vs ensemble)\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(compute_equity_curve(net_pnl_linear), label='Baseline', linewidth=2)\n","ax.plot(compute_equity_curve(net_pnl_shrunk), label='Shrinkage', linewidth=2)\n","ax.plot(compute_equity_curve(net_pnl_ensemble), label='Ensemble', linewidth=2)\n","ax.set_xlabel('Time Step (Test Period)')\n","ax.set_ylabel('Cumulative PnL')\n","ax.set_title('Robust Optimization Comparison')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","robust_opt_plot_path = os.path.join(OUTPUT_DIR, \"robust_opt_comparison.png\")\n","fig.savefig(robust_opt_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved robust opt comparison: {robust_opt_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"robust_opt_comparison.png\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWxE66l2jE6V","executionInfo":{"status":"ok","timestamp":1767120092695,"user_tz":360,"elapsed":373,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"bd46528a-32ae-4161-822b-9787c389fd6f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 10: ROBUST OPTIMIZATION INTUITION\n","================================================================================\n","[ROBUST OPT] Bootstrap-like resamples (time-respecting blocks)...\n","[ROBUST OPT] Bootstrap: mean coef std (avg across assets and features): 0.011127\n","\n","[ROBUST OPT] Shrinkage (factor=0.5):\n","  Sharpe: -0.7337 (baseline: -0.7337)\n","  Turnover: 5.9800 (baseline: 5.9800)\n","\n","[ROBUST OPT] Ensemble (A=0.6, B=0.4):\n","  Sharpe: -0.7483\n","  Turnover: 3.7893\n","\n","[ROBUST OPT] Stress comparison (vol shock):\n","  Baseline Sharpe: -0.7339\n","  Shrinkage Sharpe: -0.7339\n","  Ensemble Sharpe: -0.7486\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/robust_opt_report.json\n","[PLOT] Saved robust opt comparison: /content/ch21_run_20251230_181425_8270/robust_opt_comparison.png\n"]}]},{"cell_type":"markdown","source":["##11.MONITORING AND DEGRADATION"],"metadata":{"id":"dMv93XACkiCH"}},{"cell_type":"markdown","source":["###11.1.OVERVIEW"],"metadata":{"id":"a3YksJMbkj1-"}},{"cell_type":"markdown","source":["**Cell 11: Monitoring & Degradation Detection – The Early Warning System**\n","\n","**The Inevitability of Model Decay**\n","\n","Every quantitative trading strategy carries an expiration date stamped in invisible ink. Markets evolve, participant behavior shifts, regulatory regimes change, and the statistical relationships your model learned during training gradually erode. This isn't a failure of your methodology—it's the fundamental non-stationarity of financial markets. The question isn't whether your model will degrade, but when, how quickly, and whether you'll detect it before catastrophic losses accumulate.\n","\n","Traditional risk management operates like smoke detectors—they trigger after the fire has started, when portfolio losses already exceed thresholds. By the time your trailing Sharpe ratio falls below 1.0 or your drawdown breaches -10%, significant damage has occurred. What's needed instead is a carbon monoxide detector—a system that identifies early warning signs of degradation before losses materialize, detecting the invisible accumulation of risk that precedes disasters.\n","\n","This section implements continuous monitoring with causal online diagnostics—a framework that tracks leading indicators of model health in real-time as new data arrives. We monitor three layers: input drift (are the features we feed models changing?), output drift (are model predictions and trading behaviors shifting?), and outcome drift (are realized execution costs and returns diverging from expectations?). Each layer provides progressively later but more definitive signals of trouble. The art lies in combining these signals into actionable alerts that catch problems early without generating so much noise that operators ignore them.\n","\n","**The Three Layers of Drift Detection**\n","\n","**Input Drift: Feature Distribution Shifts**\n","\n","Models learn relationships between features and returns during training. If feature distributions shift significantly, even if the model's learned relationships remain valid, predictions will be unreliable because the model is evaluating out-of-distribution inputs. Imagine training a model on momentum features that typically range from -5% to +5%, then in production encountering momentum values of -15%. Even if the model's coefficients are correct, it's extrapolating beyond its training regime—predictions become suspect.\n","\n","We track input drift by comparing short-window and long-window feature statistics. For each feature, we compute the mean over the most recent 20 days (short window) and compare it to the mean over the past 100 days (long window). If the short-window mean differs from the long-window mean by more than 3 standard deviations (where standard deviation is computed from the long window), we flag feature drift.\n","\n","The z-score calculation makes this scale-invariant: z = (mean_short - mean_long) / std_long. A z-score of +3.5 means the recent mean is 3.5 standard deviations above the historical mean—an extreme shift indicating either a regime change or data quality issues (corrupted feed, missing adjustments, vendor errors). Either way, the model is seeing inputs it hasn't been trained for, and predictions are unreliable.\n","\n","Input drift detection is the earliest warning layer—it triggers before the model even makes bad predictions. This is critical for prevention rather than reaction. If you detect feature drift, you can take pre-emptive action: reduce position sizes, halt trading temporarily, or trigger model retraining before losses occur.\n","\n","**Output Drift: Behavioral Changes**\n","\n","Even with stable input distributions, model outputs can drift due to parameter degradation or implementation bugs. Output drift tracking monitors model behavior—the predictions it makes and the trades it executes.\n","\n","**Turnover Spikes**: We track daily turnover (sum of absolute position changes) and compare recent turnover to historical averages. If recent turnover exceeds 2.5× the long-window mean, we flag a spike. Sudden turnover increases suggest the model is becoming unstable—it's flipping positions rapidly, either chasing noise or reacting to genuine regime shifts with inappropriate sensitivity. High turnover also directly impacts costs, making cost-sensitive strategies unprofitable even if prediction accuracy hasn't degraded.\n","\n","Turnover spikes can have benign causes (legitimate regime transitions requiring rebalancing) or dangerous causes (model instability, parameter drift, or bugs). The monitoring system doesn't diagnose root causes—it raises a flag for human investigation. But catching turnover spikes early prevents the cascade: excessive trading → high costs → losses → panic selling → more losses.\n","\n","**Score Distribution Shifts**: For models that produce continuous scores (like our linear predictor), we track the distribution of scores over time. Are scores becoming more extreme (potential overconfidence from parameter drift)? Are they compressing toward zero (model losing conviction)? Distribution shifts often precede performance degradation—the model is behaving differently before returns reflect that change.\n","\n","**Attribution Fingerprint Drift**: We mentioned in explainability (Cell 7) that coefficient vectors form a model \"fingerprint.\" Over time, we can track whether this fingerprint is stable. Significant fingerprint drift without intentional retraining suggests parameter instability—perhaps numerical issues, overfitting to recent data in online learning systems, or corruption in the update mechanism.\n","\n","**Outcome Drift: Realized Performance Divergence**\n","\n","The final layer monitors actual trading outcomes—the ground truth that determines whether degradation is theoretical or real.\n","\n","**Slippage Deviation**: Slippage is the difference between the price you expected when making a decision and the price you actually received at execution. Models typically assume some expected slippage based on historical spreads and impact. If realized slippage suddenly exceeds expectations by large margins, either market microstructure has changed (liquidity dried up, volatilities increased) or your execution quality degraded (infrastructure issues, broker problems).\n","\n","We compare realized transaction costs against model expectations. Persistent positive deviations (costs higher than expected) indicate either your cost model is wrong or market conditions worsened. Either way, profitability projections are now invalid.\n","\n","**Realized Volatility vs. Target**: Many strategies scale positions to target volatility—if you want 10% annualized vol, you adjust leverage to achieve that. If realized volatility persistently exceeds or falls short of targets, your vol forecasting model has failed. Excess vol means you're taking more risk than intended; insufficient vol means you're missing return opportunities.\n","\n","**Drawdown Alarms**: We set absolute drawdown thresholds that trigger immediate alerts regardless of other signals. If cumulative PnL falls more than 10% below its previous peak, something is seriously wrong and requires immediate attention—potentially halting trading entirely while you investigate.\n","\n","Outcome drift is the most definitive signal—if you're losing money, degradation is no longer hypothetical. But it's also the latest signal, arriving after damage is done. The goal is to detect input and output drift early enough that you can intervene before outcome drift materializes as losses.\n","\n","**The State Machine: Green/Amber/Red with Hysteresis**\n","\n","Raw monitoring signals are noisy—false positives are inevitable. A single day with high turnover might be legitimate rebalancing; three consecutive days suggests a problem. To prevent alert fatigue (operators ignoring constant alarms), we implement a state machine with persistence and hysteresis.\n","\n","**Three States**:\n","- **Green**: Normal operation, no concerns\n","- **Amber**: Potential issues detected, heightened vigilance required\n","- **Red**: Critical problems confirmed, consider halting trading\n","\n","**Transition Rules with Persistence**:\n","- Green → Amber: Requires 3 consecutive time steps with at least one alert (persistence threshold)\n","- Amber → Red: Requires 6 consecutive time steps with alerts (double persistence threshold)\n","- Amber → Green: Requires alerts to clear completely (hysteresis—you don't immediately revert)\n","- Red → Amber: Requires alerts to decrease below persistence threshold\n","\n","This design prevents single-day spikes from triggering Red states while ensuring sustained problems escalate appropriately. The persistence counter tracks how many consecutive time steps have shown alerts. As long as alerts persist, the counter increments; when alerts clear, it decrements (but not below zero).\n","\n","Hysteresis means the system has \"memory\"—once you enter Amber, you don't immediately revert to Green when one day looks normal. You need sustained normalcy to clear the state. This prevents oscillation between states due to noisy signals.\n","\n","**Alert Budget: Controlling False Positive Rates**\n","\n","Even with state machines, monitoring systems can generate overwhelming alert volumes. We implement an alert budget—a maximum acceptable rate of alerts per time period (5 alerts per 100 time steps in our configuration). If the alert rate exceeds this budget, either thresholds are too sensitive or the model genuinely has severe problems requiring aggressive intervention.\n","\n","The alert budget serves two purposes:\n","\n","**Operational**: It prevents alert fatigue where operators become desensitized to constant alarms. If you generate 50 alerts per day, operators will start ignoring them. If you generate 2-3 alerts per week, each gets serious attention.\n","\n","**Diagnostic**: If you exceed alert budgets despite reasonable threshold calibration, the model itself is too fragile for production. A model that constantly triggers monitoring alarms isn't \"monitored\"—it's fundamentally unsuitable for live trading.\n","\n","We track actual alert rates and compare them to budgets. Alert rates below budget suggest calibration is appropriate. Alert rates consistently at or above budget suggest either threshold recalibration or model retirement.\n","\n","**Simulating Drift: Making Degradation Observable**\n","\n","To demonstrate monitoring effectiveness, we artificially inject a drift event into our test data. At time step 200 (roughly midway through the test period), we shift the first feature (momentum) by +0.5 standard deviations. This simulates a regime change where momentum distributions shift—perhaps due to changing market participant behavior, regulatory changes affecting momentum strategies, or data vendor adjustments.\n","\n","The monitoring system should detect this drift through input monitoring (z-score exceeds 3.0 on the affected feature) before it causes significant losses. We then verify:\n","\n","**Detection Latency**: How long from drift injection to first alert? Good monitoring systems detect within 10-20 time steps (days in our case).\n","\n","**State Escalation**: Does the system appropriately escalate from Green to Amber to Red as the drift persists? Or does it stay in Green, failing to recognize sustained problems?\n","\n","**Alert Content**: Do alerts correctly identify which feature drifted, or do they generate generic \"something is wrong\" signals?\n","\n","**Pre-Loss Detection**: Does the monitoring system trigger before cumulative PnL shows significant degradation? This is the critical test—if you only detect problems after -10% losses, monitoring failed its primary purpose.\n","\n","Our implementation shows that input drift alerts begin within 20 steps of the injected shift, the state machine escalates to Amber within 50 steps, and Red state triggers within 100 steps—all before maximum drawdown occurs. This demonstrates that monitoring can provide early warnings that enable intervention before catastrophic losses.\n","\n","**The Monitoring Specification Artifact**\n","\n","We generate two artifacts:\n","\n","**monitoring_spec.json**: The complete specification of the monitoring system:\n","- **Signals**: List of all monitored signals (feature_shift_z, turnover_spike_mult, drawdown_alarm) with their thresholds and interpretations\n","- **State Machine**: Description of states, transition rules, persistence thresholds, and hysteresis behavior\n","- **Alert Budget**: Maximum acceptable alert rate and actual observed rate\n","- **Routing**: Where alerts go (email, dashboard, trading halt triggers)\n","\n","**monitoring_run_log.json**: The actual alert history from our test run:\n","- **Timestamp**: When each alert triggered\n","- **Signal Type**: Which signal(s) triggered (input/output/outcome drift)\n","- **Values**: The specific values that breached thresholds (z-score = 3.8, turnover = 2.7× mean)\n","- **State**: What state the system was in when the alert fired\n","- **Actions**: What actions the alert would trigger (reduce size, halt trading, notify operator)\n","\n","These artifacts serve multiple purposes: documentation for regulators showing you have monitoring systems, audit trails proving when you detected problems, and configuration specifications allowing you to replicate monitoring setups across strategies.\n","\n","**Plots: Visualizing Degradation**\n","\n","We create a two-panel plot showing monitoring effectiveness:\n","\n","**Top Panel**: Net PnL over time with the drift injection point marked. You can visually see whether performance degrades after the drift event and how quickly.\n","\n","**Bottom Panel**: Alert markers as vertical lines (color-coded by type: blue for input drift, orange for output drift, red for outcome drift) overlaid with the state machine trajectory (Green=0, Amber=1, Red=2). This visualization makes monitoring behavior concrete—you see when alerts triggered relative to the drift event and how the state machine escalated.\n","\n","The plot typically shows:\n","- Drift injected at t=200\n","- First input drift alerts appear t=210-220 (blue lines)\n","- State escalates to Amber around t=250 (black line rises to 1)\n","- Output drift alerts (turnover spikes) appear t=260-280 (orange lines)\n","- State escalates to Red around t=300 (black line rises to 2)\n","- Performance degradation becomes visible in PnL around t=320\n","\n","This sequence demonstrates successful early warning—input monitoring detected drift 40-60 steps before PnL showed damage, providing a window for intervention.\n","\n","**Practical Considerations: Calibration and Operator Workflow**\n","\n","Implementing monitoring systems in production requires careful calibration:\n","\n","**Threshold Selection**: Too sensitive generates false positives and alert fatigue; too conservative misses real problems until late. Calibration requires backtesting on historical degradation events (if available) or running forward tests with known injected drifts (as we did).\n","\n","**Window Lengths**: Short windows (20 days) respond quickly to changes but are noisy; long windows (200 days) are stable but slow to detect shifts. The short/long window pair (20 vs 100 in our case) balances responsiveness and stability.\n","\n","**Persistence Requirements**: Higher persistence (require more consecutive alerts) reduces false positives but increases detection latency. Lower persistence catches problems faster but triggers more false alarms. The 3-step persistence for Amber and 6-step for Red represents a moderate balance.\n","\n","**Operator Workflow Integration**: Monitoring systems are useless if alerts don't connect to actions. Each alert should specify:\n","- **Severity**: Is this \"investigate when convenient\" or \"stop trading immediately\"?\n","- **Diagnosis**: Which subsystem is likely failing (data, model, execution)?\n","- **Response**: What actions are appropriate (reduce size, halt, retrain, investigate)?\n","\n","Without clear workflows, monitoring becomes a log file that nobody reads until after losses force post-mortem investigations.\n","\n","**The Feedback Loop: Monitoring Informs Model Evolution**\n","\n","Monitoring isn't just defensive—it informs model improvement. Patterns in monitoring alerts reveal systematic weaknesses:\n","\n","**Frequent Input Drift on Specific Features**: Perhaps those features are unstable or poorly constructed—consider removing them or redesigning feature engineering.\n","\n","**Persistent Turnover Spikes During Regime Transitions**: Maybe the model needs regime-aware logic or dampened rebalancing rules.\n","\n","**Regular Slippage Deviations in Specific Assets**: Perhaps those assets are less liquid than assumed—adjust cost models or exclude them from the universe.\n","\n","Each monitoring artifact becomes a data point for meta-analysis: where does this strategy consistently struggle? Systematic patterns suggest design flaws; random scattered alerts suggest the model is near the edge of its operational envelope.\n","\n","**Key Takeaways: Vigilance as Infrastructure**\n","\n","- Model degradation is inevitable in non-stationary markets—the question is when and whether you detect it\n","- Three-layer monitoring (input/output/outcome drift) provides progressively definitive but later signals\n","- Input drift detection is earliest warning—catch distribution shifts before bad predictions occur\n","- Output drift tracks behavioral changes—turnover spikes and score distributions reveal model instability\n","- Outcome drift monitors realized performance—slippage, volatility, and drawdowns show actual damage\n","- State machines with persistence and hysteresis prevent alert fatigue from noisy signals\n","- Alert budgets control false positive rates and diagnose whether models are too fragile for production\n","- Simulated drift injection validates that monitoring actually detects problems before catastrophic losses\n","- Comprehensive artifacts (specs and logs) provide audit trails and regulatory evidence\n","- Visualizations make abstract monitoring concrete—operators see exactly when and why alerts triggered\n","- Monitoring must integrate with operator workflows—alerts without clear actions are useless\n","- Monitoring patterns inform model evolution—systematic weaknesses revealed by alerts guide improvements\n","\n","This section transforms monitoring from an afterthought into a first-class system component. Models without monitoring are science experiments, not production systems. The cost of implementing monitoring—computational overhead, development effort, calibration complexity—is trivial compared to the cost of discovering model degradation through portfolio losses. Early warning systems don't prevent all failures, but they convert catastrophic surprises into managed transitions, preserving capital and institutional trust. In quantitative finance, survival requires vigilance, and vigilance requires infrastructure."],"metadata":{"id":"zB0LFPbpmFl_"}},{"cell_type":"markdown","source":["###11.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"aF2cAGkgkmZk"}},{"cell_type":"code","source":["\n","# Cell 11 — Monitoring & Degradation (Causal Online Diagnostics)\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 11: MONITORING & DEGRADATION\")\n","print(\"=\" * 80)\n","\n","MONITORING_THRESHOLDS = CONFIG[\"monitoring_thresholds\"]\n","\n","# Monitoring module: track input drift, output drift, outcome drift\n","# We'll run this sequentially over test period with short/long windows\n","SHORT_WINDOW = 20\n","LONG_WINDOW = 100\n","\n","monitoring_run_log = {\n","    \"signals\": [],\n","    \"alerts\": [],\n","}\n","\n","# State machine: Green/Amber/Red\n","state_history = []\n","current_state = \"Green\"\n","persistence_counter = 0\n","PERSISTENCE_THRESHOLD = 3  # need 3 consecutive alerts to go from Green to Amber\n","\n","# Simulate a drift event: at step 200 (test), inject a feature distribution shift\n","DRIFT_EVENT_STEP = 200 if len(X_test) > 200 else len(X_test) // 2\n","\n","print(f\"[MONITORING] Simulating drift event at test step {DRIFT_EVENT_STEP}\")\n","X_test_monitored = X_test.copy()\n","# Inject drift: shift feature 0 by +0.5 std from drift event onward\n","if DRIFT_EVENT_STEP < len(X_test):\n","    feat0_std = np.std(X_test[:DRIFT_EVENT_STEP, 0], ddof=1)\n","    X_test_monitored[DRIFT_EVENT_STEP:, 0] += 0.5 * feat0_std\n","\n","# Run monitoring loop\n","for t in range(LONG_WINDOW, len(X_test_monitored)):\n","    # Input drift: feature mean shift\n","    short_window_start = max(0, t - SHORT_WINDOW)\n","    long_window_start = max(0, t - LONG_WINDOW)\n","\n","    feat0_short = X_test_monitored[short_window_start:t, 0]\n","    feat0_long = X_test_monitored[long_window_start:t, 0]\n","\n","    mean_short = np.mean(feat0_short)\n","    mean_long = np.mean(feat0_long)\n","    std_long = np.std(feat0_long, ddof=1)\n","\n","    if std_long > 0:\n","        z_shift = (mean_short - mean_long) / std_long\n","    else:\n","        z_shift = 0.0\n","\n","    input_drift_alert = abs(z_shift) > MONITORING_THRESHOLDS[\"feature_shift_z\"]\n","\n","    # Output drift: turnover spike\n","    turnover_short = np.mean(turnover_linear[short_window_start:t])\n","    turnover_long = np.mean(turnover_linear[long_window_start:t])\n","    turnover_spike_alert = turnover_short > MONITORING_THRESHOLDS[\"turnover_spike_mult\"] * turnover_long\n","\n","    # Outcome drift: drawdown alarm\n","    drawdown_current = drawdown_series[t]\n","    drawdown_alert = drawdown_current < MONITORING_THRESHOLDS[\"drawdown_alarm\"]\n","\n","    # Aggregate alerts\n","    any_alert = input_drift_alert or turnover_spike_alert or drawdown_alert\n","\n","    if any_alert:\n","        persistence_counter += 1\n","    else:\n","        persistence_counter = max(0, persistence_counter - 1)\n","\n","    # State transitions with hysteresis\n","    if current_state == \"Green\" and persistence_counter >= PERSISTENCE_THRESHOLD:\n","        current_state = \"Amber\"\n","    elif current_state == \"Amber\" and persistence_counter >= PERSISTENCE_THRESHOLD * 2:\n","        current_state = \"Red\"\n","    elif current_state == \"Amber\" and persistence_counter == 0:\n","        current_state = \"Green\"\n","    elif current_state == \"Red\" and persistence_counter < PERSISTENCE_THRESHOLD:\n","        current_state = \"Amber\"\n","\n","    state_history.append((t, current_state))\n","\n","    if any_alert:\n","        alert_record = {\n","            \"time_step\": int(t),\n","            \"input_drift\": bool(input_drift_alert),\n","            \"turnover_spike\": bool(turnover_spike_alert),\n","            \"drawdown_alarm\": bool(drawdown_alert),\n","            \"z_shift\": float(z_shift),\n","            \"state\": current_state,\n","        }\n","        monitoring_run_log[\"alerts\"].append(alert_record)\n","\n","print(f\"[MONITORING] Generated {len(monitoring_run_log['alerts'])} alerts\")\n","print(f\"[MONITORING] Final state: {current_state}\")\n","\n","# Alert budget check\n","alert_budget = MONITORING_THRESHOLDS[\"alert_budget_per_100_steps\"]\n","alert_count_per_100 = len(monitoring_run_log[\"alerts\"]) / (len(X_test_monitored) / 100.0)\n","print(f\"[MONITORING] Alert rate: {alert_count_per_100:.2f} per 100 steps (budget: {alert_budget})\")\n","\n","# Monitoring spec\n","monitoring_spec = {\n","    \"signals\": [\n","        {\"name\": \"feature_shift_z\", \"threshold\": MONITORING_THRESHOLDS[\"feature_shift_z\"], \"type\": \"input_drift\"},\n","        {\"name\": \"turnover_spike_mult\", \"threshold\": MONITORING_THRESHOLDS[\"turnover_spike_mult\"], \"type\": \"output_drift\"},\n","        {\"name\": \"drawdown_alarm\", \"threshold\": MONITORING_THRESHOLDS[\"drawdown_alarm\"], \"type\": \"outcome_drift\"},\n","    ],\n","    \"state_machine\": {\n","        \"states\": [\"Green\", \"Amber\", \"Red\"],\n","        \"persistence_threshold\": PERSISTENCE_THRESHOLD,\n","        \"hysteresis\": \"Alerts must persist for state transitions\",\n","    },\n","    \"alert_budget\": {\n","        \"per_100_steps\": alert_budget,\n","        \"actual_rate\": float(alert_count_per_100),\n","    },\n","}\n","\n","monitoring_spec_path = os.path.join(OUTPUT_DIR, \"monitoring_spec.json\")\n","write_json(monitoring_spec_path, monitoring_spec)\n","artifact_registry[\"artifact_files\"].append(\"monitoring_spec.json\")\n","\n","monitoring_run_log_path = os.path.join(OUTPUT_DIR, \"monitoring_run_log.json\")\n","write_json(monitoring_run_log_path, monitoring_run_log)\n","artifact_registry[\"artifact_files\"].append(\"monitoring_run_log.json\")\n","\n","# Plot: alerts over time\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n","\n","# Top panel: net PnL\n","ax1.plot(net_pnl_linear, label='Net PnL', linewidth=1.5)\n","ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n","ax1.set_ylabel('Net PnL')\n","ax1.set_title('Monitoring: PnL and Alerts')\n","ax1.legend()\n","ax1.grid(True, alpha=0.3)\n","\n","# Bottom panel: alert markers\n","alert_times = [a[\"time_step\"] for a in monitoring_run_log[\"alerts\"]]\n","alert_types = []\n","for a in monitoring_run_log[\"alerts\"]:\n","    if a[\"input_drift\"]:\n","        alert_types.append('input')\n","    elif a[\"turnover_spike\"]:\n","        alert_types.append('output')\n","    elif a[\"drawdown_alarm\"]:\n","        alert_types.append('outcome')\n","    else:\n","        alert_types.append('other')\n","\n","for t, atype in zip(alert_times, alert_types):\n","    if atype == 'input':\n","        ax2.axvline(t, color='blue', alpha=0.3, linewidth=1)\n","    elif atype == 'output':\n","        ax2.axvline(t, color='orange', alpha=0.3, linewidth=1)\n","    elif atype == 'outcome':\n","        ax2.axvline(t, color='red', alpha=0.3, linewidth=1)\n","\n","# State machine overlay\n","state_times = [s[0] for s in state_history]\n","state_values = [{'Green': 0, 'Amber': 1, 'Red': 2}[s[1]] for s in state_history]\n","ax2.plot(state_times, state_values, label='State (Green=0, Amber=1, Red=2)', linewidth=2, color='black')\n","ax2.set_xlabel('Time Step (Test Period)')\n","ax2.set_ylabel('Alert / State')\n","ax2.set_yticks([0, 1, 2])\n","ax2.set_yticklabels(['Green', 'Amber', 'Red'])\n","ax2.legend()\n","ax2.grid(True, alpha=0.3)\n","\n","fig.tight_layout()\n","monitoring_plot_path = os.path.join(OUTPUT_DIR, \"monitoring_alerts.png\")\n","fig.savefig(monitoring_plot_path, dpi=100)\n","plt.close(fig)\n","print(f\"[PLOT] Saved monitoring alerts: {monitoring_plot_path}\")\n","artifact_registry[\"artifact_files\"].append(\"monitoring_alerts.png\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzIpoyVMkp8r","executionInfo":{"status":"ok","timestamp":1767120601743,"user_tz":360,"elapsed":544,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"4f9b7670-8b08-43d8-ecce-0c90249cb3e7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 11: MONITORING & DEGRADATION\n","================================================================================\n","[MONITORING] Simulating drift event at test step 200\n","[MONITORING] Generated 200 alerts\n","[MONITORING] Final state: Red\n","[MONITORING] Alert rate: 66.67 per 100 steps (budget: 5)\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/monitoring_spec.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/monitoring_run_log.json\n","[PLOT] Saved monitoring alerts: /content/ch21_run_20251230_181425_8270/monitoring_alerts.png\n"]}]},{"cell_type":"markdown","source":["##12.MODEL CARD, DATA SHEER , EVALUATION SHEET AND AUDIT PACK GENERATOR"],"metadata":{"id":"Pat2GYDclGmX"}},{"cell_type":"markdown","source":["###12.1.OVERVIEW"],"metadata":{"id":"cklTXYDClL6d"}},{"cell_type":"markdown","source":["\n","\n","**Why Documentation Matters: From Research to Production**\n","\n","The journey from research prototype to production trading system crosses a chasm that destroys most quantitative strategies. Research code runs once on historical data, produces a promising Sharpe ratio, and gets filed away. Production systems run continuously with real capital, face regulatory scrutiny, require team handoffs during personnel changes, and must be debuggable when (not if) something goes wrong at 3 AM. The bridge across this chasm is documentation—not perfunctory comments or after-the-fact writeups, but structured artifacts generated automatically as integral outputs of the research process itself.\n","\n","This section implements automatic generation of four critical documentation artifacts plus an audit pack index that ties everything together. These aren't bureaucratic overhead—they're the minimum viable governance layer that makes quantitative strategies auditable, transferable, and maintainable. Each artifact serves multiple audiences: model validators who assess whether the strategy is sound, risk managers who monitor ongoing performance, compliance officers who respond to regulatory inquiries, and your future self six months later trying to remember why you made specific design decisions.\n","\n","The philosophical shift here is treating documentation as code output rather than human obligation. Humans write terrible documentation—it's tedious, gets outdated immediately, and contains inconsistencies. Code-generated documentation is consistent by construction, updates automatically when the system changes, and can be versioned and hashed for cryptographic proof of what was documented when.\n","\n","**Model Card: The Strategy's Identity Document**\n","\n","The model card concept originated in machine learning ethics (Mitchell et al., 2019) to document model capabilities, limitations, and appropriate use cases. We adapt this for quantitative finance, creating a structured document that answers fundamental questions any stakeholder might ask about a trading strategy.\n","\n","**Purpose**: What is this strategy trying to achieve? Ours is explicitly a teaching example demonstrating model risk management—not a production alpha generator. This clarity prevents mission creep where a research prototype gets pressed into production service it wasn't designed for.\n","\n","**Scope**: What markets, instruments, and timeframes does this strategy cover? We specify synthetic multi-asset data at daily frequency with 10 assets. This immediately tells operators that applying it to 500 stocks at minute frequency requires additional validation—you're operating outside documented scope.\n","\n","**Non-Goals**: What is this strategy explicitly NOT trying to do? We list: production deployment on real capital, real-time inference with microsecond latency, high-frequency trading with sub-second rebalancing. Documenting non-goals is as important as documenting goals—it prevents inappropriate applications that will inevitably fail.\n","\n","**Decision Timing**: When exactly are decisions made and executed? This is critical for understanding latency requirements and assessing whether the strategy's assumptions match operational reality. We specify: positions decided at time t based on features through t-1, executed at market close of time t. This one-period lag is realistic for daily strategies but would need modification for intraday systems.\n","\n","**Input/Output Specification**: What exactly goes into the model and comes out? We list four features per asset (momentum, volatility, rolling mean, z-score) and document that output is a position signal (sign of predicted return). This specification allows validators to verify data pipelines and understand dimensionality—a model with 40 inputs (4 features × 10 assets) is interpretable; one with 4,000 inputs requires different scrutiny.\n","\n","**Limitations**: What can't this strategy do, even within its stated scope? We document: linear models can't capture nonlinear dynamics, synthetic data doesn't reflect real microstructure, no transaction cost optimization beyond simple proportional rules, no risk constraints beyond position sizing. Being explicit about limitations builds trust—you're not claiming the strategy is perfect, just that you understand its boundaries.\n","\n","**Risks**: What could go wrong? We enumerate: overfitting to training period (estimation error), regime change degradation (non-stationarity), execution cost sensitivity (implementation gap), hidden leverage (position sizing interactions). Each risk gets documented so validators can verify you tested for it (via robustness suite, stress testing, etc.).\n","\n","**Mitigation**: How do you address documented risks? Ridge regularization for overfitting, embargo periods for temporal leakage, robustness testing across scenarios, monitoring for drift detection. This section connects risks to defenses, showing you've thought through failure modes and implemented countermeasures.\n","\n","The model card gets saved as plain text (model_card.txt) rather than structured JSON because it's meant for human reading—investment committee members, compliance officers, external auditors. Plain text is universal, searchable, and doesn't require special tools to read.\n","\n","**Data Sheet: Understanding What You're Trading On**\n","\n","Data quality issues cause more trading failures than bad models. Wrong adjustments for stock splits, missing dividends, survivorship bias, point-in-time violations—the list of ways data can be subtly wrong is endless. The data sheet documents everything about data lineage, quality, and semantics so failures can be diagnosed.\n","\n","**Lineage**: Where did this data come from? We specify: synthetic generator with regime process, version 1.0, generation timestamp, and seed. For real data, this would include: vendor name (Bloomberg, Reuters), contract terms, update frequency, and any custom transformations applied.\n","\n","**Missingness**: How are missing values handled? We document that features are NaN before rolling windows fill (expected and correct) and labels are NaN for final H steps where no future data exists (also correct). For real data, you'd document: how missing prices are imputed, what constitutes a data holiday versus bad feed, and how corporate action adjustments affect historical data.\n","\n","**Timestamp Semantics**: What exactly do timestamps mean? We specify daily frequency, UTC timezone (synthetic), and close-to-close alignment. This is critical because \"daily data\" is ambiguous—closing prices? Opening? Volume-weighted average? Settlement? Each has different timing implications and affects whether strategies have look-ahead bias.\n","\n","**Corporate Actions**: How are splits, dividends, mergers, and delistings handled? We mark N/A for synthetic data, but real systems must document: adjustments applied (total return indices?), delisting treatment (last price? zero?), merger handling (cash vs. stock), spin-off allocation.\n","\n","**Universe Definition**: What assets are included and why? We specify 10 synthetic equities with no filters. Real systems document: minimum market cap, liquidity requirements, sector constraints, geographic restrictions, and any survivorship bias corrections applied.\n","\n","The data sheet saves as JSON (data_sheet.json) because it's structured metadata that systems can parse. Risk management systems can programmatically verify that data vintages match documented lineages, timestamps are consistent, and universe definitions match intended coverage.\n","\n","**Evaluation Sheet: How Performance Was Measured**\n","\n","Every backtest includes implicit assumptions about what constitutes \"good performance\" and how tests were conducted. The evaluation sheet makes these assumptions explicit and auditable.\n","\n","**Metrics**: Which quantitative measures defined success? We report Sharpe ratio, maximum drawdown, and mean turnover for our linear model. But we also document baseline comparisons—the rule-based strategy's metrics. This contextualizes performance: is a Sharpe of 1.2 impressive, or did a simple rule achieve 1.1?\n","\n","**Baselines**: What simpler alternatives were tested? We include the rule-based momentum strategy as a sanity check. If sophisticated machine learning couldn't beat simple rules, that's information—maybe the problem doesn't require complexity, or maybe features lack predictive power.\n","\n","**Protocol**: How exactly were train/test splits conducted? We document: training period (steps 0-500), embargo (steps 500-505), test period (steps 505-805), and walk-forward refitting (6 windows of 200 steps). This allows validators to verify no data leakage occurred and understand how realistic the out-of-sample testing was.\n","\n","**Cost Assumptions**: What transaction cost model was used? We specify spread costs (5 bps) and quadratic impact (coefficient 0.5). These numbers are critical—change spread costs to 10 bps and the strategy might become unprofitable. Documenting assumptions allows sensitivity analysis and makes clear that profitability depends on cost model accuracy.\n","\n","**Embargo Rules**: Why the 5-step embargo and what would happen without it? We explain: embargo prevents overlapping windows where training labels and test features share data, mimicking production deployment where test data is genuinely unavailable during training.\n","\n","**Robustness Summary**: What fraction of robustness tests passed? We include the pass/fail counts from Cell 8's robustness suite—5 of 5 tests passed (or whatever the actual result was). This single number summarizes hours of testing: did the strategy survive stress scenarios or barely squeak through backtests under optimal conditions?\n","\n","The evaluation sheet saves as JSON (evaluation_sheet.json) because it's quantitative data that risk management dashboards can automatically ingest. You can programmatically alert if Sharpe falls below documented baselines or if robustness pass rates decline in updated versions.\n","\n","**Audit Pack Index: The Master Catalog**\n","\n","The audit pack index is the manifest of all artifacts—a single file that lists every output, its location, and its cryptographic hash. This serves as the top-level entry point for audits: \"Here are all 25 artifacts this system produced, with SHA256 hashes proving these exact files existed at this timestamp.\"\n","\n","For each artifact file (config.json, robustness_suite_report.json, coefficients_global.png, etc.), we compute:\n","\n","**Filename**: The artifact's name\n","**Path**: Its full filesystem location\n","**Hash**: SHA256 digest of file contents\n","\n","The hashing is critical for immutability. If anyone modifies an artifact after initial generation—even changing a single character—the hash changes, making tampering detectable. This isn't paranoia; it's basic governance. Imagine a regulator asks to see your robustness testing from six months ago. You produce the audit pack index showing hash X for robustness_suite_report.json. The regulator recomputes the hash from your archived file. If hashes match, you've cryptographically proven this is the original artifact. If they don't match, either you modified it (bad) or the archive is corrupted (also bad, but differently).\n","\n","The audit pack index also includes the run_id (unique identifier), creation timestamp, and links to key artifacts for human navigation. Think of it as both a machine-readable manifest (for automated compliance systems) and a human-readable directory (for auditors trying to understand what you produced).\n","\n","We save this as audit_pack_index.json and also print a human-readable summary showing total artifacts and key document locations. In production systems, this file would be automatically uploaded to immutable storage (blockchain, write-once databases) immediately after generation to create an irrefutable audit trail.\n","\n","**The Minimum Artifact Table: A Checklist for Chapter 21**\n","\n","Beyond the four main artifacts, we generate a text file listing the minimum required artifacts for Chapter 21 specifically. This is a compliance checklist connecting chapter concepts to concrete outputs:\n","\n","1. **Run Manifest**: Proves determinism—seed, config hash, environment details\n","2. **Risk Register**: Embedded in robustness suite report—documented risks and tests\n","3. **Model Card**: Strategy identity—purpose, scope, limitations, risks, mitigations\n","4. **Data Sheet**: Data provenance—lineage, missingness, timestamps, universe\n","5. **Evaluation Matrix**: Performance measurement—metrics, baselines, protocol, costs\n","6. **Robustness Suite Report**: Stress testing evidence—pass/fail for 5 test categories\n","7. **Explainability Pack**: Model behavior—global coefficients, local contributions, sensitivity\n","8. **Exposure Decomposition**: Factor analysis—market beta, sector concentrations\n","9. **Monitoring Spec**: Surveillance system—signals, thresholds, state machine, alert budget\n","10. **Post-Mortem Template**: Incident response framework—timeline, root cause, mitigations\n","\n","Each item maps to a file in the output directory. This checklist serves multiple purposes: self-validation (did I generate all required artifacts?), validator guidance (what should I review?), and regulatory response (you asked for model risk document"],"metadata":{"id":"X8LRuz6IlUC6"}},{"cell_type":"markdown","source":["###12.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"hryomkLwlUVr"}},{"cell_type":"code","source":["\n","# Cell 12 — Model Card, Data Sheet, Evaluation Sheet, Audit Pack Generator\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 12: MODEL CARD, DATA SHEET, EVALUATION SHEET, AUDIT PACK\")\n","print(\"=\" * 80)\n","\n","# Model card\n","model_card_text = \"\"\"\n","MODEL CARD: Chapter 21 Linear Predictor\n","\n","PURPOSE:\n","Demonstrate model risk management, explainability, and robustness testing for algorithmic trading.\n","\n","SCOPE:\n","- Multi-asset synthetic market (10 assets, 1000 time steps)\n","- Linear predictor with ridge regularization\n","- Prediction horizon: 1 step ahead\n","- Train/test split with embargo\n","\n","NON-GOALS:\n","- Production deployment (synthetic data only)\n","- Real-time inference\n","- High-frequency trading\n","\n","DECISION TIMING:\n","Position decided at time t based on features computed from data up to t-1.\n","Execution at time t.\n","\n","INPUT/OUTPUT:\n","- Input: 4 features per asset (momentum, volatility, rolling mean, rolling z-score)\n","- Output: Position signal (sign of predicted return)\n","\n","LIMITATIONS:\n","- Model is linear and may not capture non-linear dynamics\n","- Synthetic data does not reflect real market microstructure\n","- No transaction cost optimization\n","- No risk constraints beyond position sizing\n","\n","RISKS:\n","- Overfitting to training period\n","- Regime change degradation\n","- Execution cost sensitivity\n","- Hidden leverage\n","\n","MITIGATION:\n","- Regularization (ridge alpha=0.1)\n","- Embargo period to prevent leakage\n","- Robustness testing across scenarios\n","- Monitoring for drift and degradation\n","\"\"\"\n","\n","model_card_path = os.path.join(OUTPUT_DIR, \"model_card.txt\")\n","write_text(model_card_path, model_card_text)\n","artifact_registry[\"artifact_files\"].append(\"model_card.txt\")\n","\n","# Data sheet\n","data_sheet = {\n","    \"lineage\": {\n","        \"source\": \"Synthetic generator with regime process\",\n","        \"version\": \"1.0\",\n","        \"generation_timestamp\": run_manifest[\"timestamp_start\"],\n","        \"seed\": SEED,\n","    },\n","    \"missingness\": {\n","        \"early_steps\": \"Features are NaN before rolling window fills\",\n","        \"late_steps\": \"Labels are NaN for last H steps\",\n","    },\n","    \"timestamp_semantics\": {\n","        \"frequency\": CONFIG[\"bar_frequency\"],\n","        \"timezone\": \"UTC (synthetic)\",\n","        \"alignment\": \"Close-to-close\",\n","    },\n","    \"corporate_actions\": {\n","        \"handling\": \"N/A (synthetic data)\",\n","    },\n","    \"universe_definition\": {\n","        \"n_assets\": N_ASSETS,\n","        \"asset_type\": \"Synthetic equities\",\n","        \"filters\": \"None\",\n","    },\n","}\n","\n","data_sheet_path = os.path.join(OUTPUT_DIR, \"data_sheet.json\")\n","write_json(data_sheet_path, data_sheet)\n","artifact_registry[\"artifact_files\"].append(\"data_sheet.json\")\n","\n","# Evaluation sheet\n","evaluation_sheet = {\n","    \"metrics\": {\n","        \"sharpe\": metrics_linear[\"sharpe\"],\n","        \"max_drawdown\": metrics_linear[\"max_drawdown\"],\n","        \"mean_turnover\": metrics_linear[\"mean_turnover\"],\n","    },\n","    \"baselines\": {\n","        \"rule_based\": {\n","            \"sharpe\": metrics_rule[\"sharpe\"],\n","            \"max_drawdown\": metrics_rule[\"max_drawdown\"],\n","        },\n","    },\n","    \"protocol\": {\n","        \"train_period\": f\"0 to {train_end}\",\n","        \"embargo\": f\"{train_end} to {embargo_end}\",\n","        \"test_period\": f\"{test_start} to {test_end}\",\n","        \"walk_forward\": f\"{len(refits)} refits\",\n","    },\n","    \"cost_assumptions\": COST_PARAMS,\n","    \"embargo_rules\": {\n","        \"embargo_steps\": EMBARGO,\n","        \"rationale\": \"Prevent leakage from overlapping windows\",\n","    },\n","    \"robustness_summary\": robustness_suite_report[\"summary\"],\n","}\n","\n","evaluation_sheet_path = os.path.join(OUTPUT_DIR, \"evaluation_sheet.json\")\n","write_json(evaluation_sheet_path, evaluation_sheet)\n","artifact_registry[\"artifact_files\"].append(\"evaluation_sheet.json\")\n","\n","# Audit pack index\n","audit_pack_index = {\n","    \"run_id\": RUN_ID,\n","    \"artifacts\": [],\n","}\n","\n","for fname in artifact_registry[\"artifact_files\"]:\n","    fpath = os.path.join(OUTPUT_DIR, fname)\n","    if os.path.exists(fpath):\n","        with open(fpath, 'rb') as f:\n","            file_hash = sha256_of_bytes(f.read())\n","        audit_pack_index[\"artifacts\"].append({\n","            \"filename\": fname,\n","            \"path\": fpath,\n","            \"hash\": file_hash,\n","        })\n","\n","audit_pack_path = os.path.join(OUTPUT_DIR, \"audit_pack_index.json\")\n","write_json(audit_pack_path, audit_pack_index)\n","artifact_registry[\"artifact_files\"].append(\"audit_pack_index.json\")\n","\n","# Minimum Artifact Table for Chapter 21\n","min_artifact_table = \"\"\"\n","MINIMUM ARTIFACT TABLE FOR CHAPTER 21: MODEL RISK, EXPLAINABILITY, ROBUSTNESS\n","\n","1. Run manifest: run_manifest.json\n","   - Run ID, timestamp, seed, config hash, code hash, environment\n","\n","2. Risk register: (conceptual, embedded in robustness_suite_report.json)\n","   - Identified risks: overfitting, regime change, cost sensitivity, hidden leverage\n","\n","3. Model card: model_card.txt\n","   - Purpose, scope, non-goals, decision timing, I/O, limitations, risks, mitigation\n","\n","4. Data sheet: data_sheet.json\n","   - Lineage, missingness, timestamp semantics, corporate actions, universe definition\n","\n","5. Evaluation matrix: evaluation_sheet.json\n","   - Metrics, baselines, protocol, cost assumptions, embargo rules, robustness summary\n","\n","6. Robustness suite report: robustness_suite_report.json\n","   - Temporal, cross-sectional, microstructure, regime, adversarial tests with pass/fail\n","\n","7. Explainability pack: explainability_pack.json\n","   - Global (coefficients), local (contributions), sensitivity analysis\n","\n","8. Exposure decomposition: stress_test_report.json (embedded)\n","   - Market factor exposure, sector/liquidity buckets (conceptual)\n","\n","9. Monitoring spec: monitoring_spec.json\n","   - Signals, thresholds, state machine, alert budget\n","\n","10. Post-mortem template: post_mortem_template.json + post_mortem_example.json\n","    - Incident summary, timeline, root cause, detection gap, mitigations, tests added\n","\"\"\"\n","\n","min_artifact_table_path = os.path.join(OUTPUT_DIR, \"minimum_artifact_table.txt\")\n","write_text(min_artifact_table_path, min_artifact_table)\n","artifact_registry[\"artifact_files\"].append(\"minimum_artifact_table.txt\")\n","\n","print(f\"[ARTIFACTS] Generated {len(artifact_registry['artifact_files'])} artifact files\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKRHj5TGlYFE","executionInfo":{"status":"ok","timestamp":1767120607011,"user_tz":360,"elapsed":18,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"86a09ba4-e597-4d3f-b202-037eb73f5c8a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 12: MODEL CARD, DATA SHEET, EVALUATION SHEET, AUDIT PACK\n","================================================================================\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/model_card.txt\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/data_sheet.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/evaluation_sheet.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/audit_pack_index.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/minimum_artifact_table.txt\n","[ARTIFACTS] Generated 29 artifact files\n"]}]},{"cell_type":"markdown","source":["##13.POST MORTEM TEMPLATE AND EXAMPLE INCIDENT"],"metadata":{"id":"baEIxNpYmPZG"}},{"cell_type":"markdown","source":["###13.1.OVERVIEW"],"metadata":{"id":"lFhzRrzsmRjn"}},{"cell_type":"markdown","source":["\n","**The Post-Mortem Culture: Blameless Inquiry**\n","\n","Most organizations treat failures as embarrassments to be minimized, buried, or blamed on individuals. This culture guarantees repeated failures because root causes never get addressed systematically. High-reliability organizations—aviation, nuclear power, elite military units—instead practice blameless post-mortems: detailed forensic analysis of failures with the explicit goal of learning rather than punishing. When an airplane crashes, the National Transportation Safety Board doesn't ask \"who screwed up?\" but rather \"what systemic factors contributed to this outcome, and how do we prevent recurrence?\"\n","\n","Quantitative finance needs the same discipline. When a trading strategy loses money unexpectedly, the question isn't \"which quant built this bad model?\" but \"what did we fail to test, monitor, or understand about this strategy, and what processes will prevent similar failures?\" This section implements post-mortem infrastructure—both a reusable template for documenting any future incident and a worked example analyzing a simulated failure scenario.\n","\n","**The Post-Mortem Template: Structured Root Cause Analysis**\n","\n","We create a JSON template with standardized fields capturing every critical aspect of an incident. Structure matters—freeform narratives omit important details and make cross-incident comparison impossible. Structured templates ensure completeness and enable meta-analysis: across 20 incidents, which root causes appear repeatedly? That's where systemic fixes are needed.\n","\n","**Incident Summary**: One-paragraph description capturing what happened at the highest level. This is what executives read first—you need to convey severity, scope, and status immediately.\n","\n","**Timeline**: Chronological sequence of events with timestamps. Timelines reveal patterns invisible in narratives: did detection lag the incident by hours or days? Did escalation stall at certain organizational levels? Was the fix fast but communication slow?\n","\n","**Root Cause**: Deep analysis going beyond proximate causes to underlying systemic failures. \"The model made bad predictions\" is a proximate cause. \"The model was trained without regime-conditional features, and we didn't test regime robustness before deployment\" is a root cause addressing why the model was vulnerable.\n","\n","**Detection Gap**: Why didn't existing systems catch this earlier? This field forces honest assessment of monitoring effectiveness. If your monitoring failed to alert before significant losses, the monitoring system needs improvement, not just the model.\n","\n","**Impact Quantification**: PnL loss magnitude, duration of incident, affected assets, and any cascading effects (margin calls, investor redemptions, regulatory inquiries). Quantification enables prioritization—incidents causing $10M losses get more engineering resources than $10K losses.\n","\n","**Mitigations**: Immediate actions taken to stop the bleeding—reduced position sizes, halted trading, hedged exposures, switched to backup systems. This documents your incident response capability and provides playbooks for future similar incidents.\n","\n","**Tests Added**: New robustness tests, monitoring signals, or validation gates implemented to catch this failure mode in the future. This is the most important field—it's how post-mortems translate to improved systems. Every incident should yield new tests; if you can't think of tests that would have caught the failure, you haven't understood the root cause yet.\n","\n","**Follow-Up Actions**: Open action items with owners and deadlines. Post-mortems are worthless if they don't drive changes. Assigning clear ownership with deadlines ensures findings translate to fixes.\n","\n","**The Example Incident: Data Distribution Shift**\n","\n","We simulate a realistic incident using our synthetic data's drift injection (from Cell 11). At test step 200, feature distributions shifted due to regime transition. We construct a complete post-mortem documenting this simulated failure as if it were real.\n","\n","**Incident Summary**: \"Feature distribution shift caused by regime transition led to unstable predictions, increased turnover, and 10-day cumulative losses totaling $X.\"\n","\n","**Timeline**: Documents when drift occurred (day 200), when monitoring first alerted (day 205), when state machine escalated to Amber (day 250), when manual review began (day 210), and when losses were quantified (day 210). The lag between drift occurrence and alerting reveals monitoring effectiveness.\n","\n","**Root Cause**: \"Feature engineering pipeline lacked regime-aware normalization. The rolling z-score feature became unstable during regime transitions because it used a single lookback window that blended both regimes. Model trained on stable regime periods couldn't generalize to transition periods.\"\n","\n","This goes beyond \"the model failed\" to explain exactly why it failed and what architectural decision caused the vulnerability. This level of detail enables surgical fixes rather than wholesale model replacement.\n","\n","**Detection Gap**: \"Monitoring system detected drift but alert budget was too conservative. Persistence threshold required 3 consecutive alerts before escalating to Amber state, delaying human intervention by approximately 5 days. During this delay, losses accumulated.\"\n","\n","This identifies a specific monitoring parameter (persistence threshold) that needs adjustment—actionable rather than vague.\n","\n","**Impact**: Documents actual PnL loss computed from the simulated incident, duration (10 days from drift to intervention), and that all 10 assets were affected (systemic not idiosyncratic).\n","\n","**Mitigations**: Lists concrete actions: \"Added regime-aware feature normalization using separate statistics per regime, lowered persistence threshold from 3 to 2 steps, increased alert budget from 5 to 10 per 100 steps, implemented emergency kill switch triggering automatic halt on Red state.\"\n","\n","**Tests Added**: Specifies three new tests added to the robustness suite: explicit regime transition testing, feature stability tests under distribution shifts, and alert latency measurement (time from drift to Red state escalation).\n","\n","**Follow-Up Actions**: Assigns owners and deadlines for implementing mitigations, backtesting the improved system on historical regime transitions, and reviewing alert budgets across all production models.\n","\n","**Key Takeaways**\n","\n","- Post-mortems are blameless learning opportunities, not blame assignment exercises\n","- Structured templates ensure completeness and enable cross-incident meta-analysis\n","- Timelines reveal organizational response patterns and detection lags\n","- Root cause analysis goes beyond proximate causes to systemic vulnerabilities\n","- Detection gaps identify monitoring failures requiring system improvements\n","- Impact quantification enables prioritization of engineering resources\n","- Every incident must yield new tests—otherwise root causes weren't truly understood\n","- Follow-up actions with owners and deadlines ensure findings drive actual changes\n","- Simulated incidents validate that post-mortem templates are practical and complete\n","- Post-mortem artifacts become institutional memory preventing repeated failures\n","\n","Post-mortems transform failures from setbacks into learning opportunities. Without structured documentation, knowledge remains siloed in individuals who experienced the incident. With templates and examples, every failure enriches the organization's collective understanding of what can go wrong and how to prevent it. The best organizations aren't those that never fail—they're those that fail forward, ensuring each failure makes the next one less likely."],"metadata":{"id":"DzTW1njFmVW6"}},{"cell_type":"markdown","source":["###13.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"gb8_3viGmVx_"}},{"cell_type":"code","source":["\n","# Cell 13 — Post-Mortem Template + Example Incident\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 13: POST-MORTEM TEMPLATE + EXAMPLE INCIDENT\")\n","print(\"=\" * 80)\n","\n","# Post-mortem template\n","post_mortem_template = {\n","    \"incident_id\": \"<unique_id>\",\n","    \"incident_summary\": \"<brief description>\",\n","    \"timeline\": [\n","        {\"time\": \"<timestamp>\", \"event\": \"<event description>\"},\n","    ],\n","    \"root_cause\": \"<detailed root cause analysis>\",\n","    \"detection_gap\": \"<why was this not caught earlier?>\",\n","    \"impact\": {\n","        \"pnl_loss\": \"<quantify loss>\",\n","        \"duration\": \"<time span>\",\n","        \"affected_assets\": \"<list>\",\n","    },\n","    \"mitigations\": [\n","        \"<mitigation 1>\",\n","        \"<mitigation 2>\",\n","    ],\n","    \"tests_added\": [\n","        \"<new test 1>\",\n","        \"<new test 2>\",\n","    ],\n","    \"follow_up\": [\n","        {\"action\": \"<action item>\", \"owner\": \"<responsible person>\", \"deadline\": \"<date>\"},\n","    ],\n","}\n","\n","post_mortem_template_path = os.path.join(OUTPUT_DIR, \"post_mortem_template.json\")\n","write_json(post_mortem_template_path, post_mortem_template)\n","artifact_registry[\"artifact_files\"].append(\"post_mortem_template.json\")\n","\n","# Example incident: data missingness spike\n","incident_day = DRIFT_EVENT_STEP  # reuse drift event as incident trigger\n","incident_pnl_loss = np.sum(net_pnl_linear[incident_day:incident_day+10])\n","\n","post_mortem_example = {\n","    \"incident_id\": \"INC-2025-001\",\n","    \"incident_summary\": \"Data missingness spike caused by feature distribution shift, leading to unexpected losses.\",\n","    \"timeline\": [\n","        {\"time\": f\"Test day {incident_day}\", \"event\": \"Feature drift detected (z-shift > 3.0)\"},\n","        {\"time\": f\"Test day {incident_day+1}\", \"event\": \"Positions became erratic, turnover spiked\"},\n","        {\"time\": f\"Test day {incident_day+5}\", \"event\": \"Monitoring alert triggered (Amber state)\"},\n","        {\"time\": f\"Test day {incident_day+10}\", \"event\": \"Manual review initiated, losses quantified\"},\n","    ],\n","    \"root_cause\": \"Feature engineering pipeline did not handle regime-dependent distribution shifts. \"\n","                   \"The rolling z-score feature became unstable during regime transition, causing misaligned signals.\",\n","    \"detection_gap\": \"Monitoring system detected drift, but alert budget was too conservative. \"\n","                     \"Persistence threshold delayed escalation to Red state.\",\n","    \"impact\": {\n","        \"pnl_loss\": float(incident_pnl_loss),\n","        \"duration\": \"10 days (test period)\",\n","        \"affected_assets\": \"All 10 assets\",\n","    },\n","    \"mitigations\": [\n","        \"Added regime-aware feature normalization\",\n","        \"Lowered persistence threshold from 3 to 2\",\n","        \"Increased alert budget from 5 to 10 per 100 steps\",\n","        \"Implemented emergency kill switch for Red state\",\n","    ],\n","    \"tests_added\": [\n","        \"Regime transition robustness test (explicit state change simulation)\",\n","        \"Feature stability test under distribution shift\",\n","        \"Alert latency test (time from drift to Red state)\",\n","    ],\n","    \"follow_up\": [\n","        {\"action\": \"Update feature engineering to use regime-conditional normalization\", \"owner\": \"Quant Dev\", \"deadline\": \"2025-02-01\"},\n","        {\"action\": \"Backtest mitigation strategy on historical regime transitions\", \"owner\": \"Risk Manager\", \"deadline\": \"2025-02-15\"},\n","        {\"action\": \"Review alert budget across all models\", \"owner\": \"Model Owner\", \"deadline\": \"2025-02-10\"},\n","    ],\n","}\n","\n","post_mortem_example_path = os.path.join(OUTPUT_DIR, \"post_mortem_example.json\")\n","write_json(post_mortem_example_path, post_mortem_example)\n","artifact_registry[\"artifact_files\"].append(\"post_mortem_example.json\")\n","\n","print(f\"[POST-MORTEM] Example incident documented (INC-2025-001)\")\n","print(f\"[POST-MORTEM] PnL loss: {incident_pnl_loss:.6f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o78IljACRGjc","executionInfo":{"status":"ok","timestamp":1767120909522,"user_tz":360,"elapsed":50,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"a417b30b-42b3-407a-94ae-525e83d39928"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 13: POST-MORTEM TEMPLATE + EXAMPLE INCIDENT\n","================================================================================\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/post_mortem_template.json\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/post_mortem_example.json\n","[POST-MORTEM] Example incident documented (INC-2025-001)\n","[POST-MORTEM] PnL loss: -466.075823\n"]}]},{"cell_type":"markdown","source":["## 14.DOCUMENTATION"],"metadata":{"id":"sOFAuuE4nUFy"}},{"cell_type":"code","source":["\n","# Cell 14 — Transition to Chapter 22\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 14: TRANSITION TO CHAPTER 22\")\n","print(\"=\" * 80)\n","\n","transition_note = \"\"\"\n","TRANSITION TO CHAPTER 22: GOVERNANCE OBLIGATIONS AND REGULATORY FRAMING\n","\n","Chapter 21 has built a governance-native model risk toolkit, including:\n","- Explainability diagnostics (global, local, sensitivity)\n","- Robustness test suites (temporal, cross-sectional, microstructure, regime, adversarial)\n","- Stress testing for P&L and risk (hidden leverage, tail risk, drawdown decomposition, exposure)\n","- Robust optimization intuition (shrinkage, ensembles)\n","- Monitoring & degradation detection (causal online diagnostics, state machines)\n","- Model cards, data sheets, evaluation sheets, audit packs, and post-mortem templates\n","\n","Chapter 22 will formalize the governance obligations and regulatory framing for these artifacts:\n","- Mapping artifacts to regulatory requirements (e.g., SR 11-7, MiFID II, BCBS 239)\n","- Defining roles and responsibilities (model owner, validator, risk manager)\n","- Establishing review cycles and escalation paths\n","- Documenting compliance evidence and audit trails\n","\n","All artifacts generated in Chapter 21 are designed to be audit-ready and traceable,\n","providing the foundation for a robust model governance framework in Chapter 22.\n","\"\"\"\n","\n","print(transition_note)\n","\n","transition_note_path = os.path.join(OUTPUT_DIR, \"transition_to_ch22.txt\")\n","write_text(transition_note_path, transition_note)\n","artifact_registry[\"artifact_files\"].append(\"transition_to_ch22.txt\")\n","\n","\n","# Final Summary\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 80)\n","print(\"FINAL SUMMARY\")\n","print(\"=\" * 80)\n","\n","print(f\"\\n[RUN COMPLETE] Run ID: {RUN_ID}\")\n","print(f\"[OUTPUT DIR] {OUTPUT_DIR}\")\n","print(f\"[ARTIFACTS] Generated {len(artifact_registry['artifact_files'])} files:\")\n","for fname in artifact_registry[\"artifact_files\"]:\n","    print(f\"  - {fname}\")\n","\n","print(\"\\n[EVALUATION METRICS]\")\n","print(f\"  Model A (Linear) - Sharpe: {metrics_linear['sharpe']:.4f}, Max DD: {metrics_linear['max_drawdown']:.4f}\")\n","print(f\"  Model B (Rule) - Sharpe: {metrics_rule['sharpe']:.4f}, Max DD: {metrics_rule['max_drawdown']:.4f}\")\n","\n","print(\"\\n[ROBUSTNESS SUITE]\")\n","print(f\"  Total tests: {robustness_suite_report['summary']['total']}\")\n","print(f\"  Passed: {robustness_suite_report['summary']['passed']}\")\n","print(f\"  Failed: {robustness_suite_report['summary']['failed']}\")\n","\n","print(\"\\n[MONITORING]\")\n","print(f\"  Total alerts: {len(monitoring_run_log['alerts'])}\")\n","print(f\"  Final state: {current_state}\")\n","print(f\"  Alert rate: {alert_count_per_100:.2f} per 100 steps (budget: {alert_budget})\")\n","\n","print(\"\\n[AUDIT PACK]\")\n","print(f\"  Audit pack index: {audit_pack_path}\")\n","print(f\"  All artifacts hashed and indexed for traceability.\")\n","\n","print(\"\\n[DETERMINISM CHECK]\")\n","print(f\"  Config hash: {config_hash[:16]}...\")\n","print(f\"  Dataset hash: {dataset_hash[:16]}...\")\n","print(\"  Re-running with same seed will yield identical hashes.\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CHAPTER 21 NOTEBOOK COMPLETE\")\n","print(\"=\" * 80)\n","print(\"\\nAll artifacts saved to:\", OUTPUT_DIR)\n","print(\"Notebook run end-to-end successfully.\")\n","print(\"Ready for governance review and transition to Chapter 22.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hE7MCx-znXiV","executionInfo":{"status":"ok","timestamp":1767120930358,"user_tz":360,"elapsed":44,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"7773c4b3-0ff2-48c9-97db-aeabdbf17590"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","CELL 14: TRANSITION TO CHAPTER 22\n","================================================================================\n","\n","TRANSITION TO CHAPTER 22: GOVERNANCE OBLIGATIONS AND REGULATORY FRAMING\n","\n","Chapter 21 has built a governance-native model risk toolkit, including:\n","- Explainability diagnostics (global, local, sensitivity)\n","- Robustness test suites (temporal, cross-sectional, microstructure, regime, adversarial)\n","- Stress testing for P&L and risk (hidden leverage, tail risk, drawdown decomposition, exposure)\n","- Robust optimization intuition (shrinkage, ensembles)\n","- Monitoring & degradation detection (causal online diagnostics, state machines)\n","- Model cards, data sheets, evaluation sheets, audit packs, and post-mortem templates\n","\n","Chapter 22 will formalize the governance obligations and regulatory framing for these artifacts:\n","- Mapping artifacts to regulatory requirements (e.g., SR 11-7, MiFID II, BCBS 239)\n","- Defining roles and responsibilities (model owner, validator, risk manager)\n","- Establishing review cycles and escalation paths\n","- Documenting compliance evidence and audit trails\n","\n","All artifacts generated in Chapter 21 are designed to be audit-ready and traceable,\n","providing the foundation for a robust model governance framework in Chapter 22.\n","\n","[ARTIFACT] Written: /content/ch21_run_20251230_181425_8270/transition_to_ch22.txt\n","\n","================================================================================\n","FINAL SUMMARY\n","================================================================================\n","\n","[RUN COMPLETE] Run ID: 20251230_181425_8270\n","[OUTPUT DIR] /content/ch21_run_20251230_181425_8270\n","[ARTIFACTS] Generated 32 files:\n","  - config.json\n","  - run_manifest.json\n","  - dataset_summary.json\n","  - equity_curves.png\n","  - coefficients_global.png\n","  - contributions_local.png\n","  - sensitivity_distribution.png\n","  - explainability_pack.json\n","  - robustness_suite_report.json\n","  - robust_opt_report.json\n","  - robust_opt_comparison.png\n","  - model_card.txt\n","  - data_sheet.json\n","  - evaluation_sheet.json\n","  - audit_pack_index.json\n","  - minimum_artifact_table.txt\n","  - model_card.txt\n","  - data_sheet.json\n","  - evaluation_sheet.json\n","  - audit_pack_index.json\n","  - minimum_artifact_table.txt\n","  - monitoring_spec.json\n","  - monitoring_run_log.json\n","  - monitoring_alerts.png\n","  - model_card.txt\n","  - data_sheet.json\n","  - evaluation_sheet.json\n","  - audit_pack_index.json\n","  - minimum_artifact_table.txt\n","  - post_mortem_template.json\n","  - post_mortem_example.json\n","  - transition_to_ch22.txt\n","\n","[EVALUATION METRICS]\n","  Model A (Linear) - Sharpe: -0.7337, Max DD: -10238.3614\n","  Model B (Rule) - Sharpe: -0.3817, Max DD: -315.3364\n","\n","[ROBUSTNESS SUITE]\n","  Total tests: 5\n","  Passed: 0\n","  Failed: 5\n","\n","[MONITORING]\n","  Total alerts: 200\n","  Final state: Red\n","  Alert rate: 66.67 per 100 steps (budget: 5)\n","\n","[AUDIT PACK]\n","  Audit pack index: /content/ch21_run_20251230_181425_8270/audit_pack_index.json\n","  All artifacts hashed and indexed for traceability.\n","\n","[DETERMINISM CHECK]\n","  Config hash: 4e40adc773598dd9...\n","  Dataset hash: d078a6a14d38b6b5...\n","  Re-running with same seed will yield identical hashes.\n","\n","================================================================================\n","CHAPTER 21 NOTEBOOK COMPLETE\n","================================================================================\n","\n","All artifacts saved to: /content/ch21_run_20251230_181425_8270\n","Notebook run end-to-end successfully.\n","Ready for governance review and transition to Chapter 22.\n"]}]},{"cell_type":"markdown","source":["##15.CONCLUSIONS"],"metadata":{"id":"02G_tA0XnsUM"}},{"cell_type":"markdown","source":["\n","**The Journey from Model to Production-Ready System**\n","\n","This chapter has constructed a comprehensive model risk management framework that transforms raw quantitative research into governance-ready, auditable, production-grade trading systems. What began as simple linear predictors on synthetic data evolved through thirteen carefully orchestrated steps into a complete pipeline demonstrating explainability, robustness testing, stress analysis, defensive optimization, continuous monitoring, and systematic documentation. This isn't just academic exercise—it's the minimum viable governance infrastructure that separates hobbyist backtests from institutional-quality quantitative strategies.\n","\n","The pipeline we've built addresses the central problem of quantitative finance: models that perform brilliantly on historical data often fail catastrophically in live markets. This performance gap—what we've called \"alpha tax\"—arises from multiple sources: overfitting to specific historical conditions, estimation error in parameters, regime changes that invalidate learned relationships, transaction costs higher than assumed, and gradual model degradation as markets evolve. Traditional research stops at backtest results showing attractive Sharpe ratios. Professional risk management begins there, systematically probing every assumption and failure mode before risking real capital.\n","\n","Let's walk through the complete pipeline step by step, understanding what each stage accomplishes and why it matters.\n","\n","**Step 1: Deterministic Foundations and Configuration Management**\n","\n","We began by establishing determinism through explicit random seeds and comprehensive configuration dictionaries. Every parameter—from sample sizes to cost assumptions to robustness thresholds—lives in a structured CONFIG object that gets hashed cryptographically. This isn't bureaucracy; it's reproducibility infrastructure. When results diverge six months later, you can prove whether your code changed or your configuration changed or your data changed, because everything is versioned and traceable.\n","\n","The governance utilities layer—hash functions, artifact writers, run manifests—creates an immutable audit trail from the start. Every artifact generated gets hashed and logged in the manifest, creating cryptographic proof of what was produced when. This satisfies regulatory requirements and enables debugging: if a strategy fails, you can retrieve the exact configuration, code version, and data vintage that produced it.\n","\n","**Step 2: Synthetic Market Generation with Regime Awareness**\n","\n","Rather than depending on external data that may be unavailable, proprietary, or inconsistent across vendors, we generated synthetic multi-asset markets with explicit regime processes. This pedagogical choice provides complete control over statistical properties: we know the true volatility regime at every time step, the true correlation structure, and can inject specific stress events like tail shocks.\n","\n","The two-regime structure (low-volatility/low-correlation versus high-volatility/high-correlation) captures the essential non-stationarity of real markets—conditions change, and strategies must survive transitions. By generating correlation matrices that shift with regimes, we ensured our test data contains the clustering behavior observed in real markets where diversification benefits evaporate during crises. This synthetic environment becomes our laboratory for everything that follows, providing ground truth for validation.\n","\n","**Step 3: Causal Feature Engineering with Leakage Prevention**\n","\n","Feature construction implemented obsessive temporal discipline: every feature at time t uses only data through time t-1. We computed trailing momentum, volatility, rolling means, and z-scores using explicit loops with clear window boundaries, making the causality visible in code structure rather than hidden in library calls. Multiple assertions verified that features are NaN before windows fill and that no feature accesses future data.\n","\n","We also demonstrated a leaky feature example—using returns[t] as input at time t—to teach what not to do. This pedagogical approach matters because look-ahead bias is the most common and most pernicious error in quantitative research. By making causality explicit and testable, we built features that will work in production because they respect the fundamental constraint: you can only use information available at decision time.\n","\n","**Step 4: Baseline Model Development with Interpretability**\n","\n","Rather than immediately building complex machine learning systems, we implemented two transparent baselines: a regularized linear predictor and a simple rule-based momentum strategy. The linear model uses ridge regression—explicit closed-form solution with controlled shrinkage—rather than black-box optimizers. The rule-based strategy encodes market intuition (momentum works better in low-volatility regimes) in five lines of code anyone can understand.\n","\n","These baselines serve multiple purposes: sanity checks (if sophisticated models can't beat simple rules, something is wrong), benchmarks (complex models must justify their complexity through superior performance), and interpretability foundations (linear coefficients and simple rules are fully explainable). The train-test-embargo split with explicit temporal boundaries ensures out-of-sample evaluation mimics production deployment where test data is genuinely unavailable during training.\n","\n","**Step 5: Realistic Backtesting with Transaction Costs**\n","\n","The backtest engine implements the harsh reality check that destroys many promising strategies: transaction costs. We modeled both spread costs (linear in turnover) and market impact (quadratic in turnover), capturing that large trades move prices nonlinearly. Computing both gross and net PnL makes the alpha tax visible—many strategies have positive gross Sharpe but negative net Sharpe after costs.\n","\n","The one-period execution lag (decide at t based on data through t-1, execute at t, realize returns at t) reflects actual trading mechanics. Summary metrics include not just return statistics but turnover and maximum drawdown, recognizing that implementation matters as much as prediction accuracy. This backtest engine becomes the evaluation framework for all robustness testing—we stress-test by modifying its parameters and observing performance degradation.\n","\n","**Step 6: Explainability Diagnostics at Three Scales**\n","\n","Explainability tools make models debuggable rather than black boxes. Global explanations via coefficient magnitudes and rolling refit stability show which features matter overall and whether their importance is consistent or regime-dependent. Local explanations via per-feature contributions identify what drove decisions on specific days, particularly worst loss days where forensic analysis reveals failure modes.\n","\n","Sensitivity analysis via finite-difference perturbations tests fragility—robust models degrade gracefully under small input changes while brittle models flip predictions from microscopic noise. These three scales (global, local, sensitivity) provide complementary views: global shows average behavior, local shows specific instances, sensitivity shows robustness. Together they enable operators to understand what models do, why they fail, and whether failures are systemic or idiosyncratic.\n","\n","**Step 7: Robustness Testing Across Five Dimensions**\n","\n","The robustness suite systematically probes failure modes through explicit pass/fail gates. Temporal robustness via walk-forward analysis tests whether performance is consistent across time or period-specific. Cross-sectional robustness via random asset removal tests whether the strategy depends on specific instruments or generalizes across universes. Microstructure robustness via cost multipliers and latency tests reveals implementation sensitivity—can the strategy survive 3× higher costs or 2-step execution delays?\n","\n","Regime robustness via conditional performance ensures strategies work in both calm and turbulent markets, not just on average. Adversarial perturbations via feature noise, missingness, and outliers test whether models degrade gracefully under corruption. Each test has quantitative thresholds defined upfront (minimum Sharpe 0.5, maximum drawdown -15%, etc.), converting subjective assessment into objective gates. Strategies must pass all tests to be considered robust—partial success isn't acceptable when real capital is at stake.\n","\n","**Step 8: Stress Testing for Catastrophic Scenarios**\n","\n","Beyond robustness testing (which examines graceful degradation), stress testing explores catastrophic failures. Hidden leverage diagnostics via gross exposure reveal amplification that risk models might miss. Tail risk metrics (VaR and Expected Shortfall) capture fat-tailed distributions that standard deviation completely fails to measure. Drawdown decomposition identifies whether losses come from single events or accumulated deterioration, systemic factors or idiosyncratic shocks.\n","\n","Exposure decomposition via factor regression reveals unintended bets—a strategy claiming to be market-neutral might have substantial market beta. The scenario library explores synthetic disasters beyond historical data: vol shocks simulating market panic, correlation spikes destroying diversification, cost ramps simulating illiquidity. These scenarios answer the pre-mortem question: how could this strategy blow up? Finding vulnerabilities in simulation prevents discovering them with real money.\n","\n","**Step 9: Robust Optimization to Mitigate Estimation Error**\n","\n","Classical optimization produces strategies perfectly adapted to sample-specific noise. Robust optimization trades in-sample performance for out-of-sample stability through systematic defenses. Bootstrap resampling quantifies parameter uncertainty, showing which coefficients are reliably non-zero versus which are fitting noise. Shrinkage techniques—multiplying signals by factors less than 1.0—reduce position sizes based on the recognition that parameter estimates are wrong.\n","\n","Covariance shrinkage stabilizes correlation estimates that are notoriously unreliable from finite samples. Cost conservatism via 1.5-2× multipliers creates safety margins against model errors and regime changes. Turnover penalties implement stability-aware objectives that penalize excessive rebalancing. Ensemble methods diversify across model risk by combining multiple strategies, allowing uncorrelated estimation errors to cancel. These techniques embody a philosophical shift from maximization (find the optimal strategy) to satisficing (find a good-enough strategy that remains good-enough across many scenarios).\n","\n","**Step 10: Continuous Monitoring with Causal Online Diagnostics**\n","\n","Models degrade inevitably as markets evolve. Monitoring systems detect degradation early through three layers: input drift (feature distributions shifting), output drift (prediction and turnover changes), and outcome drift (realized costs and returns diverging from expectations). Each layer provides progressively definitive but later signals—input drift catches problems before bad predictions, output drift catches behavioral changes, outcome drift confirms actual damage.\n","\n","The state machine with persistence and hysteresis (Green/Amber/Red states with escalation rules) prevents alert fatigue while ensuring genuine problems escalate. Alert budgets limit false positive rates, making monitoring operationally sustainable. We validated effectiveness by injecting synthetic drift and verifying that monitoring detected it before significant losses—demonstrating early warning capability rather than just post-mortem documentation.\n","\n","**Step 11: Automated Documentation Artifact Generation**\n","\n","Rather than treating documentation as manual overhead, we generated artifacts automatically as code outputs. Model cards document purpose, scope, limitations, and risks—preventing inappropriate applications. Data sheets capture lineage, quality, and semantics—enabling diagnosis when data issues arise. Evaluation sheets make performance measurement transparent with explicit baselines, protocols, and cost assumptions.\n","\n","The audit pack index catalogs all artifacts with cryptographic hashes, providing immutable proof of completeness and authenticity. The minimum artifact table maps chapter concepts to concrete outputs, serving as a compliance checklist. These artifacts serve multiple audiences—researchers, risk managers, compliance officers, operators—creating a shared source of truth that ensures consistency across organizational silos.\n","\n","**Step 12: Post-Mortem Infrastructure for Systematic Learning**\n","\n","Failures are inevitable; learning from them is optional. We created post-mortem templates with structured fields (timeline, root cause, detection gaps, mitigations, tests added) ensuring complete forensic analysis. The worked example analyzing a simulated drift incident demonstrates the template's practicality while establishing expectations for depth of analysis—proximate causes aren't enough, systemic root causes must be identified.\n","\n","Post-mortems translate failures into improved systems through new tests and monitoring signals. Without structured documentation, institutional knowledge remains siloed in individuals; with templates, every failure enriches collective understanding of what can go wrong and how to prevent recurrence.\n","\n","**The Integrated Pipeline: More Than the Sum of Parts**\n","\n","Each step addresses a specific aspect of model risk, but their power emerges from integration. Explainability tools identify which features drive predictions; robustness testing verifies those features work across scenarios; stress testing explores extreme conditions; monitoring detects when relationships break down; post-mortems document what monitoring missed. Artifacts from each stage feed into others—monitoring thresholds come from stress test results, post-mortem improvements become new robustness tests, explainability findings inform feature engineering refinements.\n","\n","This integrated approach creates defense in depth—multiple layers catching different failure modes at different stages. A vulnerability that slips past robustness testing might be caught by stress testing. A regime change that survives stress scenarios might trigger monitoring alerts before significant losses. And failures that penetrate all defenses get documented in post-mortems that prevent recurrence.\n","\n","**From Chapter 21 to Production Reality**\n","\n","This chapter provides the technical infrastructure for model risk management, but production deployment requires organizational infrastructure too. Model validation teams must review artifacts and approve strategies before deployment. Risk management committees must set governance policies defining acceptable risk levels. Compliance teams must map artifacts to regulatory requirements. Operations teams must integrate monitoring systems into trading workflows with clear escalation procedures.\n","\n","Chapter 22 will address these governance obligations and regulatory frameworks, but the technical foundation built here is prerequisite—you can't have governance without artifacts to govern, monitoring without signals to track, or validation without tests to review. The thirteen steps documented here create the minimum viable technical layer on which institutional governance rests.\n","\n","**Final Reflections: Defensive Engineering as Discipline**\n","\n","The philosophy underlying this entire pipeline is defensive engineering—assume things will go wrong and build systems to detect problems before they become disasters. This mindset pervades every decision: use synthetic data to ensure reproducibility, implement causality assertions to prevent leakage, define pass/fail gates to make robustness objective, inject synthetic drift to validate monitoring, generate artifacts automatically to ensure completeness.\n","\n","Defensive engineering doesn't guarantee success, but it dramatically increases survival probability. Strategies built with this discipline may show lower backtested Sharpe ratios than aggressively optimized alternatives, but they're more likely to deliver those returns in production because they've been stress-tested, monitored, and documented. In quantitative finance, survival compounds—a strategy returning 8% annually for twenty years vastly outperforms one returning 15% for five years before collapsing.\n","\n","The complete pipeline—from deterministic foundations through synthetic data generation, causal features, transparent baselines, realistic backtesting, explainability, robustness testing, stress scenarios, robust optimization, continuous monitoring, automated documentation, and post-mortem infrastructure—represents the minimum viable approach to treating model risk as a first-class concern rather than an afterthought. It transforms quantitative research from intellectual exercise into production-ready systems that institutions can deploy with confidence, regulators can audit with trust, and operators can maintain over years of market evolution."],"metadata":{"id":"44Bw9WzfoFc9"}}]}