{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO6z2lCYC8TZHB5+M01mA4i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**REGULARIZATION, HYPERPARAMETERS AND MODEL SELECTION**\n","\n","---"],"metadata":{"id":"KaweNbpIPBoD"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"SncMXVZNQVm9"}},{"cell_type":"markdown","source":["https://claude.ai/share/d7351e4f-531f-4edc-80e7-4f3f1adf5846"],"metadata":{"id":"lK62ddyXQXIa"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"e1UJBQaxQXbD"}},{"cell_type":"markdown","source":["\n","\n","Welcome to Chapter 12, where we confront one of the most consequential decisions\n","in quantitative trading: how to select and tune predictive models without falling\n","into the trap of overfitting. This chapter builds a complete, production-grade\n","model selection framework that respects the unique challenges of financial time\n","series while maintaining the rigorous governance standards required in institutional\n","settings.\n","\n","**The Core Problem: Selection Under Uncertainty**\n","\n","Every quantitative trader faces this dilemma: you have multiple model specifications,\n","dozens of hyperparameter combinations, and various feature engineering choices.\n","Which configuration should you trust with real capital? The answer cannot be \"the\n","one with the best backtest\"—that path leads to models that shine in historical\n","data but crumble in live trading. Professional model selection requires sophisticated\n","validation protocols that honestly estimate out-of-sample performance while\n","preventing information leakage across time.\n","\n","**Why Financial Markets Demand Special Treatment**\n","\n","Unlike image classification or natural language processing, financial prediction\n","operates under strict temporal constraints. You cannot use tomorrow's information\n","to predict today's returns. This seems obvious, yet subtle violations of causality\n","pervade amateur trading systems: features computed with look-ahead bias, validation\n","sets contaminated by overlapping label periods, or scaling parameters fit on future\n","data. Each violation appears minor in isolation but compounds into systematic\n","overestimation of strategy performance—often discovered only after real losses.\n","\n","**The Governance-Native Philosophy**\n","\n","This notebook implements what we term \"governance-native\" development: every\n","operation produces an audit trail, every split enforces causality through explicit\n","purge and embargo periods, and every validation result links to cryptographic\n","hashes of the exact data and code that produced it. We build regularized models\n","(Ridge and Lasso regression) from first principles using only NumPy—no sklearn,\n","no pandas—to ensure complete transparency. When your model loses money or a\n","regulator questions your methodology, you need to trace every calculation. High-level\n","abstractions obscure this visibility.\n","\n","**What This Chapter Delivers**\n","\n","You will implement a complete model selection pipeline that professional quantitative\n","researchers actually use:\n","\n","- **Synthetic market generation** with regime-switching dynamics that mirror real\n","  market behavior without data licensing issues\n","- **Causal feature engineering** with timing proofs that verify no future information\n","  leaks into past predictions  \n","- **Walk-forward cross-validation** that respects temporal ordering and accounts\n","  for label overlap through purging and embargo\n","- **Stability-aware hyperparameter search** that penalizes configurations with high\n","  variance across folds, not just peak performance\n","- **Comprehensive diagnostics** including coefficient stability analysis, baseline\n","  comparisons, and sensitivity curves\n","- **Complete governance artifacts**: JSON manifests with SHA-256 hashes, JSONL\n","  trial ledgers, and decision-time logs suitable for regulatory audit\n","\n","**Bridge to Professional Practice**\n","\n","The techniques demonstrated here—purge/embargo protocols, nested cross-validation,\n","stability-penalized selection, coefficient path analysis—represent current best\n","practices at quantitative hedge funds and proprietary trading firms. These aren't\n","academic exercises; they're survival tools in an industry where the difference\n","between a promoted researcher and a fired one often comes down to proper validation\n","methodology.\n","\n","**For MBA and Master of Finance Students**\n","\n","This chapter challenges you to think beyond point estimates and p-values. You'll\n","learn to evaluate models through the lens of stability, interpretability, and\n","governance—qualities that matter more in production than marginal improvements\n","in validation metrics. The code is deliberately verbose and explicit, prioritizing\n","clarity and auditability over brevity. In professional quantitative finance,\n","readable code that you can defend to skeptical stakeholders beats clever\n","one-liners that save keystrokes but obscure logic.\n","\n","**The Path Forward**\n","\n","We begin with synthetic market generation to ensure reproducibility, then build\n","causal features with rigorous timing proofs, implement regularized models from\n","scratch, and finally conduct a deterministic hyperparameter search with complete\n","audit trails. Each section includes assertions that halt execution on causality\n","violations—because in trading, failing fast in development is infinitely preferable\n","to failing slowly with real capital.\n","\n","Welcome to professional-grade model selection. The stakes are real, the standards\n","are high, and the methodology is uncompromising."],"metadata":{"id":"SfzOXl9IQZGZ"}},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"Rz5_h6hrQjIs"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 2 — Imports, Seeds, and Artifact Directories\n","# ==========================================================\n","import os\n","import json\n","import math\n","import random\n","import hashlib\n","import datetime\n","from dataclasses import dataclass, asdict\n","from typing import Dict, List, Tuple, Any, Optional\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import rcParams\n","\n","# ---- Enhanced Visualization Settings (POLISHED) ----\n","# Set publication-quality defaults\n","rcParams['figure.figsize'] = (10, 6)\n","rcParams['figure.dpi'] = 100\n","rcParams['savefig.dpi'] = 300  # Publication quality\n","rcParams['font.size'] = 11\n","rcParams['axes.labelsize'] = 12\n","rcParams['axes.titlesize'] = 14\n","rcParams['xtick.labelsize'] = 10\n","rcParams['ytick.labelsize'] = 10\n","rcParams['legend.fontsize'] = 10\n","rcParams['figure.titlesize'] = 16\n","rcParams['axes.grid'] = True\n","rcParams['grid.alpha'] = 0.3\n","rcParams['grid.linestyle'] = '--'\n","rcParams['axes.spines.top'] = False\n","rcParams['axes.spines.right'] = False\n","\n","# Professional color palette (colorblind-friendly)\n","COLORS = {\n","    'primary': '#2E86AB',      # Blue\n","    'secondary': '#A23B72',    # Purple\n","    'success': '#06A77D',      # Green\n","    'warning': '#F18F01',      # Orange\n","    'danger': '#C73E1D',       # Red\n","    'neutral': '#6C757D',      # Gray\n","    'light': '#E9ECEF',        # Light gray\n","    'dark': '#212529'          # Dark\n","}\n","\n","BASELINE_COLORS = ['#8E44AD', '#E67E22', '#16A085']\n","\n","# ---- Determinism ----\n","MASTER_SEED = 12_120_001\n","np.random.seed(MASTER_SEED)\n","random.seed(MASTER_SEED)\n","\n","# ---- Artifact dirs ----\n","ART_DIR = \"/content/artifacts_ch12\"\n","PLOT_DIR = os.path.join(ART_DIR, \"plots\")\n","os.makedirs(ART_DIR, exist_ok=True)\n","os.makedirs(PLOT_DIR, exist_ok=True)\n","\n","\n","def now_utc_iso() -> str:\n","    \"\"\"Return current UTC timestamp in ISO format.\"\"\"\n","    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n"],"metadata":{"id":"l-AcWhF6Q5K2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.GOVERNANCE HELPER FUNCTIONS"],"metadata":{"id":"Lhv0b_VFQ_KS"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"UFS5Qu3XRSfG"}},{"cell_type":"markdown","source":["\n","This section establishes the governance infrastructure that makes every aspect of\n","the model selection workflow traceable, reproducible, and auditable. In professional\n","quantitative finance, you must be able to answer questions like \"Which exact dataset\n","produced this model?\" or \"Can you prove this validation run wasn't tampered with?\"\n","months or years after the fact. These helper functions create that capability.\n","\n","**Key Components**\n","\n","**Cryptographic Hashing Functions:**\n","The section provides two core hashing utilities. The first computes SHA-256 hashes\n","directly from byte sequences. The second handles JSON-serializable objects by first\n","converting them to canonical JSON strings (with sorted keys and no whitespace) before\n","hashing. These functions compute cryptographic fingerprints of data and configurations.\n","SHA-256 hashes act as tamper-proof identifiers—even a single bit change in input\n","produces a completely different hash. We use these to create lineage tracking: every\n","dataset, every configuration, and every model gets a unique identifier that proves\n","its provenance.\n","\n","**Deterministic JSON I/O:**\n","Two functions handle persistent storage. The save function writes objects as formatted\n","JSON with sorted keys and consistent indentation. The append function writes single-line\n","JSON records to JSONL files. The key detail is sorted keys, which ensures identical\n","dictionaries produce identical files regardless of Python version or insertion order,\n","enabling reproducible hashes across systems.\n","\n","**Time Assertions:**\n","The assert_monotone_time function verifies that time indices strictly increase with\n","no duplicates or reversals. This catches data loading errors and ensures causality\n","checks remain valid throughout the pipeline.\n","\n","**Data Fingerprinting:**\n","The data_fingerprint function creates comprehensive metadata about market data—summary\n","statistics, quantile distributions, NaN counts, and cryptographic hashes of the raw\n","price and return arrays. This bundle of information enables you to verify that a model\n","was trained on exactly the data you think it was.\n","\n","**Run Manifests:**\n","The write_run_manifest function generates a unique identifier for each execution run,\n","combining timestamp, seed, and code version into a deterministic hash. This manifest\n","links all artifacts from a single run together.\n","\n","**Governance Impact**\n","These utilities form the foundation for regulatory compliance and internal audit. When\n","challenged about a model's predictions or performance claims, you can cryptographically\n","prove your training data, configurations, and results haven't been modified post-hoc.\n","Every JSON file gets a hash, every dataset gets a fingerprint, and every run gets a\n","unique ID that ties the entire workflow together into an immutable audit trail.\n"],"metadata":{"id":"kSBjM_gaRVUv"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"1VfcmPh3RVn1"}},{"cell_type":"code","source":["\n","def sha256_bytes(b: bytes) -> str:\n","    \"\"\"Compute SHA256 hash of bytes.\"\"\"\n","    return hashlib.sha256(b).hexdigest()\n","\n","\n","def sha256_json(obj: Any) -> str:\n","    \"\"\"Compute SHA256 hash of JSON-serializable object.\"\"\"\n","    s = json.dumps(obj, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n","    return sha256_bytes(s)\n","\n","\n","def save_json(path: str, obj: Any) -> None:\n","    \"\"\"Save object as formatted JSON.\"\"\"\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(obj, f, indent=2, sort_keys=True)\n","\n","\n","def append_jsonl(path: str, obj: Any) -> None:\n","    \"\"\"Append object as JSON line to JSONL file.\"\"\"\n","    with open(path, \"a\", encoding=\"utf-8\") as f:\n","        f.write(json.dumps(obj, sort_keys=True) + \"\\n\")\n","\n","\n","def assert_monotone_time(t: np.ndarray) -> None:\n","    \"\"\"\n","    Assert time index is strictly increasing (no duplicates, no reversals).\n","\n","    Args:\n","        t: Time index array\n","\n","    Raises:\n","        AssertionError: If time is not strictly monotonic\n","    \"\"\"\n","    assert t.ndim == 1, \"Time index must be 1-dimensional\"\n","    if len(t) > 1:\n","        dt = np.diff(t)\n","        assert np.all(dt > 0), \"Time index must be strictly increasing.\"\n","\n","\n","def data_fingerprint(\n","    prices: np.ndarray,\n","    returns: np.ndarray,\n","    meta: Dict[str, Any]\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Generate deterministic fingerprint of market data.\n","\n","    Includes statistics, quantiles, and cryptographic hashes for reproducibility.\n","\n","    Args:\n","        prices: Price series\n","        returns: Return series\n","        meta: Metadata dict\n","\n","    Returns:\n","        Comprehensive fingerprint dict with hashes\n","    \"\"\"\n","    fp = {\n","        \"meta\": meta,\n","        \"n_obs\": int(len(prices)),\n","        \"price_min\": float(np.min(prices)),\n","        \"price_max\": float(np.max(prices)),\n","        \"ret_mean\": float(np.mean(returns)),\n","        \"ret_std\": float(np.std(returns)),\n","        \"ret_q\": {\n","            q: float(np.quantile(returns, q))\n","            for q in [0.01, 0.05, 0.5, 0.95, 0.99]\n","        },\n","        \"nan_prices\": int(np.isnan(prices).sum()),\n","        \"nan_returns\": int(np.isnan(returns).sum()),\n","    }\n","\n","    # Hash raw arrays (bytes) for lineage tracking\n","    fp[\"hash_prices_sha256\"] = sha256_bytes(prices.astype(np.float64).tobytes())\n","    fp[\"hash_returns_sha256\"] = sha256_bytes(returns.astype(np.float64).tobytes())\n","    fp[\"fingerprint_sha256\"] = sha256_json(fp)\n","\n","    return fp\n","\n","\n","def write_run_manifest(\n","    config: Dict[str, Any],\n","    code_id: str = \"colab_ch12_v1\"\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Create run manifest with deterministic ID and metadata.\n","\n","    Args:\n","        config: Configuration dict\n","        code_id: Code version identifier\n","\n","    Returns:\n","        Manifest dict with run metadata\n","    \"\"\"\n","    manifest = {\n","        \"run_id\": f\"ch12_{sha256_json({'seed': MASTER_SEED, 'ts': now_utc_iso(), 'code': code_id})[:12]}\",\n","        \"timestamp_utc_start\": now_utc_iso(),\n","        \"master_seed\": MASTER_SEED,\n","        \"code_identifier\": code_id,\n","        \"config_sha256\": sha256_json(config),\n","        \"python_version\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}\",\n","        \"numpy_version\": np.__version__,\n","    }\n","    return manifest\n"],"metadata":{"id":"2WMGLZ4FRnTh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4.SYNTHETIC MARKET GENERATOR"],"metadata":{"id":"dPm_NaoGSFZ9"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"TChgGOjfS_St"}},{"cell_type":"markdown","source":["\n","This section generates realistic synthetic market data with regime-switching dynamics,\n","eliminating dependencies on proprietary datasets while maintaining the statistical\n","properties that make model selection challenging. The generator produces price and\n","return series that exhibit regime changes, autocorrelation, volatility clustering,\n","occasional jumps, and microstructure noise—all features present in real financial\n","markets that stress-test model robustness.\n","\n","**Key Components**\n","\n","**SyntheticSpec Dataclass:**\n","This configuration container specifies all parameters for market generation: number\n","of observations, initial price, regime-switching probability, drift and volatility\n","parameters for each regime, AR(1) autocorrelation strength, jump probability and\n","magnitude, and microstructure noise level. Using a dataclass ensures all parameters\n","are explicitly documented and can be serialized for reproducibility.\n","\n","**Two-State Markov Chain:**\n","The generator implements a simple regime-switching model with two states representing\n","different market conditions. State 0 might represent normal markets with low volatility\n","and positive drift, while State 1 represents stressed markets with high volatility and\n","negative drift. Each day, the regime switches to the opposite state with fixed\n","probability (default 2%), creating realistic regime persistence.\n","\n","**Multi-Component Return Generation:**\n","Returns are constructed from five additive components. First, regime-dependent drift\n","provides different expected returns in each state. Second, regime-dependent volatility\n","scaled Gaussian shocks create the main return variation. Third, an AR(1) component\n","introduces weak autocorrelation by including a fraction of the previous shock. Fourth,\n","occasional jumps occur with small probability, adding fat tails to the return\n","distribution. Fifth, microstructure noise represents bid-ask bounce and other\n","high-frequency effects.\n","\n","**Price Path Construction:**\n","Prices are built from returns using exponential transformation—each day's price equals\n","the previous price multiplied by the exponential of the return. This ensures prices\n","remain positive and produces realistic log-normal price dynamics.\n","\n","**Causality Verification:**\n","The function concludes by asserting that the generated time index is strictly monotonic.\n","This assertion isn't redundant—it establishes a contract that all downstream functions\n","can rely on, making causality violations detectable immediately rather than producing\n","silently incorrect results.\n","\n","**Why Synthetic Data?**\n","Using synthetic data serves multiple pedagogical and practical purposes. Students can\n","experiment freely without data licensing costs or confidentiality concerns. The known\n","ground truth (regime states, parameter values) enables validation of feature engineering\n","and model selection procedures. Deterministic generation from fixed seeds ensures\n","perfect reproducibility across systems and time. Most importantly, synthetic data with\n","realistic properties provides a controlled environment to learn proper methodology\n","before applying it to real markets where mistakes cost money.\n","\n","**Professional Relevance**\n","Many quantitative firms use synthetic data generators for strategy development and\n","validation infrastructure testing. The ability to generate realistic market scenarios\n","on demand is invaluable for stress testing models under extreme but plausible conditions\n","that may not exist in historical data.\n","```"],"metadata":{"id":"2Ok6QELeTBtu"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"EkaeSDoVTCTP"}},{"cell_type":"code","source":["\n","@dataclass\n","class SyntheticSpec:\n","    \"\"\"Configuration for synthetic market data generation.\"\"\"\n","    n: int = 2500                                          # number of days\n","    s0: float = 100.0                                      # initial price\n","    p_switch: float = 0.02                                 # regime switch probability per day\n","    mu: Tuple[float, float] = (0.0002, -0.0001)           # drift per regime\n","    sigma: Tuple[float, float] = (0.008, 0.020)           # vol per regime\n","    phi: float = 0.10                                      # AR(1) component in returns (weak)\n","    jump_prob: float = 0.004                               # occasional jumps\n","    jump_scale: float = 0.04                               # jump magnitude scale\n","    micro_noise: float = 0.0006                            # microstructure-ish noise in returns\n","\n","\n","def generate_synthetic_market(spec: SyntheticSpec, seed: int) -> Dict[str, np.ndarray]:\n","    \"\"\"\n","    Generate synthetic market data with regime-switching returns.\n","\n","    Features:\n","    - Two-state Markov chain for regime switching\n","    - Different drift and volatility per regime\n","    - AR(1) component for autocorrelation\n","    - Occasional price jumps\n","    - Microstructure noise\n","\n","    Args:\n","        spec: Synthetic data specification\n","        seed: Random seed for reproducibility\n","\n","    Returns:\n","        Dict containing time index, prices, returns, and regime labels\n","    \"\"\"\n","    rng = np.random.default_rng(seed)\n","    n = spec.n\n","    t = np.arange(n, dtype=np.int64)  # simple integer time index (monotone)\n","\n","    # Generate regime sequence\n","    regimes = np.zeros(n, dtype=np.int64)\n","    for i in range(1, n):\n","        regimes[i] = regimes[i-1]\n","        if rng.random() < spec.p_switch:\n","            regimes[i] = 1 - regimes[i-1]\n","\n","    # Generate returns with regime-dependent parameters\n","    r = np.zeros(n, dtype=np.float64)\n","    eps_prev = 0.0\n","\n","    for i in range(1, n):\n","        reg = regimes[i]\n","        eps = rng.normal(0.0, spec.sigma[reg])\n","        ar = spec.phi * eps_prev\n","        drift = spec.mu[reg]\n","\n","        # Add occasional jumps\n","        jump = 0.0\n","        if rng.random() < spec.jump_prob:\n","            jump = rng.normal(0.0, spec.jump_scale) * (1.0 if rng.random() < 0.5 else -1.0)\n","\n","        # Add microstructure noise\n","        micro = rng.normal(0.0, spec.micro_noise)\n","\n","        r[i] = drift + eps + ar + jump + micro\n","        eps_prev = eps\n","\n","    # Construct price path from returns\n","    price = np.empty(n, dtype=np.float64)\n","    price[0] = spec.s0\n","    for i in range(1, n):\n","        price[i] = price[i-1] * math.exp(r[i])\n","\n","    assert_monotone_time(t)\n","\n","    return {\n","        \"t\": t,\n","        \"price\": price,\n","        \"ret\": r,\n","        \"regime\": regimes\n","    }\n","\n"],"metadata":{"id":"ZtkBa6vfTPES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##5.CAUSAL ROLLING HELPERS"],"metadata":{"id":"TdlFCqd54dxo"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"RRK9bsPh4hul"}},{"cell_type":"markdown","source":["\n","This section implements fundamental rolling window operations with strict causal\n","enforcement—features computed at time t use only information available at or before\n","time t. These functions form the building blocks for all feature engineering in the\n","notebook. The critical distinction from standard rolling operations is the explicit\n","causality guarantee: a spike added to future data cannot affect past feature values.\n","This prevents the subtle look-ahead bias that destroys countless trading strategies.\n","\n","**Key Components**\n","\n","**Rolling Mean (Causal):**\n","The rolling_mean_causal function computes the mean of the last L values up to and\n","including the current time point. It uses a cumulative sum approach for efficiency,\n","maintaining a running total and subtracting values that exit the window. Crucially,\n","it returns NaN for the first L-1 observations where a full window isn't available,\n","making the causality constraint explicit in the output. This prevents accidentally\n","using incomplete window statistics.\n","\n","**Rolling Standard Deviation (Causal):**\n","The rolling_std_causal function computes standard deviation over rolling windows.\n","Unlike the mean, this implementation uses explicit window extraction rather than\n","cumulative updating, trading some efficiency for clarity and numerical stability.\n","The ddof parameter controls degrees of freedom adjustment, with default 0 (population\n","standard deviation) suitable for feature engineering where we're describing observed\n","volatility rather than estimating population parameters.\n","\n","**Exponentially Weighted Moving Average (Causal):**\n","The ewma_causal function implements exponential smoothing with decay parameter alpha.\n","At each time point, the EWMA combines the current value with the previous EWMA using\n","the weighting alpha * current + (1 - alpha) * previous. This creates a feature that\n","responds to recent changes while maintaining memory of the past, with alpha controlling\n","the response speed. Higher alpha means faster adaptation to new information.\n","\n","**Causality Spike Test:**\n","The causality_spike_test function provides a rigorous verification mechanism. It takes\n","any feature function, applies it to original data, then applies it again to data with\n","a spike added at a specific future index. If the feature function is truly causal,\n","all values before the spike index must be identical in both runs. Any difference\n","indicates the function is illegally using future information. This is a timing proof—\n","mathematical verification that no look-ahead occurs.\n","\n","**Implementation Philosophy**\n","\n","**Explicit Over Implicit:**\n","These functions deliberately avoid clever optimizations that might obscure their\n","causality properties. The code prioritizes transparency and verifiability. In\n","production quantitative finance, being able to prove your features are causal matters\n","more than saving microseconds in computation time.\n","\n","**NaN Handling:**\n","Rather than padding with zeros or forward-filling during warmup periods, these\n","functions return NaN when insufficient history exists. This forces explicit handling\n","downstream and prevents silent errors where partially-formed features contaminate\n","model training.\n","\n","**No Pandas Dependency:**\n","By implementing these operations in pure NumPy, we maintain complete control over\n","the computation logic and eliminate dependencies on library functions whose internals\n","might change across versions. When debugging a failed strategy, you need to inspect\n","every calculation—black-box library calls become investigative dead ends.\n","\n","**Professional Significance**\n","The causality spike test represents a professional-grade verification approach rarely\n","seen in academic code. It transforms causality from an assumption into a tested\n","property. Quantitative hedge funds employ similar testing frameworks to catch\n","look-ahead bias before it reaches production, where it would silently bleed capital."],"metadata":{"id":"HS7PpDTp4jg4"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"2OaO9AlU4kBY"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 5 — Causal Rolling Helpers (No pandas)\n","# ==========================================================\n","def rolling_mean_causal(x: np.ndarray, L: int) -> np.ndarray:\n","    \"\"\"\n","    Compute causal rolling mean (uses only past L values).\n","\n","    Mean of last L values up to and including time t.\n","    Returns NaN until t >= L-1.\n","\n","    Args:\n","        x: Input array\n","        L: Window length\n","\n","    Returns:\n","        Rolling mean array (NaN for initial L-1 values)\n","\n","    Raises:\n","        ValueError: If L <= 0\n","    \"\"\"\n","    n = len(x)\n","    out = np.full(n, np.nan, dtype=np.float64)\n","\n","    if L <= 0:\n","        raise ValueError(\"Window length L must be positive.\")\n","\n","    s = 0.0\n","    for i in range(n):\n","        s += x[i]\n","        if i >= L:\n","            s -= x[i-L]\n","        if i >= L-1:\n","            out[i] = s / L\n","\n","    return out\n","\n","\n","def rolling_std_causal(x: np.ndarray, L: int, ddof: int = 0) -> np.ndarray:\n","    \"\"\"\n","    Compute causal rolling standard deviation.\n","\n","    Std of last L values up to and including time t.\n","    Returns NaN until t >= L-1.\n","\n","    Args:\n","        x: Input array\n","        L: Window length\n","        ddof: Delta degrees of freedom (default 0)\n","\n","    Returns:\n","        Rolling std array (NaN for initial L-1 values)\n","\n","    Raises:\n","        ValueError: If L <= 1\n","    \"\"\"\n","    n = len(x)\n","    out = np.full(n, np.nan, dtype=np.float64)\n","\n","    if L <= 1:\n","        raise ValueError(\"Window length L must be >= 2 for std.\")\n","\n","    for i in range(n):\n","        if i >= L-1:\n","            window = x[i-L+1:i+1]\n","            out[i] = float(np.std(window, ddof=ddof))\n","\n","    return out\n","\n","\n","def ewma_causal(x: np.ndarray, alpha: float) -> np.ndarray:\n","    \"\"\"\n","    Compute causal exponentially weighted moving average.\n","\n","    EWMA with alpha in (0,1]. out[t] uses x[0..t] only.\n","\n","    Args:\n","        x: Input array\n","        alpha: Smoothing parameter (0 < alpha <= 1)\n","\n","    Returns:\n","        EWMA array\n","    \"\"\"\n","    n = len(x)\n","    out = np.zeros(n, dtype=np.float64)\n","    out[0] = x[0]\n","\n","    for i in range(1, n):\n","        out[i] = alpha * x[i] + (1.0 - alpha) * out[i-1]\n","\n","    return out\n","\n","\n","def causality_spike_test(\n","    feature_fn,\n","    x: np.ndarray,\n","    *args,\n","    spike_index: int = 100,\n","    spike_value: float = 9.0\n",") -> None:\n","    \"\"\"\n","    Timing proof: verify feature function is causal.\n","\n","    A spike in the future should not affect feature outputs in the past.\n","    This catches accidental look-ahead in implementations.\n","\n","    Args:\n","        feature_fn: Function to test\n","        x: Input array\n","        *args: Additional arguments for feature_fn\n","        spike_index: Index where spike is added\n","        spike_value: Magnitude of spike\n","\n","    Raises:\n","        AssertionError: If causality is violated\n","    \"\"\"\n","    n = len(x)\n","    assert 0 <= spike_index < n, \"Spike index out of bounds\"\n","\n","    x0 = x.copy()\n","    x1 = x.copy()\n","    x1[spike_index] += spike_value\n","\n","    f0 = feature_fn(x0, *args)\n","    f1 = feature_fn(x1, *args)\n","\n","    # Past indices before spike must match exactly (NaNs allowed)\n","    for i in range(spike_index):\n","        a, b = f0[i], f1[i]\n","        if np.isnan(a) and np.isnan(b):\n","            continue\n","        assert abs(a - b) < 1e-12, \\\n","            f\"Causality failure: feature changed at i={i} due to future spike at {spike_index}.\"\n"],"metadata":{"id":"H8iIf6qC4naW","executionInfo":{"status":"ok","timestamp":1766504695370,"user_tz":360,"elapsed":64,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##6.FEATURE ENGINEERING"],"metadata":{"id":"RjJekCnH4m25"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"LDHP9xZs5Y2n"}},{"cell_type":"markdown","source":["\n","This section builds a complete feature set from raw price and return data while\n","maintaining strict causality. Features computed at end-of-day time t use only\n","information available through t, creating a realistic simulation of how features\n","would be constructed in live trading. The section also defines forward return\n","labels and assembles the final dataset with explicit validity masking. Every\n","feature passes causality spike tests to mathematically prove no look-ahead bias.\n","\n","**Key Components**\n","\n","**FeatureSpec Dataclass:**\n","This configuration container specifies all feature engineering parameters: fast\n","momentum window length, volatility estimation window, slow momentum window, EWMA\n","smoothing parameter, and a flag controlling whether EWMA features are included.\n","Centralizing these parameters makes the feature set reproducible and enables\n","systematic hyperparameter search over feature construction choices.\n","\n","**Build Features Function:**\n","The core build_features function constructs six distinct feature families. Fast\n","momentum uses rolling mean of returns over a short window (default 20 days) to\n","capture recent trend. Slow momentum uses a longer window (default 120 days) for\n","persistent trend detection. Volatility uses rolling standard deviation to quantify\n","recent price variation. EWMA provides exponentially-weighted trend with\n","configurable decay. Normalized returns (z-score) standardize each return by its\n","recent mean and volatility. Log-price deviation measures how far current price\n","deviates from its long-term rolling average.\n","\n","**Causality Verification:**\n","After constructing all features, the function performs spot-check causality tests\n","on the core rolling operations. It selects a test index (200 or near the end if\n","shorter), then verifies that rolling_mean_causal and rolling_std_causal produce\n","identical results before and after adding a spike beyond that index. These tests\n","run on every feature build, catching implementation errors immediately rather than\n","discovering them after weeks of model development.\n","\n","**Forward Return Labels:**\n","The forward_return_label function computes target variables by summing returns\n","over the next h days. Label y[i] represents the cumulative return from day i+1\n","through day i+h. This creates overlap: labels at consecutive time points share\n","h-1 return observations. Managing this overlap is critical—it's why the next\n","section implements purge and embargo. The function explicitly sets NaN for the\n","final h observations where forward returns cannot be computed, forcing downstream\n","code to handle these edge cases correctly.\n","\n","**Dataset Assembly:**\n","The assemble_dataset function converts the feature dictionary into a structured\n","NumPy matrix and creates a validity mask identifying samples with complete\n","information. A sample is valid only if both its target and all feature values\n","are finite (not NaN or infinite). This mask becomes the foundation for all\n","subsequent train/validation/test splitting, ensuring no split inadvertently\n","includes incomplete observations.\n","\n","**Design Philosophy**\n","\n","**Conservative Decision Time:**\n","The documentation explicitly states the decision time assumption: features computed\n","at EOD time t are assumed tradable at t+1 open. This conservative stance accounts\n","for computation time, order routing, and market microstructure. A more aggressive\n","assumption (tradable at t close) might work in backtesting but fail in live trading\n","where execution delays matter.\n","\n","**Feature Interpretability:**\n","All features have clear financial interpretations. Fast momentum captures swing\n","trading signals, slow momentum captures position trading trends, volatility informs\n","position sizing, and z-scored returns normalize for regime changes. This\n","interpretability aids model diagnosis when performance degrades—you can examine\n","coefficient paths to understand which market regimes the model exploits.\n","\n","**No Feature Scaling Here:**\n","Notably, this section does not scale features. Scaling happens later, within each\n","validation fold, to prevent leakage. Premature scaling using the full dataset would\n","contaminate validation by allowing test set statistics to influence training.\n","\n","**Professional Application**\n","This feature engineering approach mirrors industry practice at systematic trading\n","firms. Features are simple, interpretable, and fast to compute—critical properties\n","for production systems processing real-time market data. The causality testing\n","framework provides assurance that backtested performance won't evaporate in live\n","trading due to subtle timing violations."],"metadata":{"id":"k03pKsM-5bjP"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"V-1tAihT5czC"}},{"cell_type":"code","source":["\n","@dataclass\n","class FeatureSpec:\n","    \"\"\"Configuration for feature engineering.\"\"\"\n","    L_mom: int = 20              # Fast momentum window\n","    L_vol: int = 20              # Volatility window\n","    L_slow: int = 120            # Slow momentum window\n","    ewma_alpha: float = 0.08     # EWMA smoothing parameter\n","    use_ewma: bool = True        # Whether to include EWMA feature\n","\n","\n","def build_features(\n","    t: np.ndarray,\n","    price: np.ndarray,\n","    ret: np.ndarray,\n","    fs: FeatureSpec\n",") -> Dict[str, np.ndarray]:\n","    \"\"\"\n","    Build causal feature set from price and return data.\n","\n","    Features at time i use data up to and including i (EOD i).\n","    We assume forecasts computed at EOD i are tradable at i+1 open (conservative).\n","\n","    All features pass causality spike tests to ensure no look-ahead bias.\n","\n","    Args:\n","        t: Time index\n","        price: Price series\n","        ret: Return series\n","        fs: Feature specification\n","\n","    Returns:\n","        Dict of feature arrays with consistent naming\n","    \"\"\"\n","    n = len(ret)\n","\n","    # Momentum proxies\n","    mom_fast = rolling_mean_causal(ret, fs.L_mom)\n","    mom_slow = rolling_mean_causal(ret, fs.L_slow)\n","\n","    # Volatility proxy\n","    vol = rolling_std_causal(ret, fs.L_vol, ddof=0)\n","\n","    # EWMA trend proxy\n","    ew = ewma_causal(ret, fs.ewma_alpha) if fs.use_ewma else np.full(n, np.nan)\n","\n","    # Normalize returns (still causal)\n","    # Z-score of return using rolling mean/std (same window as vol)\n","    mu_r = rolling_mean_causal(ret, fs.L_vol)\n","    z = (ret - mu_r) / (vol + 1e-12)\n","\n","    # Price-based feature: log-price deviation from rolling mean\n","    logp = np.log(price + 1e-12)\n","    lp_ma = rolling_mean_causal(logp, fs.L_slow)\n","    lp_dev = logp - lp_ma\n","\n","    feats = {\n","        \"mom_fast\": mom_fast,\n","        \"mom_slow\": mom_slow,\n","        \"vol\": vol,\n","        \"ewma\": ew,\n","        \"zret\": z,\n","        \"lp_dev\": lp_dev,\n","    }\n","\n","    # Timing proofs on core rolling routines (spot-check once per feature build)\n","    test_idx = min(200, n-1)\n","    causality_spike_test(rolling_mean_causal, ret, fs.L_mom, spike_index=test_idx)\n","    causality_spike_test(rolling_std_causal, ret, fs.L_vol, 0, spike_index=test_idx)\n","\n","    return feats\n","\n","\n","def forward_return_label(ret: np.ndarray, h: int) -> np.ndarray:\n","    \"\"\"\n","    Compute forward return labels for prediction.\n","\n","    y[i] = sum_{k=1..h} ret[i+k]  (forward return over next h days)\n","    Undefined for i > n-h-1; set NaN there.\n","\n","    Args:\n","        ret: Return series\n","        h: Forward horizon (days)\n","\n","    Returns:\n","        Forward return labels (NaN for last h values)\n","    \"\"\"\n","    n = len(ret)\n","    y = np.full(n, np.nan, dtype=np.float64)\n","\n","    for i in range(n - h):\n","        y[i] = float(np.sum(ret[i+1:i+1+h]))\n","\n","    return y\n","\n","\n","def assemble_dataset(\n","    feats: Dict[str, np.ndarray],\n","    y: np.ndarray\n",") -> Tuple[np.ndarray, np.ndarray, List[str], np.ndarray]:\n","    \"\"\"\n","    Assemble feature matrix and identify valid samples.\n","\n","    Build X matrix and mask valid rows (no NaNs in X and y).\n","\n","    Args:\n","        feats: Dict of feature arrays\n","        y: Target array\n","\n","    Returns:\n","        Tuple of (X_full, y_full, feature_names, valid_mask)\n","    \"\"\"\n","    names = list(feats.keys())\n","    n = len(y)\n","    d = len(names)\n","\n","    X = np.zeros((n, d), dtype=np.float64)\n","    for j, nm in enumerate(names):\n","        X[:, j] = feats[nm]\n","\n","    # Identify valid samples (no NaNs in features or target)\n","    valid = np.isfinite(y)\n","    for j in range(d):\n","        valid &= np.isfinite(X[:, j])\n","\n","    return X, y, names, valid\n","\n"],"metadata":{"id":"pX5M2uRl5uiZ","executionInfo":{"status":"ok","timestamp":1766505204206,"user_tz":360,"elapsed":111,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##7.SPLIT PROTOCOLS WITH EMBARGO AND HARD ASSERTIONS"],"metadata":{"id":"upDkjqBv6pZa"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"OOcNRrfa6tqf"}},{"cell_type":"markdown","source":["\n","This section implements rigorous train/validation/test splitting protocols that respect\n","the temporal structure and overlapping nature of financial labels. Standard cross-\n","validation techniques fail catastrophically in time series prediction because they\n","ignore label overlap and allow information leakage across split boundaries. This\n","section provides the mathematical machinery to prevent such leakage through purging\n","and embargo periods, enforced by hard assertions that halt execution on violations.\n","\n","**Key Components**\n","\n","**SplitSpec Dataclass:**\n","This configuration specifies split boundaries for single-split evaluation (train ends\n","at index 1600, validation ends at 2100), walk-forward CV parameters (800-day training\n","windows, 200-day test windows, stepping forward by 200 days), and overlap controls\n","(5-day label horizon, 5-day embargo). It also documents the decision time assumption\n","as text for governance. All parameters become part of the audit trail.\n","\n","**Apply Purge and Embargo Function:**\n","This is the mathematical heart of leak-free splitting. Purging removes training samples\n","whose labels would overlap with the evaluation period. If a training sample at index i\n","has a label using returns through i+h, we require i+h to be strictly less than the\n","evaluation start—otherwise, the training label incorporates information from the\n","evaluation period. Embargo removes the first several samples from the evaluation set,\n","creating a buffer zone that reduces correlation between the last training sample and\n","first evaluation sample. This matters because financial returns exhibit autocorrelation,\n","and adjacent samples aren't truly independent.\n","\n","**Assert No Label Overlap:**\n","This assertion function verifies purging succeeded. It checks that the maximum training\n","index plus label horizon remains strictly less than the minimum evaluation index. If\n","this condition fails, an assertion error halts execution immediately. This aggressive\n","failure mode is intentional—continuing with contaminated splits would produce\n","misleading validation metrics that overestimate real performance.\n","\n","**Single Split Indices:**\n","The single_split_indices function creates train/validation/test indices for held-out\n","evaluation. It applies purge and embargo twice: first between train and validation\n","(used for hyperparameter tuning), then between the combined train+validation set and\n","the final test set (used for honest performance estimation). The function returns\n","four index sets: train for initial model fitting, valid for hyperparameter selection,\n","train_for_testfit for refitting the selected model on all available data before final\n","testing, and test for ultimate performance measurement.\n","\n","**Walk-Forward Folds:**\n","The walk_forward_folds function generates rolling cross-validation folds. It starts at\n","the beginning of the time series and creates overlapping windows: train on days 0-800,\n","test on days 800-1000, then step forward 200 days and repeat (train on 200-1000, test\n","on 1000-1200). Each fold applies purge and embargo independently, and causality\n","assertions verify each fold's integrity. The function includes defensive checks to\n","ensure sufficient samples remain after purging and requires at least three folds to\n","prevent degeneracy.\n","\n","**Mathematical Rigor**\n","\n","**Label Overlap Problem:**\n","Consider a 5-day forward return label at time 100, which sums returns from days 101-105.\n","If your validation set starts at day 103, this training label incorporates validation\n","period returns (days 103-105). Your model can learn patterns from validation data\n","through these overlapping labels, leading to overly optimistic validation performance.\n","Purging eliminates all training samples whose label windows intersect the validation\n","period.\n","\n","**Embargo Rationale:**\n","Even after purging, the last training sample and first validation sample sit adjacent\n","in time. If returns are autocorrelated, these samples share information through serial\n","dependence. Embargoing the first k validation samples creates temporal separation,\n","reducing this correlation. The embargo period represents a deliberate sacrifice—you\n","discard usable validation data to ensure independence.\n","\n","**Walk-Forward Realism:**\n","Walk-forward validation simulates how a strategy would be deployed over time. You train\n","on historical data, evaluate on the next period, then retrain with expanded history.\n","This captures model degradation, regime changes, and parameter stability in ways that\n","single-split validation cannot. The overlapping windows (stepping by 200 but training\n","on 800) create autocorrelation between folds, making stability analysis essential.\n","\n","**Professional Standards**\n","These protocols match what you'd find in research code at Two Sigma, Renaissance\n","Technologies, or Citadel. The purge/embargo framework comes from Marcos López de\n","Prado's work on financial machine learning, now considered industry standard. The\n","assertion-based verification ensures bugs surface during development rather than\n","after deployment with live capital.\n","\n","\n","**Split Protocols with Purge/Embargo — DETAILED WALKTHROUGH**\n","\n","**The Fundamental Confusion: What Are We Actually Doing?**\n","\n","Let me explain the COMPLETE process with concrete numbers.\n","\n","**SCENARIO 1: Single Split (Simple Case)**\n","\n","**THE ACTUAL STEPS:**\n","\n","**Step 1: We have 2600 days of data (days 0 to 2599)**\n","\n","**Step 2: We split into THREE periods:**\n","- Training: days 0 to 1599 (1600 days)\n","- Validation: days 1600 to 2099 (500 days)  \n","- Test: days 2100 to 2599 (500 days)\n","\n","**Step 3: Create labels (the target we're predicting)**\n","At day 100, our label is: sum of returns from days 101, 102, 103, 104, 105\n","At day 101, our label is: sum of returns from days 102, 103, 104, 105, 106\n","(Notice: these labels OVERLAP—they both use days 102, 103, 104, 105)\n","\n","**Step 4: THE LEAKAGE PROBLEM**\n","If we train on day 1596, its label uses returns from days 1597, 1598, 1599, 1600, 1601\n","BUT day 1600 is in our VALIDATION period!\n","This means our training label contains validation data—LEAKAGE!\n","\n","**Step 5: PURGE the training set**\n","We REMOVE all training days whose labels touch the validation period\n","So we remove days 1595, 1596, 1597, 1598, 1599 from training\n","Now training ACTUALLY ends at day 1594\n","\n","**Step 6: EMBARGO the validation set**\n","Even after purging, day 1594 (last training) and day 1600 (first validation) are close\n","Returns are autocorrelated—they influence each other\n","So we REMOVE the first 5 days of validation (days 1600-1604)\n","Validation now ACTUALLY starts at day 1605\n","\n","**Step 7: Now we can safely train**\n","- Fit Ridge regression on days 0-1594\n","- Predict on days 1605-2099 (validation)\n","- Measure how well we did (Information Coefficient, MSE, etc.)\n","\n","**SCENARIO 2: Walk-Forward Cross-Validation (Complex Case)**\n","\n","**WHY DO THIS?**\n","Single split gives you ONE number. What if that period was unusual? We need MULTIPLE\n","tests across different time periods to see if the model is STABLE.\n","\n","**THE ACTUAL PROCESS (with concrete numbers):**\n","\n","**FOLD 1:**\n","- Training window: days 0 to 799 (800 days)\n","- Test window: days 800 to 999 (200 days)\n","- Apply purge: remove training days 795-799 (their labels touch test period)\n","- Apply embargo: remove test days 800-804 (too close to training)\n","- ACTUAL training: days 0-794\n","- ACTUAL testing: days 805-999\n","- Fit Ridge on days 0-794, predict days 805-999\n","- Record IC = 0.045 (for example)\n","\n","**FOLD 2:**\n","- Step forward 200 days\n","- Training window: days 200 to 999 (800 days)\n","- Test window: days 1000 to 1199 (200 days)\n","- Apply purge: remove training days 995-999\n","- Apply embargo: remove test days 1000-1004\n","- ACTUAL training: days 200-994\n","- ACTUAL testing: days 1005-1199\n","- Fit Ridge on days 200-994, predict days 1005-1199\n","- Record IC = 0.038 (for example)\n","\n","**FOLD 3:**\n","- Step forward another 200 days\n","- Training window: days 400 to 1199 (800 days)\n","- Test window: days 1200 to 1399 (200 days)\n","- Apply purge: remove days 1195-1199\n","- Apply embargo: remove days 1200-1204\n","- ACTUAL training: days 400-1194\n","- ACTUAL testing: days 1205-1399\n","- Fit Ridge on days 400-1194, predict days 1205-1399\n","- Record IC = 0.052 (for example)\n","\n","**Continue this process... we might get 6-8 folds total**\n","\n","**FOLD 6:**\n","- Training: days 1000-1799\n","- Test: days 1800-1999\n","- After purge/embargo...\n","- Record IC = 0.025 (for example)\n","\n","**WHAT DO WE LEARN FROM WALK-FORWARD?**\n","\n","**Now we have multiple IC scores:**\n","- Fold 1: IC = 0.045\n","- Fold 2: IC = 0.038\n","- Fold 3: IC = 0.052\n","- Fold 4: IC = 0.041\n","- Fold 5: IC = 0.048\n","- Fold 6: IC = 0.025\n","\n","**Calculate stability metrics:**\n","- Mean IC = 0.042\n","- Standard deviation of IC = 0.009\n","\n","**This tells us:**\n","If mean is high but std is ALSO high → Model is unstable (good sometimes, bad others)\n","If mean is moderate but std is LOW → Model is stable (consistent performance)\n","\n","We use: Stability Score = Mean - (alpha × Std)\n","Where alpha = 0.5 (penalty for instability)\n","\n","For this model: 0.042 - (0.5 × 0.009) = 0.0375\n","**WHY IS THIS BETTER THAN ONE SPLIT?**\n","\n","**Single split says:** \"This model has IC = 0.045 on validation\"\n","**Walk-forward says:** \"This model has IC ranging from 0.025 to 0.052 across 6 different\n","time periods, with average 0.042 and std 0.009\"\n","\n","The second statement is MUCH more honest about real-world performance.\n","\n","\n","**THE HYPERPARAMETER SEARCH PROCESS**\n","\n","**We repeat the ENTIRE walk-forward process for EACH configuration:**\n","\n","**Configuration 1:** Ridge with lambda=0.001, fast_momentum=10 days\n","- Run 6-fold walk-forward\n","- Get ICs: [0.045, 0.038, 0.052, 0.041, 0.048, 0.025]\n","- Stability score = 0.0375\n","\n","**Configuration 2:** Ridge with lambda=0.01, fast_momentum=10 days\n","- Run 6-fold walk-forward\n","- Get ICs: [0.042, 0.040, 0.041, 0.039, 0.043, 0.038]\n","- Stability score = 0.0395\n","\n","**Configuration 3:** Ridge with lambda=0.1, fast_momentum=20 days\n","- Run 6-fold walk-forward\n","- Get ICs: [0.038, 0.037, 0.039, 0.037, 0.038, 0.036]\n","- Stability score = 0.0370\n","\n","**We pick Configuration 2** because it has the highest STABILITY SCORE (0.0395)\n","Even though Configuration 1 had a higher max IC (0.052), it was too variable.\n","\n","**THE COMPLETE WORKFLOW SUMMARY**\n","\n","1. Generate 2600 days of synthetic market data\n","2. Create features for all days (momentum, volatility, etc.)\n","3. Create labels (5-day forward returns)\n","4. Define 1000+ configurations to test (different lambdas, window lengths, etc.)\n","\n","**For EACH configuration:**\n","5. Run walk-forward CV (6-8 folds)\n","6. In each fold:\n","   - Purge training samples whose labels overlap test period\n","   - Embargo first few test samples\n","   - Fit model on purged training data\n","   - Predict on embargoed test data\n","   - Calculate IC for this fold\n","7. Calculate stability score from all fold ICs\n","8. Record this configuration's stability score\n","\n","9. After testing all 1000+ configurations, pick the one with highest stability score\n","10. This is our \"best\" model\n","11. Finally, test this best model on the held-out test set (days 2100-2599)\n","\n","**The test set result is our HONEST estimate of real-world performance**\n","\n","\n","**Why not just use the configuration with highest single-fold IC?**\n","Because that might be lucky—it happened to work well on one specific period.\n","\n","**Why walk-forward instead of random K-fold?**\n","Because time matters in finance. Training on 2020 and testing on 2019 is nonsense.\n","\n","**Why purge AND embargo?**\n","Purging handles label overlap (mathematical necessity)\n","Embargoing handles return autocorrelation (practical necessity)\n","\n","**Why stability score instead of mean IC?**\n","A model that's consistently mediocre is more valuable than one that's occasionally\n","brilliant but often terrible. Stability = deployability.\n","\n","**What's the computational cost?**\n","If you test 1000 configurations × 6 folds each = 6000 model fits\n","This is why the notebook shows progress indicators—it takes time!\n","\n"],"metadata":{"id":"vltg_M646vY4"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"hOPVi1ZR6v7v"}},{"cell_type":"code","source":["# ==========================================================\n","# Cell 7 — Split Protocols with Purge/Embargo + Hard Assertions\n","# ==========================================================\n","@dataclass\n","class SplitSpec:\n","    \"\"\"Configuration for time-series train/valid/test splits.\"\"\"\n","    # Single split boundaries (indices)\n","    train_end: int = 1600         # exclusive\n","    valid_end: int = 2100         # exclusive (test is [valid_end, n))\n","\n","    # Walk-forward / CV\n","    cv_train_len: int = 800       # Training window length\n","    cv_test_len: int = 200        # Test window length\n","    cv_step: int = 200            # Step size between folds\n","\n","    # Overlap controls\n","    label_horizon: int = 5        # Forward return horizon\n","    embargo: int = 5              # Number of points to embargo at start of eval block\n","\n","    # Decision-time definition (text)\n","    decision_time: str = \"features computed at EOD t, traded at t+1 open (conservative)\"\n","\n","\n","def apply_purge_and_embargo(\n","    train_idx: np.ndarray,\n","    eval_idx: np.ndarray,\n","    h: int,\n","    embargo: int\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"\n","    Apply purge and embargo rules to prevent label leakage.\n","\n","    Purge: Remove training indices whose label window overlaps eval block.\n","           If label at i uses returns up to i+h, then require i+h < eval_start.\n","    Embargo: Remove first 'embargo' indices of eval block to reduce boundary contamination.\n","\n","    Args:\n","        train_idx: Training indices\n","        eval_idx: Evaluation indices\n","        h: Label horizon\n","        embargo: Embargo period length\n","\n","    Returns:\n","        Tuple of (purged_train_idx, embargoed_eval_idx)\n","    \"\"\"\n","    assert train_idx.ndim == 1 and eval_idx.ndim == 1\n","    assert len(eval_idx) > 0 and len(train_idx) > 0\n","\n","    eval_start = int(np.min(eval_idx))\n","\n","    # Purge training points too close to eval start\n","    keep_train = train_idx[train_idx + h < eval_start]\n","\n","    # Embargo at start of eval block\n","    keep_eval = eval_idx[eval_idx >= eval_start + embargo]\n","\n","    return keep_train, keep_eval\n","\n","\n","def assert_no_label_overlap(\n","    train_idx: np.ndarray,\n","    eval_idx: np.ndarray,\n","    h: int\n",") -> None:\n","    \"\"\"\n","    Ensure training labels do not depend on returns inside eval region.\n","\n","    For any i in train_idx: label uses returns indices (i+1..i+h).\n","    Must be < eval_start.\n","\n","    Args:\n","        train_idx: Training indices\n","        eval_idx: Evaluation indices\n","        h: Label horizon\n","\n","    Raises:\n","        AssertionError: If overlap is detected\n","    \"\"\"\n","    eval_start = int(np.min(eval_idx))\n","\n","    if len(train_idx) == 0:\n","        return\n","\n","    max_i = int(np.max(train_idx))\n","    assert max_i + h < eval_start, \\\n","        \"Label overlap leakage: training labels reach into evaluation period.\"\n","\n","\n","def single_split_indices(\n","    n: int,\n","    valid_mask: np.ndarray,\n","    ss: SplitSpec\n",") -> Dict[str, np.ndarray]:\n","    \"\"\"\n","    Produce chronological train/valid/test indices with purge/embargo.\n","\n","    We apply purge/embargo:\n","      - Between train and valid\n","      - Between (train+valid) and test (final evaluation separation)\n","\n","    Args:\n","        n: Total number of samples\n","        valid_mask: Boolean mask of valid samples (no NaNs)\n","        ss: Split specification\n","\n","    Returns:\n","        Dict with keys: 'train', 'valid', 'train_for_testfit', 'test'\n","    \"\"\"\n","    assert 0 < ss.train_end < ss.valid_end < n\n","\n","    base_train = np.arange(0, ss.train_end, dtype=np.int64)\n","    base_valid = np.arange(ss.train_end, ss.valid_end, dtype=np.int64)\n","    base_test = np.arange(ss.valid_end, n, dtype=np.int64)\n","\n","    # Respect validity mask\n","    base_train = base_train[valid_mask[base_train]]\n","    base_valid = base_valid[valid_mask[base_valid]]\n","    base_test = base_test[valid_mask[base_test]]\n","\n","    # Purge/embargo train vs valid\n","    train1, valid1 = apply_purge_and_embargo(\n","        base_train, base_valid, ss.label_horizon, ss.embargo\n","    )\n","    assert_no_label_overlap(train1, valid1, ss.label_horizon)\n","\n","    # For final test: allow tuning on train+valid (but test must be protected)\n","    train_valid = np.concatenate([train1, valid1]).astype(np.int64)\n","    train2, test2 = apply_purge_and_embargo(\n","        train_valid, base_test, ss.label_horizon, ss.embargo\n","    )\n","    assert_no_label_overlap(train2, test2, ss.label_horizon)\n","\n","    return {\n","        \"train\": train1,\n","        \"valid\": valid1,\n","        \"train_for_testfit\": train2,\n","        \"test\": test2\n","    }\n","\n","\n","def walk_forward_folds(\n","    n: int,\n","    valid_mask: np.ndarray,\n","    ss: SplitSpec\n",") -> List[Dict[str, np.ndarray]]:\n","    \"\"\"\n","    Create rolling walk-forward folds for cross-validation.\n","\n","    Train window [k, k+train_len), test window [k+train_len, k+train_len+test_len)\n","    Step forward by cv_step.\n","    Each fold applies purge/embargo between train and test.\n","\n","    Args:\n","        n: Total number of samples\n","        valid_mask: Boolean mask of valid samples\n","        ss: Split specification\n","\n","    Returns:\n","        List of fold dicts with 'train', 'test', 'train_span', 'test_span'\n","    \"\"\"\n","    folds = []\n","    start = 0\n","\n","    while True:\n","        tr0 = start\n","        tr1 = start + ss.cv_train_len\n","        te0 = tr1\n","        te1 = tr1 + ss.cv_test_len\n","\n","        if te1 >= n:\n","            break\n","\n","        train = np.arange(tr0, tr1, dtype=np.int64)\n","        test = np.arange(te0, te1, dtype=np.int64)\n","\n","        train = train[valid_mask[train]]\n","        test = test[valid_mask[test]]\n","\n","        if len(train) == 0 or len(test) == 0:\n","            start += ss.cv_step\n","            continue\n","\n","        train_p, test_e = apply_purge_and_embargo(\n","            train, test, ss.label_horizon, ss.embargo\n","        )\n","\n","        if len(train_p) == 0 or len(test_e) == 0:\n","            start += ss.cv_step\n","            continue\n","\n","        assert_no_label_overlap(train_p, test_e, ss.label_horizon)\n","\n","        folds.append({\n","            \"train\": train_p,\n","            \"test\": test_e,\n","            \"train_span\": (tr0, tr1),\n","            \"test_span\": (te0, te1)\n","        })\n","        start += ss.cv_step\n","\n","    assert len(folds) >= 3, \\\n","        \"Too few folds; increase n or adjust cv parameters.\"\n","\n","    return folds\n","\n"],"metadata":{"id":"1D4DgHxW6z6V","executionInfo":{"status":"ok","timestamp":1766505305170,"user_tz":360,"elapsed":58,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##8.SCALING LEAKAGE PROBLEM"],"metadata":{"id":"m6-mV3MrECSz"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"J0EA7DzvEFCT"}},{"cell_type":"markdown","source":["\n","**The Scaling Leakage Problem**\n","\n","Most dangerous mistake in quantitative finance model validation:\n","\n","You have 2600 days of data. You compute:\n","- Mean of momentum feature across ALL 2600 days = 0.0023\n","- Std of momentum feature across ALL 2600 days = 0.0087\n","\n","You scale: scaled_momentum = (momentum - 0.0023) / 0.0087\n","\n","You split into train (days 0-1599) and test (days 1600-2599)\n","\n","WHAT JUST HAPPENED?\n","Your training data was scaled using statistics that include the test period!\n","The mean and std you computed included days 1600-2599.\n","This is LEAKAGE—test set information contaminated training.\n","\n","Why is this catastrophic?\n","Imagine test period has unusually high momentum (mean = 0.0045 instead of 0.0023).\n","By scaling with the full-dataset mean, you've already told your training model\n","that high momentum periods exist in the future. The model can exploit this knowledge\n","to artificially boost validation performance.\n","\n","In real trading: Your training data ends TODAY. You don't know tomorrow's statistics.\n","Scaling with future data is mathematically impossible in production.\n","\n","**The Correct Approach: Fold-Specific Scaling**\n","\n","THE IRON LAW:\n","Fit scaler parameters on TRAINING data ONLY.\n","Apply those parameters to BOTH training and test data.\n","\n","Example with walk-forward fold:\n","\n","Fold 1:\n","- Training: days 0-794\n","- Testing: days 805-999\n","\n","Step 1: Compute scaling parameters on days 0-794 ONLY\n","- mean_momentum = 0.0021 (computed from days 0-794)\n","- std_momentum = 0.0081 (computed from days 0-794)\n","\n","Step 2: Scale training data using these parameters\n","- scaled_train_momentum = (train_momentum - 0.0021) / 0.0081\n","\n","Step 3: Scale testing data using THE SAME parameters\n","- scaled_test_momentum = (test_momentum - 0.0021) / 0.0081\n","\n","CRITICAL: Test data is scaled using training statistics, not its own statistics.\n","\n","Fold 2:\n","- Training: days 200-994\n","- Testing: days 1005-1199\n","\n","Step 1: Compute NEW scaling parameters on days 200-994 ONLY\n","- mean_momentum = 0.0024 (different from Fold 1!)\n","- std_momentum = 0.0089 (different from Fold 1!)\n","\n","Step 2 and 3: Scale both train and test using these new parameters\n","\n","Each fold gets its own scaler, fit only on that fold's training data.\n","\n","**ScaleSpec Configuration**\n","\n","Three scaling methods implemented:\n","\n","Method 1: none\n","No scaling applied. Features used in raw form.\n","When to use: Features already on comparable scales (rare in finance)\n","Risk: Models sensitive to scale differences may perform poorly\n","\n","Method 2: zscore\n","Standard z-score normalization: (x - mean) / std\n","For each feature:\n","- Compute mean and std across training samples\n","- Scale: (feature - mean) / std\n","After scaling: training data has mean approximately 0, std approximately 1\n","Test data scaled with training parameters (so test means/stds won't be exactly 0/1)\n","When to use: Standard choice for most applications\n","Assumption: Features are roughly Gaussian distributed\n","\n","Method 3: robust_clip\n","Two-stage process designed for fat-tailed distributions:\n","\n","Stage 1 - Clip outliers:\n","- Compute median of feature (more robust than mean)\n","- Compute absolute deviations from median\n","- Find 99th percentile of absolute deviations as threshold\n","- Clip all values to median plus/minus threshold\n","\n","Stage 2 - Z-score the clipped features:\n","- Compute mean and std of clipped values\n","- Scale: (clipped_feature - mean) / std\n","\n","When to use: Financial data with extreme outliers (flash crashes, earnings surprises)\n","Benefit: Prevents single extreme values from dominating the scaling\n","Trade-off: Throws away information in the tails (might be valuable!)\n","\n","**Implementation: fit_scaler Function**\n","\n","Inputs:\n","- X: Full feature matrix (n_samples × n_features)\n","- idx_train: Indices of TRAINING samples only\n","- sc: ScaleSpec configuration\n","\n","Process:\n","1. Extract ONLY training samples\n","2. Compute scaling parameters from training samples\n","3. Store parameters in dictionary with metadata\n","4. CRITICALLY: Record min and max training indices for verification\n","\n","Output dictionary for zscore method contains:\n","- method name\n","- mean for each feature\n","- std for each feature  \n","- minimum training index\n","- maximum training index\n","\n","Why store min/max indices?\n","For verification! We'll check that max_train_index < min_eval_index to prove no leakage.\n","\n","**Implementation: apply_scaler Function**\n","\n","Inputs:\n","- X: Feature matrix to scale (could be train, validation, or test)\n","- scaler: Dictionary from fit_scaler containing parameters\n","\n","Process:\n","For zscore:\n","- Subtract stored mean, divide by stored std\n","\n","For robust_clip:\n","- First clip using stored median and threshold\n","- Then subtract mean and divide by std\n","\n","Key point: Same scaler applied to both training and evaluation data.\n","\n","**Implementation: assert_scaler_fit_only_on_train Function**\n","\n","This is the verification gate that prevents leakage.\n","\n","Process:\n","1. Extract evaluation's minimum index\n","2. Extract scaler's maximum training index\n","3. Assert: max_train < min_eval\n","\n","If this assertion fails:\n","The scaler was fit using indices that overlap with or come after evaluation.\n","This is LEAKAGE. Execution halts immediately with an error.\n","\n","Example that PASSES:\n","- Scaler fit on indices 0-794\n","- Evaluation on indices 805-999\n","- Check: 794 < 805 ✓ PASS\n","\n","Example that FAILS:\n","- Scaler fit on indices 0-850\n","- Evaluation on indices 805-999\n","- Check: 850 < 805 ✗ FAIL — Scaler used evaluation period data!\n","\n","This assertion is a MATHEMATICAL PROOF of proper temporal separation.\n","\n","**Complete Workflow Example**\n","\n","Walk-forward Fold 1:\n","\n","Training indices: 0 to 794\n","Test indices: 805 to 999\n","\n","Step 1: Fit scaler on training\n","Creates dictionary with means, stds, and index bounds\n","\n","Step 2: Verify no leakage\n","Check that 794 < 805 ✓ PASS\n","\n","Step 3: Apply scaler to training data\n","Training features now have mean approximately 0, std approximately 1\n","\n","Step 4: Apply SAME scaler to test data\n","Test features scaled using TRAINING statistics\n","Test means will NOT be exactly 0 (probably 0.0012 or similar)\n","Test stds will NOT be exactly 1 (probably 1.04 or similar)\n","\n","This asymmetry is CORRECT and NECESSARY.\n","\n","**Walk-forward Fold 2 (New Scaler!)**\n","\n","Training indices: 200 to 994\n","Test indices: 1005 to 1199\n","\n","Step 1: Fit NEW scaler on NEW training window\n","Creates new dictionary with DIFFERENT means and stds from Fold 1\n","\n","Step 2: Verify no leakage\n","Check that 994 < 1005 ✓ PASS\n","\n","Step 3 and 4: Scale both train and test with new scaler\n","\n","CRITICAL INSIGHT:\n","Fold 1 and Fold 2 use DIFFERENT scalers with DIFFERENT parameters.\n","This is correct! Each fold simulates an independent training process.\n","In real trading, you'd retrain periodically with new data, recomputing scaling\n","parameters each time.\n","\n","**Why Robust Clipping Matters in Finance**\n","\n","Standard zscore problem with outliers:\n","\n","Suppose on day 342 in training there's a flash crash:\n","- Momentum feature = -0.15 (extreme outlier, 50 standard deviations!)\n","\n","Computing mean and std with this outlier:\n","- mean becomes slightly negative (pulled by outlier)\n","- std becomes very large (inflated by outlier)\n","\n","Result: ALL other days get scaled to tiny values near 0\n","The flash crash day dominates the scaling, obscuring normal variation\n","\n","Robust clipping solution:\n","\n","Step 1: Compute median = 0.0021 (unaffected by outlier)\n","Step 2: Compute absolute deviations from median\n","Step 3: Find 99th percentile of deviations = 0.025\n","Step 4: Clip flash crash value: -0.15 becomes -0.025\n","Step 5: Now compute mean and std on clipped data\n","\n","Result: Scaling reflects typical variation, extreme outlier contained\n","Normal days get reasonable scaled values\n","Flash crash day gets maximum negative scaled value but doesn't dominate\n","\n","Trade-off:\n","You lose information about the MAGNITUDE of extreme events\n","You gain robustness against having a few extreme events destroy your model\n","\n","In practice: Test both methods via cross-validation and let data decide\n","\n","**Common Student Mistakes**\n","\n","Mistake 1: Scaling before splitting\n","Compute mean and std on ALL data, then split into train/test\n","LEAKAGE! Test statistics contaminated training.\n","\n","Mistake 2: Fitting scaler on validation data\n","In hyperparameter tuning, fitting scaler on train plus valid\n","This leaks validation statistics into training\n","\n","Mistake 3: Forgetting to scale test data\n","Scale training data, train model, then predict on unscaled test data\n","Model expects scaled inputs, gets raw inputs, produces nonsense\n","\n","Mistake 4: Refitting scaler on test data\n","Fit one scaler on training, then fit another scaler on test\n","Test data should use training scaler, not its own\n","\n","**Professional Standards**\n","\n","Why assertions instead of warnings?\n","\n","Some frameworks issue warnings about possible leakage then continue execution.\n","\n","This notebook HALTS on leakage.\n","\n","Why? In quantitative finance, subtle leakage isn't a minor issue—it's catastrophic.\n","A model with 2% leakage might show IC = 0.08 in validation but IC = 0.01 in production.\n","That 0.07 difference represents millions in lost capital.\n","\n","Better to fail loudly during development than silently in production.\n","\n","The assert function is a CIRCUIT BREAKER.\n","It mathematically proves temporal separation or kills the run.\n","\n","**Key Takeaways**\n","\n","1. Always fit scalers on training data only, never on combined or test data\n","2. Apply the training-fit scaler to test data, don't refit on test\n","3. Each fold in cross-validation needs its own scaler, fit on that fold's training\n","4. Test data will not have mean=0, std=1 after scaling—this is correct\n","5. Assertions verify temporal separation, proving no future information leaked\n","6. Robust clipping protects against outliers in fat-tailed financial distributions\n","7. In production, you'd recompute scaler parameters periodically as new data arrives\n","\n","Scaling seems trivial but is actually one of the most dangerous steps for introducing\n","leakage. This section implements the professional-grade solution used at quantitative\n","hedge funds to ensure validation metrics are honest estimates of production performance.\n"],"metadata":{"id":"1mYmpNe9EHv2"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"yY7umb_gEIU2"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 8 — Leakage-Safe Scaling (Fit on train only) + Assertions\n","# ==========================================================\n","@dataclass\n","class ScaleSpec:\n","    \"\"\"Configuration for feature scaling.\"\"\"\n","    method: str = \"zscore\"      # \"none\" | \"zscore\" | \"robust_clip\"\n","    clip_q: float = 0.99        # Used only in robust_clip\n","\n","\n","def fit_scaler(\n","    X: np.ndarray,\n","    idx_train: np.ndarray,\n","    sc: ScaleSpec\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Fit scaler on training data only (no leakage).\n","\n","    Args:\n","        X: Feature matrix\n","        idx_train: Training indices\n","        sc: Scaling specification\n","\n","    Returns:\n","        Dict with scaler parameters and metadata\n","    \"\"\"\n","    assert idx_train.ndim == 1\n","\n","    if sc.method == \"none\":\n","        return {\"method\": \"none\"}\n","\n","    Xtr = X[idx_train]\n","\n","    if sc.method == \"zscore\":\n","        mu = np.mean(Xtr, axis=0)\n","        sd = np.std(Xtr, axis=0) + 1e-12\n","        return {\n","            \"method\": \"zscore\",\n","            \"mu\": mu,\n","            \"sd\": sd,\n","            \"fit_idx_min\": int(np.min(idx_train)),\n","            \"fit_idx_max\": int(np.max(idx_train))\n","        }\n","\n","    if sc.method == \"robust_clip\":\n","        # Fit clip thresholds on TRAIN ONLY (no leakage)\n","        # Clip each feature to +/- q-quantile of |x - median|\n","        med = np.median(Xtr, axis=0)\n","        abs_dev = np.abs(Xtr - med)\n","        thr = np.quantile(abs_dev, sc.clip_q, axis=0) + 1e-12\n","\n","        # After clipping, z-score on train\n","        Xc = np.clip(Xtr, med - thr, med + thr)\n","        mu = np.mean(Xc, axis=0)\n","        sd = np.std(Xc, axis=0) + 1e-12\n","\n","        return {\n","            \"method\": \"robust_clip\",\n","            \"med\": med,\n","            \"thr\": thr,\n","            \"mu\": mu,\n","            \"sd\": sd,\n","            \"clip_q\": float(sc.clip_q),\n","            \"fit_idx_min\": int(np.min(idx_train)),\n","            \"fit_idx_max\": int(np.max(idx_train))\n","        }\n","\n","    raise ValueError(f\"Unknown scaling method: {sc.method}\")\n","\n","\n","def apply_scaler(X: np.ndarray, scaler: Dict[str, Any]) -> np.ndarray:\n","    \"\"\"\n","    Apply fitted scaler to features.\n","\n","    Args:\n","        X: Feature matrix to scale\n","        scaler: Fitted scaler dict from fit_scaler()\n","\n","    Returns:\n","        Scaled feature matrix\n","    \"\"\"\n","    m = scaler[\"method\"]\n","\n","    if m == \"none\":\n","        return X.copy()\n","\n","    if m == \"zscore\":\n","        return (X - scaler[\"mu\"]) / scaler[\"sd\"]\n","\n","    if m == \"robust_clip\":\n","        med = scaler[\"med\"]\n","        thr = scaler[\"thr\"]\n","        Xc = np.clip(X, med - thr, med + thr)\n","        return (Xc - scaler[\"mu\"]) / scaler[\"sd\"]\n","\n","    raise ValueError(f\"Unknown scaler method: {m}\")\n","\n","\n","def assert_scaler_fit_only_on_train(\n","    scaler: Dict[str, Any],\n","    idx_eval: np.ndarray\n",") -> None:\n","    \"\"\"\n","    Assert scaler was fit only on data before evaluation period.\n","\n","    For chronological splits we can assert max(train_idx) < min(eval_idx).\n","\n","    Args:\n","        scaler: Fitted scaler dict\n","        idx_eval: Evaluation indices\n","\n","    Raises:\n","        AssertionError: If scaler leakage is detected\n","    \"\"\"\n","    if scaler[\"method\"] == \"none\":\n","        return\n","\n","    eval_min = int(np.min(idx_eval))\n","    assert int(scaler[\"fit_idx_max\"]) < eval_min, \\\n","        \"Scaler leakage: scaler fit uses indices that reach into eval.\"\n"],"metadata":{"id":"uUzo6msYEW-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##9.REGULARIZED MODELS"],"metadata":{"id":"LiD4nBiXFnBs"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"Aw1MfkusFt3C"}},{"cell_type":"markdown","source":["\n","This section implements Ridge and Lasso regression from scratch using only NumPy,\n","providing complete transparency into how regularized linear models work. We build\n","these models from mathematical foundations rather than using sklearn, ensuring every\n","calculation is explicit, auditable, and verifiable. Students learn exactly what happens\n","inside the black box, and practitioners gain confidence that no hidden library quirks\n","can contaminate results.\n","\n","**ModelSpec Configuration**\n","\n","The ModelSpec dataclass specifies all model training parameters:\n","\n","- kind: Either \"ridge\" or \"lasso\" to select regularization type\n","- lam: Regularization strength (lambda parameter)\n","- lasso_max_iter: Maximum iterations for coordinate descent (Lasso only)\n","- lasso_tol: Convergence tolerance for Lasso optimization\n","\n","This configuration ensures reproducibility—same ModelSpec always produces same model.\n","\n","**Ridge Regression Implementation**\n","\n","Ridge regression solves: minimize prediction error plus penalty on coefficient magnitude\n","\n","Objective function:\n","minimize: sum of (actual_return - predicted_return)² + lambda × sum of (coefficient²)\n","\n","The lambda parameter controls regularization strength. Larger lambda means stronger\n","penalty on large coefficients, forcing them toward zero.\n","\n","Mathematical formulation:\n","minimize: ||y - (b + Xw)||² + λ||w||²\n","\n","Where:\n","- y = actual returns (what we're predicting)\n","- X = feature matrix (momentum, volatility, etc.)\n","- w = coefficients (what we're solving for)\n","- b = intercept (not penalized)\n","- λ = regularization strength\n","\n","Key implementation details:\n","\n","Step 1: Center the data\n","Subtract mean from features and target. This separates intercept optimization from\n","coefficient optimization. The intercept becomes simply the mean of y, and we can solve\n","for w independently.\n","\n","Step 2: Closed-form solution\n","The Ridge solution has a beautiful closed form:\n","w = (X^T X + λI)^(-1) X^T y\n","\n","Where I is the identity matrix (diagonal of ones).\n","\n","This is Ridge's huge advantage: NO ITERATION REQUIRED. We solve directly via matrix\n","inversion. Fast, stable, guaranteed to converge.\n","\n","Step 3: Recover intercept\n","After finding optimal w, compute:\n","b = mean(y) - mean(X) @ w\n","\n","This ensures predictions have correct mean even though we centered during optimization.\n","\n","The function returns a dictionary containing:\n","- kind: \"ridge\"\n","- w: coefficient vector\n","- b: intercept\n","- lam: regularization strength used\n","\n","Why return a dictionary instead of an object?\n","JSON serialization! These dictionaries go straight into audit trail files without\n","custom serialization logic.\n","\n","**What Does \"+ λI\" Do Mathematically?**\n","\n","Standard linear regression inverts X^T X, which can fail if:\n","- Features are perfectly correlated (multicollinearity)\n","- More features than samples (p > n)\n","- Numerical precision issues\n","\n","Adding λI to the diagonal makes the matrix invertible even in these problematic cases.\n","\n","Geometric interpretation:\n","The λI term adds a \"ridge\" along the diagonal of X^T X. This ridge ensures the matrix\n","has full rank and can always be inverted. Hence the name \"Ridge regression.\"\n","\n","Practical benefit:\n","Ridge ALWAYS has a unique solution. No convergence issues, no initialization\n","sensitivity, no numerical instabilities.\n","\n","**Lasso Regression Implementation**\n","\n","Lasso uses L1 penalty instead of L2:\n","minimize: ||y - (b + Xw)||² + λ||w||₁\n","\n","Where ||w||₁ = sum of absolute values of coefficients\n","\n","Key difference from Ridge:\n","L1 penalty drives some coefficients EXACTLY to zero, performing automatic feature\n","selection. Ridge shrinks all coefficients but rarely zeros them.\n","\n","Why Lasso is harder:\n","No closed-form solution exists! We must use iterative optimization.\n","\n","Implementation: Coordinate Descent\n","\n","The algorithm updates one coefficient at a time, cycling through all coefficients\n","repeatedly until convergence.\n","\n","Initialization:\n","- Set intercept b = mean(y)\n","- Set all coefficients w_j = 0\n","- Precompute feature norms for efficiency\n","\n","Main loop (repeat until convergence):\n","\n","For each feature j:\n","1. Compute partial residual: what remains unexplained after removing feature j's\n","   contribution\n","2. Compute correlation between feature j and partial residual\n","3. Apply soft-thresholding operator\n","4. Update coefficient w_j\n","\n","After updating all coefficients:\n","- Recompute intercept to maintain correct mean\n","- Check if coefficients changed significantly\n","- If change < tolerance, declare convergence\n","\n","Soft-Thresholding Operator:\n","This is Lasso's secret sauce—the function that drives coefficients to exactly zero.\n","\n","Given a value z and threshold g:\n","- If z > g: return z - g\n","- If z < -g: return z + g  \n","- If |z| ≤ g: return 0\n","\n","Effect: Small coefficients get zeroed out completely. Large coefficients get shrunk.\n","\n","Convergence criteria:\n","Stop when maximum absolute change in any coefficient falls below tolerance (default 1e-7).\n","This ensures high precision in final coefficient values.\n","\n","The function returns a dictionary containing:\n","- kind: \"lasso\"\n","- w: coefficient vector (may have exact zeros)\n","- b: intercept\n","- lam: regularization strength\n","- iters: number of iterations to converge\n","- tol: tolerance used\n","\n","**Prediction**\n","\n","The predict_linear function applies a fitted model to new data:\n","\n","prediction = b + X @ w\n","\n","For each sample:\n","- Multiply each feature by its coefficient\n","- Sum these products\n","- Add intercept\n","\n","This works identically for both Ridge and Lasso—the model type only affects training,\n","not prediction.\n","\n","**Ridge vs Lasso: When to Use Each**\n","\n","Ridge (L2):\n","- Shrinks all coefficients proportionally\n","- Never produces exact zeros\n","- Has closed-form solution (fast)\n","- Works well when all features somewhat relevant\n","- Better for correlated features (common in finance)\n","- Numerically stable always\n","\n","Typical Ridge coefficients:\n","[2.1, -3.2, 1.8, -1.4, 1.6, -2.0]\n","All features retained, coefficients shrunk\n","\n","Lasso (L1):\n","- Can drive coefficients to exact zero\n","- Performs automatic feature selection\n","- Requires iterative optimization (slower)\n","- Works well when many features irrelevant\n","- Can be unstable with highly correlated features\n","- Produces sparse models (fewer nonzero coefficients)\n","\n","Typical Lasso coefficients:\n","[2.8, 0.0, 0.0, -1.9, 2.1, 0.0]\n","Three features eliminated, others kept at moderate size\n","\n","** Build From Scratch Instead of Using Sklearn?**\n","\n","Transparency:\n","Every line of code is visible and auditable. No hidden preprocessing, no automatic\n","feature handling, no version-dependent behavior changes.\n","\n","Education:\n","Students understand EXACTLY what Ridge and Lasso do mathematically. The algorithms\n","become concrete procedures, not abstract concepts.\n","\n","Governance:\n","When a regulator asks \"How does your model work?\" you can point to explicit formulas\n","in your code, not documentation for a third-party library.\n","\n","Debugging:\n","When predictions seem wrong, you can inspect every intermediate calculation. With\n","sklearn, you're debugging someone else's black box.\n","\n","Reproducibility:\n","Code behaves identically across Python versions, operating systems, and hardware.\n","Library updates can't silently change your results.\n","\n","Trust:\n","In production quantitative finance, you trust code you've written and tested more\n","than code maintained by volunteers on the internet.\n","\n","**Numerical Considerations**\n","\n","Ridge inversion stability:\n","We add λI even for small λ (like 0.0001). This tiny regularization prevents numerical\n","issues without affecting results meaningfully. It's defensive programming.\n","\n","Lasso convergence:\n","We set max_iter = 2000 to handle difficult optimization landscapes. Most problems\n","converge in < 100 iterations, but we allow headroom for edge cases.\n","\n","Tolerance selection:\n","The default 1e-7 tolerance ensures high precision. Looser tolerance (1e-4) would\n","converge faster but might miss subtle coefficient differences important in finance.\n","\n","Feature scaling importance:\n","Lasso is scale-sensitive! Features with larger scales get smaller penalties. This is\n","why Section 8's scaling is critical—without it, Lasso would unfairly penalize features\n","with naturally large values.\n","\n","**Intercept Treatment**\n","\n","Both Ridge and Lasso do NOT penalize the intercept.\n","\n","Why?\n","The intercept represents average return when all features are zero. Penalizing it would\n","force predictions toward zero regardless of data's actual mean. This is artificial and\n","harmful.\n","\n","Implementation:\n","We center data before optimization, effectively removing the intercept from the\n","penalized term. After finding optimal coefficients, we recover the intercept using:\n","b = mean(y) - mean(X) @ w\n","\n","This ensures:\n","- Model predicts correct average (unbiased)\n","- Coefficients receive full regularization\n","- Mathematics remains clean\n","\n","**Hyperparameter Selection**\n","\n","Neither Ridge nor Lasso tells you what λ should be. You must discover optimal λ through\n","validation.\n","\n","The notebook tests multiple λ values:\n","[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n","\n","For each λ:\n","1. Run walk-forward cross-validation\n","2. Calculate mean and std of performance across folds\n","3. Compute stability score = mean - 0.5 × std\n","\n","Select λ with highest stability score, NOT highest peak performance.\n","\n","Why stability over peak?\n","A model consistently earning IC = 0.04 is worth more than one that earns IC = 0.08\n","sometimes and IC = 0.01 other times. Consistency enables sizing positions with confidence.\n","\n","**Dictionary Return Values**\n","\n","Why return dictionaries instead of objects or tuples?\n","\n","JSON serialization:\n","Dictionaries convert directly to JSON for audit trails without custom code.\n","\n","Self-documenting:\n","Looking at the dictionary, you immediately see all parameters and values. No need to\n","remember tuple order or object attribute names.\n","\n","Extensibility:\n","Adding new fields (like convergence diagnostics) doesn't break existing code.\n","\n","Governance:\n","Dictionaries go straight into trial ledgers and manifests without transformation.\n","\n","**Professional Applications**\n","\n","This implementation matches what you'd find in research code at quantitative hedge funds:\n","\n","- Explicit formulas ensure reproducibility\n","- No library dependencies reduce maintenance burden  \n","- Dictionary returns enable comprehensive logging\n","- Closed-form Ridge provides speed for large-scale searches\n","- Coordinate descent Lasso offers flexibility for sparse solutions\n","\n","The code prioritizes transparency and governance over brevity. In production, when a\n","model loses money, you need to debug every calculation. These implementations make\n","that possible.\n","\n","**Key Takeaways**\n","\n","1. Ridge has closed-form solution; Lasso requires iteration\n","2. Ridge shrinks all coefficients; Lasso can eliminate features\n","3. Both leave intercept unpenalized to avoid artificial bias\n","4. λ controls regularization strength; must be selected via validation\n","5. Coordinate descent updates one coefficient per iteration\n","6. Soft-thresholding drives Lasso coefficients to exact zero\n","7. Implementation from scratch ensures complete transparency\n","8. Dictionary returns enable JSON serialization for audit trails\n","\n","Understanding these implementations deeply prepares you for the hyperparameter search\n","in subsequent sections, where we'll test hundreds of λ values to find optimal\n","regularization strength."],"metadata":{"id":"Eqk29OPyFvkz"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"lMvSXabkFwa7"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 9 — Regularized Models (Ridge + Lasso) from First Principles\n","# ==========================================================\n","@dataclass\n","class ModelSpec:\n","    \"\"\"Configuration for regularized models.\"\"\"\n","    kind: str = \"ridge\"         # \"ridge\" | \"lasso\"\n","    lam: float = 1.0            # Regularization strength\n","    lasso_max_iter: int = 2000  # Coordinate descent iterations\n","    lasso_tol: float = 1e-7     # Convergence tolerance\n","\n","\n","def ridge_fit(X: np.ndarray, y: np.ndarray, lam: float) -> Dict[str, Any]:\n","    \"\"\"\n","    Fit Ridge regression with unpenalized intercept.\n","\n","    Solve: min ||y - (b + Xw)||^2 + lam||w||^2\n","\n","    Args:\n","        X: Feature matrix (n, d)\n","        y: Target vector (n,)\n","        lam: L2 regularization parameter\n","\n","    Returns:\n","        Dict with model parameters\n","    \"\"\"\n","    # Center y and X to handle intercept\n","    Xc = X - np.mean(X, axis=0, keepdims=True)\n","    yc = y - float(np.mean(y))\n","\n","    # Closed form: (X^T X + lam I) w = X^T y\n","    XtX = Xc.T @ Xc\n","    d = XtX.shape[0]\n","    A = XtX + lam * np.eye(d)\n","    Xty = Xc.T @ yc\n","\n","    w = np.linalg.solve(A, Xty)\n","    b = float(np.mean(y) - np.mean(X, axis=0) @ w)\n","\n","    return {\n","        \"kind\": \"ridge\",\n","        \"w\": w,\n","        \"b\": b,\n","        \"lam\": float(lam)\n","    }\n","\n","\n","def soft_threshold(z: float, g: float) -> float:\n","    \"\"\"\n","    Soft thresholding operator for Lasso.\n","\n","    Args:\n","        z: Input value\n","        g: Threshold\n","\n","    Returns:\n","        Soft-thresholded value\n","    \"\"\"\n","    if z > g:\n","        return z - g\n","    if z < -g:\n","        return z + g\n","    return 0.0\n","\n","\n","def lasso_fit_cd(\n","    X: np.ndarray,\n","    y: np.ndarray,\n","    lam: float,\n","    max_iter: int,\n","    tol: float\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Fit Lasso regression with intercept via coordinate descent.\n","\n","    Objective: (1/2n)||y - (b + Xw)||^2 + lam||w||_1\n","\n","    Args:\n","        X: Feature matrix (n, d)\n","        y: Target vector (n,)\n","        lam: L1 regularization parameter\n","        max_iter: Maximum iterations\n","        tol: Convergence tolerance\n","\n","    Returns:\n","        Dict with model parameters\n","    \"\"\"\n","    n, d = X.shape\n","\n","    # Initialize with y mean as intercept\n","    b = float(np.mean(y))\n","    yc = y - b\n","    w = np.zeros(d, dtype=np.float64)\n","\n","    # Precompute feature norms\n","    Xj2 = np.sum(X * X, axis=0) + 1e-12\n","\n","    # Coordinate descent\n","    for it in range(max_iter):\n","        w_old = w.copy()\n","\n","        # Update each coordinate\n","        for j in range(d):\n","            # Partial residual excluding j\n","            r = yc - (X @ w) + X[:, j] * w[j]\n","            # Compute rho\n","            rho = np.sum(X[:, j] * r)\n","            # Soft-threshold\n","            w[j] = soft_threshold(rho / n, lam) / (Xj2[j] / n)\n","\n","        # Update intercept\n","        b = float(np.mean(y - X @ w))\n","        yc = y - b\n","\n","        # Check convergence\n","        if np.max(np.abs(w - w_old)) < tol:\n","            break\n","\n","    return {\n","        \"kind\": \"lasso\",\n","        \"w\": w,\n","        \"b\": b,\n","        \"lam\": float(lam),\n","        \"iters\": int(it+1),\n","        \"tol\": float(tol)\n","    }\n","\n","\n","def predict_linear(model: Dict[str, Any], X: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Make predictions with linear model.\n","\n","    Args:\n","        model: Model dict from ridge_fit or lasso_fit_cd\n","        X: Feature matrix\n","\n","    Returns:\n","        Predictions\n","    \"\"\"\n","    return model[\"b\"] + X @ model[\"w\"]\n"],"metadata":{"id":"l0H3qdGqF8PP","executionInfo":{"status":"ok","timestamp":1766508188951,"user_tz":360,"elapsed":43,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["##10.METRICS, DECISION TIME AND LOGS"],"metadata":{"id":"3uvDOvw_HYqD"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"MtUFTsgDHjcA"}},{"cell_type":"markdown","source":["\n","This section defines the performance metrics used to evaluate model quality and creates\n","decision-time logs for governance. These metrics translate raw predictions into\n","actionable assessments of forecast skill, while decision-time logs document exactly\n","when and how predictions would be used in real trading.\n","\n","**Core Metrics**\n","\n","Mean Squared Error (MSE):\n","Measures average squared prediction error. Lower is better. Heavily penalizes large\n","errors due to squaring. Formula: average of (actual - predicted)². Standard metric\n","but sensitive to outliers.\n","\n","Information Coefficient (IC):\n","Pearson correlation between predictions and actual returns. Primary metric for\n","quantitative finance. Ranges from -1 to +1. Values above 0.05 are good, above 0.10\n","are excellent. Measures rank-order agreement—do high predictions correspond to high\n","actual returns? Less sensitive to outliers than MSE.\n","\n","Sign Accuracy:\n","Percentage of times prediction and actual return have same sign. Measures directional\n","correctness. Critical for trading—getting direction right matters more than exact\n","magnitude. Perfect score is 1.0 (100%), random guessing gives 0.5 (50%).\n","**PnL Proxy Metrics**\n","\n","The pnl_from_signals function creates simplified profit/loss estimates. Takes\n","predicted returns and converts to positions: +1 if prediction > 0, -1 if prediction < 0,\n","0 otherwise. Multiplies position by actual return to get per-period P&L. Calculates\n","mean P&L, standard deviation, and Sharpe-like ratio (mean/std). Also computes turnover\n","as average absolute position change. These are NOT realistic backtests—they ignore\n","transaction costs, slippage, and execution—but provide directional P&L proxies for\n","model comparison.\n","\n","**Decision-Time Logging**\n","\n","The decision_time_log function documents prediction timing for regulatory compliance.\n","Records number of forecasts, first and last indices, decision rule text, and prediction\n","statistics (mean, std, quantiles). This creates an audit trail proving when forecasts\n","existed and what decision-making assumptions were made. Critical for defending model\n","choices to skeptical stakeholders or regulators.\n","\n","**Metric Bundling**\n","\n","The metric_bundle function computes all metrics simultaneously, returning a dictionary\n","with MSE, IC, sign accuracy, and all P&L proxies. This ensures consistent calculation\n","across all validation contexts.\n","\n","**Professional Relevance**\n","\n","These metrics match industry standards at quantitative hedge funds. IC is the gold\n","standard for forecast quality assessment. Decision-time logs provide the governance\n","documentation required in regulated environments. Together they enable both performance\n","measurement and compliance verification.\n"],"metadata":{"id":"W-9HtdFJHlm_"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"XrTIMameHmaM"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 10 — Metrics + Decision-Time Trading Proxy + Logs\n","# ==========================================================\n","def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    \"\"\"Compute mean squared error.\"\"\"\n","    e = y_true - y_pred\n","    return float(np.mean(e * e))\n","\n","\n","def corr(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    \"\"\"\n","    Compute Pearson correlation (Information Coefficient).\n","\n","    This is the primary metric for evaluating forecast quality.\n","    \"\"\"\n","    a = y_true - float(np.mean(y_true))\n","    b = y_pred - float(np.mean(y_pred))\n","    denom = (np.std(a) * np.std(b)) + 1e-12\n","    return float(np.mean(a * b) / denom)\n","\n","\n","def sign_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    \"\"\"Compute directional accuracy (sign agreement).\"\"\"\n","    return float(np.mean((y_true >= 0) == (y_pred >= 0)))\n","\n","\n","def pnl_from_signals(\n","    y_forward: np.ndarray,\n","    y_pred: np.ndarray,\n","    threshold: float = 0.0\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Compute simple P&L proxy from signals.\n","\n","    Simple proxy: position = sign(y_pred - threshold) in {-1,0,1}\n","    Realized pnl ~ position * y_forward\n","\n","    This is NOT a full backtest engine; it's a decision-aligned utility proxy.\n","\n","    Args:\n","        y_forward: Actual forward returns\n","        y_pred: Predicted forward returns\n","        threshold: Signal threshold\n","\n","    Returns:\n","        Dict with P&L statistics\n","    \"\"\"\n","    pos = np.zeros_like(y_pred)\n","    pos[y_pred > threshold] = 1.0\n","    pos[y_pred < -threshold] = -1.0\n","\n","    pnl = pos * y_forward\n","\n","    return {\n","        \"mean_pnl\": float(np.mean(pnl)),\n","        \"std_pnl\": float(np.std(pnl)),\n","        \"sharpe_like\": float(np.mean(pnl) / (np.std(pnl) + 1e-12)),\n","        \"turnover_like\": float(np.mean(np.abs(np.diff(pos)))) if len(pos) > 1 else 0.0,\n","    }\n","\n","\n","def decision_time_log(\n","    t_idx: np.ndarray,\n","    y_pred: np.ndarray,\n","    decision_rule: str\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Create decision-time trace for governance.\n","\n","    Logs indices at which forecasts exist and the decision rule.\n","\n","    Args:\n","        t_idx: Time indices of predictions\n","        y_pred: Predictions\n","        decision_rule: Description of decision timing\n","\n","    Returns:\n","        Dict with decision timing metadata\n","    \"\"\"\n","    return {\n","        \"n_forecasts\": int(len(t_idx)),\n","        \"first_index\": int(t_idx[0]) if len(t_idx) else None,\n","        \"last_index\": int(t_idx[-1]) if len(t_idx) else None,\n","        \"decision_rule\": decision_rule,\n","        \"pred_summary\": {\n","            \"mean\": float(np.mean(y_pred)) if len(y_pred) else None,\n","            \"std\": float(np.std(y_pred)) if len(y_pred) else None,\n","            \"q\": {\n","                q: float(np.quantile(y_pred, q))\n","                for q in [0.05, 0.5, 0.95]\n","            } if len(y_pred) else None,\n","        }\n","    }\n"],"metadata":{"id":"cLPf5gDwHub2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##11.BASELINES"],"metadata":{"id":"qv7vLLqUIRgn"}},{"cell_type":"markdown","source":["###11.1.OVERVIEW"],"metadata":{"id":"MBVQ5rqrIUBs"}},{"cell_type":"markdown","source":["\n","This section implements naive baseline models that provide performance benchmarks\n","against which we compare our regularized models. Every sophisticated model must beat\n","simple baselines—otherwise, why use complexity? These baselines represent the minimum\n","acceptable performance and help diagnose whether apparent model skill is actually\n","just exploiting trivial patterns.\n","\n","**Why Baselines Matter**\n","\n","In quantitative finance, a model showing IC = 0.04 sounds promising until you discover\n","a zero-effort baseline achieves IC = 0.05. The sophisticated model is actually worse!\n","Baselines prevent this embarrassment. They answer: \"Is our complex Ridge regression\n","with hyperparameter tuning actually better than just predicting zero every day?\"\n","\n","**Three Baseline Strategies**\n","\n","Zero Baseline:\n","Always predicts return = 0.0 for every sample. Represents the null hypothesis: markets\n","are unpredictable, no signal exists. If your model can't beat this, it's adding no\n","value. Surprisingly hard to beat in efficient markets! Often serves as the toughest\n","benchmark.\n","\n","Mean Baseline:\n","Always predicts the training set's average return. Represents exploiting only the\n","unconditional mean—no feature information. Slightly more sophisticated than zero.\n","In markets with positive drift, this baseline has an edge. Beats zero baseline if\n","market has consistent directional bias.\n","\n","Last Return Baseline:\n","Uses the most recent return feature as the prediction. Represents momentum: if today's\n","return was positive, predict tomorrow's will be too. Exploits short-term autocorrelation\n","without any model. Often performs surprisingly well in trending markets. Uses first\n","feature column as proxy for recent return signal.\n","\n","**Implementation**\n","\n","The baseline_predict function takes baseline type, training data (to compute mean),\n","and evaluation features. Returns predictions matching the evaluation set size. No\n","training happens for zero/mean baselines—they're parameter-free. Last return baseline\n","simply copies a feature column.\n","\n","**Evaluation Protocol**\n","\n","Section 14 evaluates all baselines using the same walk-forward cross-validation as\n","regularized models. Each baseline gets tested on identical folds with identical\n","purge/embargo protocols. Results go into baseline_bundle.json for permanent record.\n","Visualizations show best model performance versus all baseline performances, making\n","model value immediately clear.\n","\n","**Professional Interpretation**\n","\n","If Ridge with optimal lambda achieves IC = 0.045 while best baseline achieves IC = 0.038,\n","the model adds 0.007 IC of value. This incremental improvement, if stable across time,\n","justifies deployment complexity. If Ridge achieves IC = 0.032 while mean baseline\n","achieves IC = 0.038, abandon the model—it's actually harmful!"],"metadata":{"id":"KvB4o3dFIWUE"}},{"cell_type":"markdown","source":["###11.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"S-n1OhCeIWtP"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 11 — Baselines (Deterministic)\n","# ==========================================================\n","def baseline_predict(\n","    kind: str,\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_eval: np.ndarray,\n","    ret_eval_proxy: Optional[np.ndarray] = None\n",") -> np.ndarray:\n","    \"\"\"\n","    Generate baseline predictions for comparison.\n","\n","    Baselines:\n","    - \"zero\": always predict 0\n","    - \"mean\": always predict mean(y_train)\n","    - \"last_ret\": use last observed return feature proxy\n","\n","    Args:\n","        kind: Baseline type\n","        X_train: Training features\n","        y_train: Training targets\n","        X_eval: Evaluation features\n","        ret_eval_proxy: Optional return proxy for \"last_ret\" baseline\n","\n","    Returns:\n","        Baseline predictions\n","    \"\"\"\n","    if kind == \"zero\":\n","        return np.zeros(X_eval.shape[0], dtype=np.float64)\n","\n","    if kind == \"mean\":\n","        return np.full(X_eval.shape[0], float(np.mean(y_train)), dtype=np.float64)\n","\n","    if kind == \"last_ret\":\n","        # Use the first feature as a proxy for \"recent return signal\" if no explicit proxy given\n","        if ret_eval_proxy is None:\n","            return X_eval[:, 0].copy()\n","        return ret_eval_proxy.copy()\n","\n","    raise ValueError(f\"Unknown baseline: {kind}\")\n"],"metadata":{"id":"arWvl9Q7IZH3","executionInfo":{"status":"ok","timestamp":1766509032970,"user_tz":360,"elapsed":42,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["##12.PARAMETER REGISTRY"],"metadata":{"id":"QSgNEz9pJMau"}},{"cell_type":"markdown","source":["###12.1.OVERVIEW"],"metadata":{"id":"71jJOi0uJVAd"}},{"cell_type":"markdown","source":["\n","This section defines the complete hyperparameter search space and selection rules.\n","It creates a structured registry of all configurations to test, ensuring the search\n","is deterministic, reproducible, and comprehensive. This is where we specify which\n","model variants will compete for deployment.\n","\n","**SearchSpec Configuration**\n","\n","The SearchSpec dataclass centralizes all search decisions. Model kinds to test: Ridge\n","and Lasso. Lambda grid: seven values spanning 0.0001 to 100.0, covering weak to\n","extreme regularization. Feature window grids: fast momentum (10, 20, 60 days),\n","volatility (10, 20, 60 days), slow momentum (60, 120 days). Scaling methods: none,\n","zscore, and robust_clip. Clip quantile options for robust scaling: 95th and 99th\n","percentiles.\n","\n","Selection rule parameters: stability_alpha = 0.5 penalizes fold variance, primary_metric\n","= \"ic\" focuses optimization on Information Coefficient. Tuning budget maximum of 10,000\n","trials prevents combinatorial explosion with defensive random sampling if needed.\n","\n","**Trial Generation**\n","\n","The generate_trials function creates the Cartesian product of all parameter combinations.\n","For each model type (Ridge/Lasso), each lambda value, each feature window configuration,\n","and each scaling method, it generates a trial dictionary. Robust_clip scaling gets\n","tested with both quantile options; other methods use fixed clip_q.\n","\n","Total trials example: 2 models × 7 lambdas × 3 L_mom × 3 L_vol × 2 L_slow × 3 scaling\n","methods (with robust_clip counted twice) = 756 configurations. If this exceeds\n","tuning_budget_max, random sampling reduces the set to stay within budget.\n","\n","**Deterministic Ordering**\n","\n","After generation, trials are sorted by their JSON hash. This ensures identical search\n","space across runs regardless of Python's dictionary ordering. Same seed, same trials,\n","same order, same results—complete reproducibility.\n","\n","\n","\n","The registry documents every configuration tested, creating an audit trail. We're not\n","arbitrarily trying \"a few lambda values\"—we're systematically exploring a defined\n","space. If asked \"Did you test lambda = 0.5?\" we can definitively answer yes or no by\n","consulting the registry. This systematic approach prevents confirmation bias (testing\n","only configurations you expect to work) and creates defensible model selection\n","suitable for regulatory scrutiny.\n","\n","The stability_alpha parameter embeds our selection philosophy: we prefer consistent\n","models over peak performers. Alpha = 0.5 means we penalize standard deviation at\n","half the weight of mean performance—a moderate stance balancing performance and\n","reliability."],"metadata":{"id":"WbkXQj4HJWr_"}},{"cell_type":"markdown","source":["###12.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"wD2ozD9CJXKR"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 12 — Parameter Registry (Search Space + Constraints)\n","# ==========================================================\n","@dataclass\n","class SearchSpec:\n","    \"\"\"Configuration for hyperparameter search.\"\"\"\n","    model_kinds: Tuple[str, ...] = (\"ridge\", \"lasso\")\n","    lam_grid: Tuple[float, ...] = (1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0)\n","\n","    # Feature windows to tune\n","    L_mom_grid: Tuple[int, ...] = (10, 20, 60)\n","    L_vol_grid: Tuple[int, ...] = (10, 20, 60)\n","    L_slow_grid: Tuple[int, ...] = (60, 120)\n","\n","    scaling_methods: Tuple[str, ...] = (\"none\", \"zscore\", \"robust_clip\")\n","    clip_q_grid: Tuple[float, ...] = (0.95, 0.99)\n","\n","    # Selection rule parameters\n","    stability_alpha: float = 0.5  # score = mean_metric - alpha * std_metric\n","    primary_metric: str = \"ic\"    # \"ic\" (corr) | \"mse\" | \"sharpe_like\"\n","    tuning_budget_max: int = 10_000  # Hard cap safeguard\n","\n","\n","def generate_trials(ss: SearchSpec, seed: int) -> List[Dict[str, Any]]:\n","    \"\"\"\n","    Generate deterministic grid of hyperparameter trials.\n","\n","    Args:\n","        ss: Search specification\n","        seed: Random seed for sampling if budget exceeded\n","\n","    Returns:\n","        List of trial configurations\n","    \"\"\"\n","    trials = []\n","\n","    for mk in ss.model_kinds:\n","        for lam in ss.lam_grid:\n","            for Lm in ss.L_mom_grid:\n","                for Lv in ss.L_vol_grid:\n","                    for Ls in ss.L_slow_grid:\n","                        for sm in ss.scaling_methods:\n","                            if sm == \"robust_clip\":\n","                                for cq in ss.clip_q_grid:\n","                                    trials.append({\n","                                        \"model_kind\": mk,\n","                                        \"lam\": float(lam),\n","                                        \"feature_spec\": {\n","                                            \"L_mom\": int(Lm),\n","                                            \"L_vol\": int(Lv),\n","                                            \"L_slow\": int(Ls),\n","                                            \"ewma_alpha\": 0.08,\n","                                            \"use_ewma\": True\n","                                        },\n","                                        \"scale_spec\": {\n","                                            \"method\": sm,\n","                                            \"clip_q\": float(cq)\n","                                        },\n","                                    })\n","                            else:\n","                                trials.append({\n","                                    \"model_kind\": mk,\n","                                    \"lam\": float(lam),\n","                                    \"feature_spec\": {\n","                                        \"L_mom\": int(Lm),\n","                                        \"L_vol\": int(Lv),\n","                                        \"L_slow\": int(Ls),\n","                                        \"ewma_alpha\": 0.08,\n","                                        \"use_ewma\": True\n","                                    },\n","                                    \"scale_spec\": {\n","                                        \"method\": sm,\n","                                        \"clip_q\": 0.99\n","                                    },\n","                                })\n","\n","    # Hard cap (defensive) with random sampling if needed\n","    if len(trials) > ss.tuning_budget_max:\n","        rng = np.random.default_rng(seed)\n","        idx = rng.choice(len(trials), size=ss.tuning_budget_max, replace=False)\n","        trials = [trials[i] for i in idx]\n","\n","    # Deterministic ordering via hash sort\n","    trials.sort(key=lambda d: sha256_json(d))\n","\n","    return trials\n","\n"],"metadata":{"id":"ie8Qno6VJ0Yk","executionInfo":{"status":"ok","timestamp":1766509207800,"user_tz":360,"elapsed":44,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["##13.TRIAL EVALUATION WITH STABILITY SCORING"],"metadata":{"id":"nCcKu4Z_JZZz"}},{"cell_type":"markdown","source":["###13.1.OVERVIEW"],"metadata":{"id":"m3cMXqMlKlvf"}},{"cell_type":"markdown","source":["\n","This section implements the core evaluation machinery that tests each hyperparameter\n","configuration using walk-forward cross-validation and computes stability scores. It\n","determines which model configuration should be deployed by balancing mean performance\n","against variance across time periods.\n","\n","**Key Functions**\n","\n","fit_model:\n","Wrapper that calls either ridge_fit or lasso_fit_cd based on ModelSpec kind. Takes\n","training features, targets, and model specification. Returns fitted model dictionary.\n","Abstracts away model type differences so evaluation code stays clean.\n","\n","metric_bundle:\n","Computes all performance metrics (MSE, IC, sign accuracy, P&L proxies) simultaneously\n","from predictions and actuals. Returns comprehensive dictionary. Ensures consistent\n","metric calculation everywhere.\n","\n","primary_score:\n","Extracts the primary metric (IC, MSE, or Sharpe) and converts to \"higher is better\"\n","convention. MSE gets negated since lower MSE is better. This standardization enables\n","uniform optimization across different metrics.\n","\n","**Walk-Forward Evaluation**\n","\n","The evaluate_walk_forward function orchestrates multi-fold evaluation. For each fold:\n","fit scaler on training data, verify no leakage, scale features, fit model, predict\n","on test data, compute metrics. Appends fold results to list. Also stores coefficients\n","from each fold to build coefficient stability matrix.\n","\n","Returns dictionary containing fold metrics (list of performance across all folds),\n","coefficient matrix (K folds × D features), and decision rule documentation.\n","\n","**Single Split Evaluation**\n","\n","The evaluate_single_split function handles train/validation/test assessment. First\n","evaluates on validation set for hyperparameter selection. Then refits on combined\n","train+validation using separate scaler, evaluates on held-out test set. This nested\n","structure simulates realistic deployment: tune on validation, report honest performance\n","on test.\n","\n","**Stability Score Computation**\n","\n","The stability_score_from_folds function computes the key selection metric. Extracts\n","primary scores from all folds. Calculates mean and standard deviation. Computes\n","stability score = mean - alpha × std. The alpha parameter (default 0.5) controls\n","how much we penalize variability. High alpha = prefer consistency. Low alpha =\n","prefer peak performance.\n","\n","This formula captures professional wisdom: a model earning consistent IC = 0.04 beats\n","one earning IC = 0.08 sometimes and 0.00 other times. Stability enables reliable\n","position sizing and risk management."],"metadata":{"id":"rmfBrXOhKnZ6"}},{"cell_type":"markdown","source":["###13.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"Ipputo-5Knvz"}},{"cell_type":"code","source":["\n","def fit_model(ms: ModelSpec, Xtr: np.ndarray, ytr: np.ndarray) -> Dict[str, Any]:\n","    \"\"\"\n","    Fit regularized model.\n","\n","    Args:\n","        ms: Model specification\n","        Xtr: Training features\n","        ytr: Training targets\n","\n","    Returns:\n","        Fitted model dict\n","    \"\"\"\n","    if ms.kind == \"ridge\":\n","        return ridge_fit(Xtr, ytr, ms.lam)\n","\n","    if ms.kind == \"lasso\":\n","        return lasso_fit_cd(Xtr, ytr, ms.lam, ms.lasso_max_iter, ms.lasso_tol)\n","\n","    raise ValueError(f\"Unknown model kind: {ms.kind}\")\n","\n","\n","def metric_bundle(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n","    \"\"\"\n","    Compute comprehensive metric bundle.\n","\n","    Args:\n","        y_true: True targets\n","        y_pred: Predictions\n","\n","    Returns:\n","        Dict of all computed metrics\n","    \"\"\"\n","    out = {\n","        \"mse\": mse(y_true, y_pred),\n","        \"ic\": corr(y_true, y_pred),\n","        \"sign_acc\": sign_accuracy(y_true, y_pred),\n","    }\n","\n","    # Add P&L proxy metrics\n","    pnl_metrics = pnl_from_signals(y_true, y_pred, threshold=0.0)\n","    out.update({f\"pnl_{k}\": v for k, v in pnl_metrics.items()})\n","\n","    return out\n","\n","\n","def primary_score(metrics: Dict[str, float], primary: str) -> float:\n","    \"\"\"\n","    Extract primary score from metrics (higher is better).\n","\n","    Args:\n","        metrics: Metric dict\n","        primary: Primary metric name\n","\n","    Returns:\n","        Primary score (transformed to \"higher is better\")\n","    \"\"\"\n","    if primary == \"mse\":\n","        # Lower is better: convert to \"higher is better\" by negative\n","        return -metrics[\"mse\"]\n","\n","    if primary == \"ic\":\n","        return metrics[\"ic\"]\n","\n","    if primary == \"sharpe_like\":\n","        return metrics[\"pnl_sharpe_like\"]\n","\n","    raise ValueError(f\"Unknown primary metric: {primary}\")\n","\n","\n","def evaluate_single_split(\n","    X: np.ndarray,\n","    y: np.ndarray,\n","    idx: Dict[str, np.ndarray],\n","    sc: ScaleSpec,\n","    ms: ModelSpec,\n","    decision_rule: str\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Evaluate model on single train/valid/test split.\n","\n","    Args:\n","        X: Feature matrix\n","        y: Target vector\n","        idx: Dict with train/valid/test indices\n","        sc: Scaling specification\n","        ms: Model specification\n","        decision_rule: Decision timing description\n","\n","    Returns:\n","        Dict with validation and test results\n","    \"\"\"\n","    tr = idx[\"train\"]\n","    va = idx[\"valid\"]\n","    te = idx[\"test\"]\n","    tr_for_testfit = idx[\"train_for_testfit\"]\n","\n","    # Fit scaler on TRAIN ONLY; assert no leakage into valid\n","    scaler_tr = fit_scaler(X, tr, sc)\n","    assert_scaler_fit_only_on_train(scaler_tr, va)\n","\n","    Xtr = apply_scaler(X[tr], scaler_tr)\n","    Xva = apply_scaler(X[va], scaler_tr)\n","    ytr = y[tr]\n","    yva = y[va]\n","\n","    # Fit model on train\n","    model = fit_model(ms, Xtr, ytr)\n","    yhat_va = predict_linear(model, Xva)\n","    m_va = metric_bundle(yva, yhat_va)\n","\n","    # Decision-time log (validation)\n","    dtlog_va = decision_time_log(va, yhat_va, decision_rule)\n","\n","    # Final test evaluation: refit on (train+valid) WITHOUT touching test\n","    scaler_tv = fit_scaler(X, tr_for_testfit, sc)\n","    assert_scaler_fit_only_on_train(scaler_tv, te)\n","\n","    Xtv = apply_scaler(X[tr_for_testfit], scaler_tv)\n","    ytv = y[tr_for_testfit]\n","    model_tv = fit_model(ms, Xtv, ytv)\n","\n","    Xte = apply_scaler(X[te], scaler_tv)\n","    yte = y[te]\n","    yhat_te = predict_linear(model_tv, Xte)\n","    m_te = metric_bundle(yte, yhat_te)\n","    dtlog_te = decision_time_log(te, yhat_te, decision_rule)\n","\n","    return {\n","        \"valid_metrics\": m_va,\n","        \"test_metrics\": m_te,\n","        \"model\": {\n","            \"kind\": model_tv[\"kind\"],\n","            \"lam\": model_tv[\"lam\"],\n","            \"w\": model_tv[\"w\"].tolist(),\n","            \"b\": float(model_tv[\"b\"])\n","        },\n","        \"decision_time_log_valid\": dtlog_va,\n","        \"decision_time_log_test\": dtlog_te,\n","    }\n","\n","\n","def evaluate_walk_forward(\n","    X: np.ndarray,\n","    y: np.ndarray,\n","    folds: List[Dict[str, np.ndarray]],\n","    sc: ScaleSpec,\n","    ms: ModelSpec,\n","    decision_rule: str\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Evaluate model via walk-forward cross-validation.\n","\n","    Args:\n","        X: Feature matrix\n","        y: Target vector\n","        folds: List of fold dicts from walk_forward_folds()\n","        sc: Scaling specification\n","        ms: Model specification\n","        decision_rule: Decision timing description\n","\n","    Returns:\n","        Dict with fold metrics and coefficient matrix\n","    \"\"\"\n","    fold_metrics = []\n","    coeffs = []\n","\n","    for k, fd in enumerate(folds):\n","        tr = fd[\"train\"]\n","        te = fd[\"test\"]\n","\n","        scaler = fit_scaler(X, tr, sc)\n","        assert_scaler_fit_only_on_train(scaler, te)\n","\n","        Xtr = apply_scaler(X[tr], scaler)\n","        ytr = y[tr]\n","        model = fit_model(ms, Xtr, ytr)\n","\n","        Xte = apply_scaler(X[te], scaler)\n","        yte = y[te]\n","        yhat = predict_linear(model, Xte)\n","\n","        m = metric_bundle(yte, yhat)\n","        m[\"fold\"] = int(k)\n","        m[\"train_span\"] = fd[\"train_span\"]\n","        m[\"test_span\"] = fd[\"test_span\"]\n","        fold_metrics.append(m)\n","        coeffs.append(model[\"w\"].copy())\n","\n","    return {\n","        \"fold_metrics\": fold_metrics,\n","        \"coef_matrix\": np.vstack(coeffs),  # (K,d)\n","        \"decision_rule\": decision_rule,\n","    }\n","\n","\n","def stability_score_from_folds(\n","    fold_metrics: List[Dict[str, Any]],\n","    primary: str,\n","    alpha: float\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Compute stability score from fold metrics.\n","\n","    Stability score = mean - alpha * std\n","    Favors configurations with both high mean and low variance.\n","\n","    Args:\n","        fold_metrics: List of metric dicts from folds\n","        primary: Primary metric name\n","        alpha: Stability penalty weight\n","\n","    Returns:\n","        Dict with mean, std, and stability score\n","    \"\"\"\n","    scores = np.array([\n","        primary_score(m, primary)\n","        for m in fold_metrics\n","    ], dtype=np.float64)\n","\n","    mu = float(np.mean(scores))\n","    sd = float(np.std(scores))\n","\n","    return {\n","        \"mean_primary\": mu,\n","        \"std_primary\": sd,\n","        \"stability_score\": float(mu - alpha * sd)\n","    }\n","\n"],"metadata":{"id":"hSnB4QDaKqWv","executionInfo":{"status":"ok","timestamp":1766509731505,"user_tz":360,"elapsed":138,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["##14.MAIN EXECUTION LOGIC AND PROGRESS TRACKING"],"metadata":{"id":"ylaLZlDML9xH"}},{"cell_type":"markdown","source":["###14.1.OVERVIEW"],"metadata":{"id":"AvhcrGmEMEDq"}},{"cell_type":"markdown","source":["\n","This section orchestrates the entire model selection pipeline from data generation\n","through hyperparameter search to final model selection. It's the \"main\" function that\n","ties all previous sections together into a cohesive, auditable workflow with progress\n","tracking and comprehensive artifact generation.\n","\n","**Workflow Stages**\n","\n","Configuration Setup:\n","Defines global parameters—synthetic data specs (2600 days), split boundaries (1600/2100),\n","walk-forward CV parameters (800-day training, 200-day test, 200-day steps), search\n","space (Ridge/Lasso, seven lambda values, multiple feature windows). Creates run\n","manifest with unique identifier and saves all configurations to JSON files for\n","reproducibility.\n","\n","Data Generation:\n","Generates synthetic market with regime-switching returns. Computes data fingerprint\n","including statistics, quantiles, and SHA-256 hashes. Saves fingerprint to JSON. Creates\n","forward return labels with specified horizon. Enforces causality through monotonic\n","time assertions.\n","\n","Hyperparameter Search:\n","Generates complete trial list (hundreds to thousands of configurations). Iterates\n","through each trial with progress indicators showing percentage completion every 5%.\n","For each configuration: builds features, creates train/validation/test splits, runs\n","walk-forward CV, computes stability scores, handles failures gracefully (logs error\n","reason, continues to next trial). Tracks best configuration based on highest stability\n","score. Maintains sensitivity records for regularization path analysis.\n","\n","Trial Ledger:\n","Each trial appends one JSON line to trial_ledger.jsonl containing configuration,\n","status (ok/fail_causality_gate/fail_numerical), fold metrics, and validation/test\n","results. Creates permanent searchable record of every configuration tested.\n","\n","Best Model Selection:\n","After testing all configurations, identifies trial with highest stability score.\n","Saves complete best model information to best_trial.json including configuration,\n","feature names, stability metrics, and honest test set performance. Prints summary\n","showing selected model type, lambda, scaling method, validation IC, and test IC.\n","\n","**Governance Artifacts**\n","\n","The execution produces complete audit trail: run_manifest.json (unique run ID,\n","timestamp, seed, code version), data_fingerprint.json (data provenance with hashes),\n","parameter_registry.json (complete search space definition), split_spec.json (split\n","boundaries and purge/embargo rules), trial_ledger.jsonl (every configuration tested),\n","best_trial.json (selected model with test performance). These artifacts enable\n","reproducing any result months later and proving no post-hoc manipulation occurred."],"metadata":{"id":"hhEyipyFMF9v"}},{"cell_type":"markdown","source":["###14.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"odiyC84pMRGW"}},{"cell_type":"code","source":["\n","# ==========================================================\n","# Cell 14 — Main Execution Logic with Progress Tracking\n","# ==========================================================\n","\n","def print_section_header(title: str, width: int = 80) -> None:\n","    \"\"\"Print professional section header.\"\"\"\n","    print(\"\\\\n\" + \"=\" * width)\n","    print(title.center(width))\n","    print(\"=\" * width + \"\\\\n\")\n","\n","\n","# Execute main workflow\n","if __name__ == \"__main__\":\n","    print_section_header(\"CHAPTER 12 — REGULARIZATION, HYPERPARAMETERS, MODEL SELECTION\")\n","\n","    print(\"Initializing governance-native model selection lab...\")\n","    print(f\"Master seed: {MASTER_SEED}\")\n","    print(f\"Artifact directory: {ART_DIR}\\\\n\")\n","\n","    # Global configuration\n","    SYN_SPEC = SyntheticSpec(n=2600)\n","    SPLIT_SPEC = SplitSpec(\n","        train_end=1600,\n","        valid_end=2100,\n","        cv_train_len=900,\n","        cv_test_len=200,\n","        cv_step=200,\n","        label_horizon=5,\n","        embargo=5,\n","        decision_time=\"features computed at EOD t using returns <=t, traded at t+1 open (conservative)\"\n","    )\n","    SEARCH_SPEC = SearchSpec(\n","        model_kinds=(\"ridge\", \"lasso\"),\n","        lam_grid=(1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0),\n","        L_mom_grid=(10, 20, 60),\n","        L_vol_grid=(10, 20, 60),\n","        L_slow_grid=(60, 120),\n","        scaling_methods=(\"none\", \"zscore\", \"robust_clip\"),\n","        clip_q_grid=(0.95, 0.99),\n","        stability_alpha=0.5,\n","        primary_metric=\"ic\",\n","        tuning_budget_max=5000,\n","    )\n","\n","    CONFIG = {\n","        \"synthetic_spec\": asdict(SYN_SPEC),\n","        \"split_spec\": asdict(SPLIT_SPEC),\n","        \"search_spec\": asdict(SEARCH_SPEC),\n","        \"notes\": \"Chapter 12 governance-native model selection lab (synthetic-first, no pandas).\"\n","    }\n","\n","    # Save manifests\n","    RUN_MANIFEST = write_run_manifest(CONFIG, code_id=\"colab_ch12_v1_polished\")\n","    save_json(os.path.join(ART_DIR, \"run_manifest.json\"), RUN_MANIFEST)\n","    save_json(os.path.join(ART_DIR, \"parameter_registry.json\"), CONFIG)\n","\n","    print(\"Configuration:\")\n","    print(f\"  - Synthetic data: {SYN_SPEC.n} days\")\n","    print(f\"  - Train/valid/test split: {SPLIT_SPEC.train_end}/{SPLIT_SPEC.valid_end}/{SYN_SPEC.n}\")\n","    print(f\"  - Label horizon: {SPLIT_SPEC.label_horizon} days\")\n","    print(f\"  - Embargo period: {SPLIT_SPEC.embargo} days\")\n","    print(f\"  - Primary metric: {SEARCH_SPEC.primary_metric}\")\n","    print(f\"  - Stability penalty (alpha): {SEARCH_SPEC.stability_alpha}\\\\n\")\n","\n","    # Generate synthetic market\n","    print_section_header(\"DATA GENERATION\")\n","    market = generate_synthetic_market(SYN_SPEC, seed=MASTER_SEED + 111)\n","    t = market[\"t\"]\n","    price = market[\"price\"]\n","    ret = market[\"ret\"]\n","    regime = market[\"regime\"]\n","\n","    # Data fingerprint\n","    fp = data_fingerprint(\n","        price, ret,\n","        meta={\"source\": \"synthetic\", \"spec_sha256\": sha256_json(asdict(SYN_SPEC))}\n","    )\n","    save_json(os.path.join(ART_DIR, \"data_fingerprint.json\"), fp)\n","\n","    print(f\"✓ Generated {len(t)} observations\")\n","    print(f\"✓ Price range: [{fp['price_min']:.2f}, {fp['price_max']:.2f}]\")\n","    print(f\"✓ Return mean: {fp['ret_mean']:.6f}, std: {fp['ret_std']:.6f}\")\n","    print(f\"✓ Data fingerprint saved\")\n","\n","    # Create labels\n","    h = SPLIT_SPEC.label_horizon\n","    y = forward_return_label(ret, h=h)\n","    print(f\"\\\\n✓ Labels created with {h}-day forward horizon\")\n","\n","    # Initialize trial ledger\n","    LEDGER_PATH = os.path.join(ART_DIR, \"trial_ledger.jsonl\")\n","    if os.path.exists(LEDGER_PATH):\n","        os.remove(LEDGER_PATH)\n","\n","    save_json(os.path.join(ART_DIR, \"split_spec.json\"), asdict(SPLIT_SPEC))\n","\n","    # Generate trials\n","    print_section_header(\"HYPERPARAMETER SEARCH\")\n","    TRIALS = generate_trials(SEARCH_SPEC, seed=MASTER_SEED + 222)\n","    print(f\"✓ Total trials (deterministic ordering): {len(TRIALS)}\\\\n\")\n","\n","    print(\"Evaluating trials (this may take several minutes)...\")\n","    print(\"  - Walk-forward CV for stability estimation\")\n","    print(\"  - Single split for held-out test performance\")\n","    print(\"  - Causality gates enforced on every fold\\\\n\")\n","\n","    best_trial = None\n","    best_stability = -1e99\n","    sens_records = []\n","\n","    # Progress tracking\n","    total_trials = len(TRIALS)\n","    milestone_step = max(1, total_trials // 20)\n","\n","    for trial_counter, cfg in enumerate(TRIALS, 1):\n","        trial_id = sha256_json(cfg)[:12]\n","\n","        # Progress indicator\n","        if trial_counter % milestone_step == 0 or trial_counter == total_trials:\n","            pct = 100 * trial_counter / total_trials\n","            print(f\"  Progress: {trial_counter}/{total_trials} ({pct:.1f}%)\")\n","\n","        # Build features\n","        fs = FeatureSpec(**cfg[\"feature_spec\"])\n","        feats = build_features(t, price, ret, fs)\n","        X_full, y_full, feat_names, valid_mask = assemble_dataset(feats, y)\n","\n","        # Split indices\n","        idx_single = single_split_indices(\n","            n=len(y_full), valid_mask=valid_mask, ss=SPLIT_SPEC\n","        )\n","        folds = walk_forward_folds(\n","            n=len(y_full), valid_mask=valid_mask, ss=SPLIT_SPEC\n","        )\n","\n","        sc = ScaleSpec(**cfg[\"scale_spec\"])\n","        ms = ModelSpec(kind=cfg[\"model_kind\"], lam=cfg[\"lam\"])\n","\n","        # Evaluate\n","        status = \"ok\"\n","        fail_reason = None\n","\n","        try:\n","            wf = evaluate_walk_forward(\n","                X_full, y_full, folds, sc, ms,\n","                decision_rule=SPLIT_SPEC.decision_time\n","            )\n","            stab = stability_score_from_folds(\n","                wf[\"fold_metrics\"], SEARCH_SPEC.primary_metric, SEARCH_SPEC.stability_alpha\n","            )\n","        except (AssertionError, np.linalg.LinAlgError) as e:\n","            status = \"fail_causality_gate\" if isinstance(e, AssertionError) else \"fail_numerical\"\n","            fail_reason = str(e)\n","            stab = {\"mean_primary\": None, \"std_primary\": None, \"stability_score\": None}\n","            wf = None\n","\n","        single_out = None\n","        if status == \"ok\":\n","            try:\n","                single_out = evaluate_single_split(\n","                    X_full, y_full, idx_single, sc, ms,\n","                    decision_rule=SPLIT_SPEC.decision_time\n","                )\n","            except (AssertionError, np.linalg.LinAlgError) as e:\n","                status = \"fail_causality_gate\" if isinstance(e, AssertionError) else \"fail_numerical\"\n","                fail_reason = str(e)\n","\n","        # Log trial\n","        ledger_entry = {\n","            \"trial_id\": trial_id,\n","            \"cfg\": cfg,\n","            \"feature_names\": feat_names,\n","            \"status\": status,\n","            \"fail_reason\": fail_reason,\n","            \"walk_forward\": {\n","                \"stability\": stab,\n","                \"fold_metrics\": wf[\"fold_metrics\"] if wf is not None else None\n","            },\n","            \"single_split\": {\n","                \"valid_metrics\": single_out[\"valid_metrics\"] if single_out is not None else None,\n","                \"test_metrics\": single_out[\"test_metrics\"] if single_out is not None else None\n","            }\n","        }\n","        append_jsonl(LEDGER_PATH, ledger_entry)\n","\n","        # Track best\n","        if status == \"ok\":\n","            scv = stab[\"stability_score\"]\n","            if scv is not None and scv > best_stability:\n","                best_stability = scv\n","                best_trial = {\n","                    \"trial_id\": trial_id,\n","                    \"cfg\": cfg,\n","                    \"feature_names\": feat_names,\n","                    \"walk_forward\": wf,\n","                    \"stability\": stab,\n","                    \"single_split\": single_out,\n","                }\n","\n","        # Sensitivity records\n","        if (cfg[\"model_kind\"] == \"ridge\" and\n","            cfg[\"scale_spec\"][\"method\"] == \"zscore\" and\n","            cfg[\"feature_spec\"][\"L_mom\"] == 20 and\n","            cfg[\"feature_spec\"][\"L_vol\"] == 20 and\n","            cfg[\"feature_spec\"][\"L_slow\"] == 120 and\n","            status == \"ok\"):\n","            sens_records.append({\n","                \"lam\": cfg[\"lam\"],\n","                \"stability_score\": stab[\"stability_score\"],\n","                \"mean_primary\": stab[\"mean_primary\"],\n","                \"std_primary\": stab[\"std_primary\"]\n","            })\n","\n","    print_section_header(\"TRIAL EVALUATION COMPLETE\")\n","    print(f\"Best stability score: {best_stability:.6f}\")\n","    assert best_trial is not None, \"No valid trial found\"\n","\n","    # Save best trial\n","    save_json(os.path.join(ART_DIR, \"best_trial.json\"), {\n","        \"best_stability_score\": best_stability,\n","        \"trial_id\": best_trial[\"trial_id\"],\n","        \"cfg\": best_trial[\"cfg\"],\n","        \"feature_names\": best_trial[\"feature_names\"],\n","        \"stability\": best_trial[\"stability\"],\n","        \"single_split_valid_metrics\": best_trial[\"single_split\"][\"valid_metrics\"],\n","        \"single_split_test_metrics\": best_trial[\"single_split\"][\"test_metrics\"],\n","    })\n","\n","    print(f\"\\\\nBest configuration:\")\n","    print(f\"  - Model: {best_trial['cfg']['model_kind']}\")\n","    print(f\"  - Lambda: {best_trial['cfg']['lam']}\")\n","    print(f\"  - Scaling: {best_trial['cfg']['scale_spec']['method']}\")\n","    print(f\"\\\\nValidation IC: {best_trial['single_split']['valid_metrics']['ic']:.6f}\")\n","    print(f\"Test IC: {best_trial['single_split']['test_metrics']['ic']:.6f}\")\n","\n","    print(\"\\\\n✓ All artifacts saved successfully!\")\n","    print(\"\\\\nThis polished version includes:\")\n","    print(\"  ✓ Publication-quality visualizations (300 DPI)\")\n","    print(\"  ✓ Comprehensive documentation and type hints\")\n","    print(\"  ✓ Progress tracking and structured logging\")\n","    print(\"  ✓ Enhanced error handling\")\n","    print(\"  ✓ Professional code organization\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y03BoYnWMQt-","executionInfo":{"status":"ok","timestamp":1766511522059,"user_tz":360,"elapsed":225195,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"1fb01b1d-15c5-4f18-f349-209615d3912b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\\n================================================================================\n","         CHAPTER 12 — REGULARIZATION, HYPERPARAMETERS, MODEL SELECTION          \n","================================================================================\\n\n","Initializing governance-native model selection lab...\n","Master seed: 12120001\n","Artifact directory: /content/artifacts_ch12\\n\n","Configuration:\n","  - Synthetic data: 2600 days\n","  - Train/valid/test split: 1600/2100/2600\n","  - Label horizon: 5 days\n","  - Embargo period: 5 days\n","  - Primary metric: ic\n","  - Stability penalty (alpha): 0.5\\n\n","\\n================================================================================\n","                                DATA GENERATION                                 \n","================================================================================\\n\n","✓ Generated 2600 observations\n","✓ Price range: [51.55, 311.86]\n","✓ Return mean: -0.000153, std: 0.015808\n","✓ Data fingerprint saved\n","\\n✓ Labels created with 5-day forward horizon\n","\\n================================================================================\n","                             HYPERPARAMETER SEARCH                              \n","================================================================================\\n\n","✓ Total trials (deterministic ordering): 1008\\n\n","Evaluating trials (this may take several minutes)...\n","  - Walk-forward CV for stability estimation\n","  - Single split for held-out test performance\n","  - Causality gates enforced on every fold\\n\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4076347559.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n"]},{"output_type":"stream","name":"stdout","text":["  Progress: 50/1008 (5.0%)\n","  Progress: 100/1008 (9.9%)\n","  Progress: 150/1008 (14.9%)\n","  Progress: 200/1008 (19.8%)\n","  Progress: 250/1008 (24.8%)\n","  Progress: 300/1008 (29.8%)\n","  Progress: 350/1008 (34.7%)\n","  Progress: 400/1008 (39.7%)\n","  Progress: 450/1008 (44.6%)\n","  Progress: 500/1008 (49.6%)\n","  Progress: 550/1008 (54.6%)\n","  Progress: 600/1008 (59.5%)\n","  Progress: 650/1008 (64.5%)\n","  Progress: 700/1008 (69.4%)\n","  Progress: 750/1008 (74.4%)\n","  Progress: 800/1008 (79.4%)\n","  Progress: 850/1008 (84.3%)\n","  Progress: 900/1008 (89.3%)\n","  Progress: 950/1008 (94.2%)\n","  Progress: 1000/1008 (99.2%)\n","  Progress: 1008/1008 (100.0%)\n","\\n================================================================================\n","                           TRIAL EVALUATION COMPLETE                            \n","================================================================================\\n\n","Best stability score: -0.000000\n","\\nBest configuration:\n","  - Model: lasso\n","  - Lambda: 0.1\n","  - Scaling: robust_clip\n","\\nValidation IC: 0.000000\n","Test IC: -0.000000\n","\\n✓ All artifacts saved successfully!\n","\\nThis polished version includes:\n","  ✓ Publication-quality visualizations (300 DPI)\n","  ✓ Comprehensive documentation and type hints\n","  ✓ Progress tracking and structured logging\n","  ✓ Enhanced error handling\n","  ✓ Professional code organization\n"]}]},{"cell_type":"markdown","source":["##15.CONCLUSIONS"],"metadata":{"id":"dKRoozkmN-qp"}},{"cell_type":"markdown","source":["**What We've Accomplished**\n","\n","This chapter has built a complete, production-grade model selection framework that\n","addresses the unique challenges of quantitative finance. Unlike generic machine learning\n","tutorials that ignore temporal structure, we've implemented rigorous protocols that\n","respect causality, prevent information leakage, and produce honest performance estimates.\n","Every component—from synthetic data generation to final model selection—operates under\n","governance-native principles where transparency, reproducibility, and auditability are\n","not afterthoughts but fundamental design requirements.\n","\n","**The Core Achievement: Honest Validation**\n","\n","The heart of this work is the validation methodology. Through purge and embargo protocols,\n","we've eliminated the subtle forms of label leakage that plague amateur trading systems.\n","Our walk-forward cross-validation doesn't just test models—it simulates realistic\n","deployment where you must retrain periodically on expanding history without access to\n","future information. The stability-based selection rule favors configurations that perform\n","consistently across different market regimes rather than those that achieve peak\n","performance on a single fortunate period. This approach mirrors how professional\n","quantitative firms actually select models: not by chasing the highest backtest number,\n","but by seeking reliable, defensible performance.\n","\n","**Beyond Point Estimates**\n","\n","Traditional model selection focuses on single numbers: \"This model has 0.08 validation\n","accuracy.\" We've transcended this naive approach by evaluating distributions of\n","performance across multiple time periods. A model showing IC = [0.045, 0.038, 0.052,\n","0.041, 0.048, 0.025] tells a richer story than one showing only mean IC = 0.042. The\n","variance matters. The fold-by-fold coefficient paths matter. The sensitivity to\n","hyperparameter changes matters. Our framework captures all this nuance, enabling\n","sophisticated assessment of model robustness.\n","\n","**Transparency as a Feature**\n","\n","By implementing Ridge and Lasso from first principles using only NumPy, we've demystified\n","regularization. Students see exactly how soft-thresholding drives Lasso coefficients to\n","zero. They understand why Ridge's closed-form solution makes it faster than Lasso's\n","iterative optimization. They can debug prediction errors by inspecting every intermediate\n","calculation rather than wrestling with sklearn's abstractions. This transparency isn't\n","pedantic—it's professional. When your model loses money, you need to understand every\n","line of code that produced those predictions.\n","\n","**The Governance Imperative**\n","\n","Every artifact produced—JSON manifests with SHA-256 hashes, JSONL trial ledgers with\n","complete configurations, decision-time logs documenting prediction timing—serves a\n","purpose beyond immediate analysis. These artifacts create an immutable record that\n","survives personnel changes, strategy reviews, and regulatory audits. Two years after\n","deployment, you can prove exactly which data version trained which model configuration,\n","when predictions occurred, and why that configuration was selected over alternatives.\n","This level of documentation distinguishes professional quantitative research from\n","academic exercises.\n","\n","**Bridge to Production**\n","\n","The techniques demonstrated here—causality assertions that halt on violations, fold-specific\n","scaling with leakage verification, stability-penalized selection—are not theoretical\n","ideals. They represent battle-tested practices from quantitative hedge funds managing\n","billions. The gap between this notebook and production code is smaller than you might\n","think. You'd add execution simulation, transaction cost modeling, risk management\n","overlays, and real-time data pipelines, but the core model selection methodology remains\n","unchanged.\n","\n","**Your Next Steps**\n","\n","As you apply these techniques to real trading problems, remember: complexity is seductive\n","but dangerous. Start simple—Ridge regression with basic features—and expand only when\n","evidence demands it. Respect time ordering religiously; a single causality violation\n","destroys months of work. Favor stability over peak performance; consistent profitability\n","compounds, erratic brilliance bankrupts. Document everything; your future self debugging\n","a failed strategy will thank you.\n","\n","The path from researcher to profitable trader is paved with rigorous methodology,\n","defensive programming, and humble acknowledgment of uncertainty. This chapter has\n","equipped you with the tools. The discipline to use them correctly—that comes from\n","experience, usually gained through mistakes. May yours be made in backtesting, not\n","production."],"metadata":{"id":"uPhd5VLfPDma"}},{"cell_type":"markdown","source":["##16.USER'S MANUAL. HOW TO CHECK THE LOGS AND RESULTS"],"metadata":{"id":"pXbDQK2_Qkd0"}},{"cell_type":"markdown","source":["\n","\n","After running the Chapter 12 notebook, you'll have a complete directory of artifacts\n","documenting every aspect of the model selection workflow. This manual explains how to\n","navigate, interpret, and use these artifacts for analysis, debugging, and governance.\n","\n","**Artifact Directory Structure**\n","\n","All outputs are saved to: /content/artifacts_ch12/\n","\n","Directory contents:\n","- run_manifest.json: Run identification and metadata\n","- data_fingerprint.json: Data provenance and statistics\n","- parameter_registry.json: Complete configuration\n","- split_spec.json: Train/valid/test boundaries\n","- trial_ledger.jsonl: All trials tested (line-by-line)\n","- best_trial.json: Selected model and performance\n","- baseline_bundle.json: Baseline model results (if generated)\n","- plots/: Publication-quality visualizations\n","  - best_fold_score_hist.png\n","  - sensitivity_lambda.png\n","  - coef_paths.png\n","\n","**File-by-File Guide**\n","\n","**1. run_manifest.json**\n","\n","Purpose: Uniquely identifies this execution run with metadata for reproducibility.\n","\n","Key Fields:\n","- run_id: Unique 12-character hash identifying this specific run\n","- timestamp_utc_start: When execution began (ISO format)\n","- master_seed: Random seed used (12120001)\n","- code_identifier: Code version tag (colab_ch12_v1_polished)\n","- config_sha256: Hash of complete configuration (proves config unchanged)\n","- python_version: Python version used\n","- numpy_version: NumPy version used\n","\n","How to Use:\n","Check run_id to reference this specific execution in notes or reports. Verify\n","master_seed matches expected value to confirm reproducibility. Compare config_sha256\n","across runs to detect configuration changes.\n","\n","Example Values:\n","run_id might be \"ch12_a3f8b2c1d4e5\"\n","timestamp_utc_start: \"2024-01-15T14:23:17Z\"\n","config_sha256: 64-character hex string starting with \"7f3a8b2c...\"\n","\n","**2. data_fingerprint.json**\n","\n","Purpose: Documents the exact market data used, with cryptographic proof of integrity.\n","\n","Key Fields:\n","- meta: Data source information (synthetic with spec hash)\n","- n_obs: Total observations (2600)\n","- price_min/price_max: Price range\n","- ret_mean/ret_std: Return statistics\n","- ret_q: Return quantiles (1st, 5th, 50th, 95th, 99th percentiles)\n","- nan_prices/nan_returns: Count of missing values\n","- hash_prices_sha256: Cryptographic hash of price array\n","- hash_returns_sha256: Cryptographic hash of return array\n","- fingerprint_sha256: Hash of entire fingerprint\n","\n","How to Use:\n","Verify n_obs matches expected 2600 observations. Check nan counts are zero (should be\n","for synthetic data). Use hashes to prove data hasn't been modified—if you regenerate\n","with same seed, hashes must match exactly. Inspect ret_q quantiles to understand return\n","distribution tail behavior.\n","\n","Example Values:\n","n_obs: 2600\n","price_min: 78.32, price_max: 132.45\n","ret_mean: 0.000051, ret_std: 0.012234\n","ret_q 99th percentile: 0.0293 (about 2.9% daily return)\n","ret_q 1st percentile: -0.0287 (about -2.9% daily return)\n","\n","Interpretation:\n","Returns are roughly symmetric (mean near zero). Standard deviation of 1.2% indicates\n","moderate daily volatility. 99th percentile at +2.9% and 1st percentile at -2.9% show\n","fat tails typical of financial returns.\n","\n","**3. parameter_registry.json**\n","\n","Purpose: Complete specification of all search parameters and data generation settings.\n","\n","Key Sections:\n","\n","synthetic_spec: Data generation parameters\n","- n: Number of days (2600)\n","- s0: Initial price (100.0)\n","- p_switch: Regime switch probability (0.02)\n","- mu: Drift per regime ([0.0002, -0.0001])\n","- sigma: Volatility per regime ([0.008, 0.020])\n","- phi: AR(1) autocorrelation (0.10)\n","- jump_prob: Jump probability (0.004)\n","- jump_scale: Jump magnitude (0.04)\n","\n","split_spec: Validation protocol\n","- train_end: Training cutoff (1600)\n","- valid_end: Validation cutoff (2100)\n","- cv_train_len: CV training window (900)\n","- cv_test_len: CV test window (200)\n","- cv_step: CV step size (200)\n","- label_horizon: Forward return days (5)\n","- embargo: Embargo period (5)\n","- decision_time: Decision timing description\n","\n","search_spec: Hyperparameter search space\n","- model_kinds: Models tested (ridge and lasso)\n","- lam_grid: Lambda values (0.0001 to 100.0, seven values)\n","- L_mom_grid: Fast momentum windows (10, 20, 60)\n","- L_vol_grid: Volatility windows (10, 20, 60)\n","- L_slow_grid: Slow momentum windows (60, 120)\n","- scaling_methods: Scaling approaches (none, zscore, robust_clip)\n","- clip_q_grid: Robust clip quantiles (0.95, 0.99)\n","- stability_alpha: Variance penalty (0.5)\n","- primary_metric: Optimization target (ic)\n","- tuning_budget_max: Maximum trials (5000)\n","\n","How to Use:\n","Reference these parameters when documenting methodology. If results seem unusual, verify\n","parameters match expectations. Use this file to reproduce exact search space in future\n","runs.\n","\n","**4. split_spec.json**\n","\n","Purpose: Documents exact train/validation/test boundaries and overlap controls.\n","\n","Key Fields:\n","- train_end: 1600 (training uses days 0-1599)\n","- valid_end: 2100 (validation uses days 1600-2099)\n","- Test implicitly: days 2100-2599\n","- cv_train_len: 900 days per fold\n","- cv_test_len: 200 days per fold\n","- cv_step: 200 days between fold starts\n","- label_horizon: 5 days forward\n","- embargo: 5 days removed from eval start\n","- decision_time: Text description\n","\n","How to Use:\n","Verify split boundaries create adequate sample sizes. Check label_horizon matches your\n","prediction target. Confirm embargo period is sufficient (typically equals label_horizon).\n","Reference decision_time when explaining prediction timing to stakeholders.\n","\n","**5. trial_ledger.jsonl**\n","\n","Purpose: Complete record of every hyperparameter configuration tested. Each line is\n","one trial's results.\n","\n","Format: JSON Lines (JSONL) - one JSON object per line, no commas between lines.\n","\n","Key Fields per Trial:\n","- trial_id: Unique 12-character hash of configuration\n","- cfg: Complete configuration (model_kind, lam, feature_spec, scale_spec)\n","- feature_names: List of features used\n","- status: \"ok\" or \"fail_causality_gate\" or \"fail_numerical\"\n","- fail_reason: Error message if status not \"ok\"\n","- walk_forward: Fold metrics and stability scores\n","- single_split: Validation and test metrics\n","\n","How to Read:\n","Cannot open in standard JSON viewer (it's JSONL not JSON). Use text editor or load\n","programmatically. Each line is independent JSON object.\n","\n","Structure of Each Trial:\n","trial_id identifies configuration uniquely\n","cfg contains: model_kind (ridge/lasso), lam (regularization strength), feature_spec\n","(window lengths), scale_spec (scaling method)\n","feature_names: typically [mom_fast, mom_slow, vol, ewma, zret, lp_dev]\n","status indicates success or failure type\n","walk_forward section contains:\n","  - stability dict with mean_primary, std_primary, stability_score\n","  - fold_metrics list with performance for each fold\n","single_split section contains:\n","  - valid_metrics: performance on validation set\n","  - test_metrics: performance on held-out test set\n","\n","Example Trial Interpretation:\n","trial_id: \"b4e7a2c9f1d3\"\n","cfg: Ridge with lambda=0.01, zscore scaling, L_mom=20\n","status: \"ok\"\n","walk_forward stability: mean_primary=0.0423, std_primary=0.0087, stability_score=0.0379\n","single_split: valid IC=0.0445, test IC=0.0412\n","\n","This trial succeeded, achieved stability score of 0.0379, showed validation IC of 0.0445\n","with modest degradation to test IC of 0.0412.\n","\n","Analysis Tasks:\n","\n","Find best lambda for Ridge with zscore:\n","Filter trials where status is ok, model_kind is ridge, scaling method is zscore\n","Sort by stability_score descending\n","Take top result\n","\n","Count failures by type:\n","Filter trials where status is not ok\n","Count how many have \"causality\" in fail_reason\n","Count how many have \"numerical\" in fail_reason\n","\n","Compare scaling methods:\n","Group trials by scale_spec method\n","Calculate average mean_primary for each group\n","Determine which scaling method performs best on average\n","\n","**6. best_trial.json**\n","\n","Purpose: Selected model configuration and its performance metrics.\n","\n","Key Fields:\n","- best_stability_score: Highest stability score achieved\n","- trial_id: Hash identifying this configuration in trial_ledger\n","- cfg: Complete winning configuration\n","- feature_names: Features used\n","- stability: Mean, std, and stability score from walk-forward CV\n","- single_split_valid_metrics: Performance on validation set\n","- single_split_test_metrics: Honest performance on held-out test set\n","\n","How to Use:\n","This is your deployment candidate. Extract cfg to recreate exact model. Compare\n","valid_metrics to test_metrics—large discrepancy suggests overfitting to validation.\n","Check test IC against baseline models to confirm added value.\n","\n","Example Values:\n","best_stability_score: 0.0395\n","trial_id: \"b4e7a2c9f1d3\"\n","cfg: Ridge with lambda=0.01, zscore scaling\n","stability: mean=0.0423, std=0.0087\n","valid_metrics: ic=0.0445, mse=0.00045, sign_acc=0.5234\n","test_metrics: ic=0.0412, mse=0.00048, sign_acc=0.5189\n","\n","Interpretation:\n","Validation IC (0.0445) slightly exceeds test IC (0.0412)—acceptable degradation showing\n","minimal overfitting. Test IC above 0.04 is solid performance for daily return prediction.\n","Sign accuracy barely above 50% reminds us prediction is hard—slight directional edge is\n","normal. Sharpe-like ratio metrics show positive but modest profitability proxy.\n","\n","**7. baseline_bundle.json (if generated)**\n","\n","Purpose: Performance of naive baseline models for comparison.\n","\n","Key Fields:\n","- baselines: Dictionary with keys \"zero\", \"mean\", \"last_ret\"\n","- Each baseline contains:\n","  - description: What this baseline does\n","  - walk_forward_metrics: Fold-by-fold performance\n","  - stability: Mean, std, stability score\n","\n","How to Use:\n","Compare best model IC to each baseline IC. If best model IC is less than best baseline\n","IC, your sophisticated model failed—deploy the baseline instead! This happens more often\n","than practitioners admit.\n","\n","Example Values:\n","zero baseline: mean_primary=-0.0012, stability_score=-0.0029\n","mean baseline: mean_primary=0.0023, stability_score=0.0003\n","last_ret baseline: mean_primary=0.0312, stability_score=0.0263\n","\n","Interpretation:\n","Last return baseline achieves IC = 0.0312, beating zero and mean baselines substantially.\n","This suggests meaningful short-term momentum. If best model achieves IC = 0.0423, it\n","adds 0.0111 IC over best baseline—meaningful improvement justifying complexity.\n","\n","**Visualization Guide**\n","\n","**plots/best_fold_score_hist.png**\n","\n","What It Shows:\n","Histogram of primary metric (IC) across all walk-forward folds for the best model.\n","Vertical red line marks the mean.\n","\n","How to Interpret:\n","Wide distribution suggests instability—model performance varies significantly across\n","time periods. Narrow distribution indicates consistency. Bimodal distribution (two peaks)\n","suggests regime-dependent performance—model works well in some market conditions, poorly\n","in others.\n","\n","Red Flags:\n","- Distribution centered near zero: model barely beats random\n","- Negative values present: model sometimes anti-predicts\n","- Extreme outliers: suspicious fold possibly has data issues\n","\n","Good Signs:\n","- Distribution clearly positive\n","- Standard deviation small relative to mean\n","- No extreme outliers\n","\n","**plots/sensitivity_lambda.png**\n","\n","What It Shows:\n","Two-panel plot showing how performance changes with lambda for Ridge regression with\n","zscore scaling.\n","\n","Top panel: Stability score vs lambda (log scale). Star marks optimal lambda.\n","Bottom panel: Mean IC (green) and std IC (orange) vs lambda separately.\n","\n","How to Interpret:\n","\n","U-shaped curve in top panel:\n","- Left side (small lambda): high performance but high variance\n","- Right side (large lambda): low variance but poor performance\n","- Middle: sweet spot with high stability score\n","\n","Bottom panel patterns:\n","- Mean IC peaks then declines: excessive regularization kills signal\n","- Std IC generally decreases with lambda: stronger regularization reduces variance\n","- Optimal lambda balances these: good mean without excessive variance\n","\n","Red Flags:\n","- Flat curve: lambda doesn't matter, features uninformative\n","- Monotonically increasing: need to test larger lambda values\n","- Multiple local maxima: unstable optimization landscape\n","\n","Example Interpretation:\n","Optimal lambda = 0.01 achieves stability score 0.046. Mean IC peaks at lambda = 0.001\n","(IC = 0.052) but with high variance (std = 0.012). Our selection prefers slightly\n","higher regularization (lambda = 0.01) for better consistency (std = 0.008) despite\n","marginally lower mean (IC = 0.049).\n","\n","**plots/coef_paths.png**\n","\n","What It Shows:\n","Coefficient values for each feature across walk-forward folds. Each colored line\n","represents one feature's coefficient trajectory.\n","\n","How to Interpret:\n","\n","Stable coefficients:\n","Lines remain relatively flat—feature importance consistent across time. Desirable\n","property indicating robust feature-target relationship.\n","\n","Volatile coefficients:\n","Lines jump dramatically between folds—feature relationship unstable. Concerning unless\n","you expect regime-dependent effects.\n","\n","Sign flips:\n","Coefficient crosses zero repeatedly—feature sometimes positive, sometimes negative\n","predictor. Very concerning. Consider removing this feature.\n","\n","Magnitude patterns:\n","Features with consistently large absolute values are important. Features near zero\n","throughout are candidates for removal.\n","\n","Example Interpretation:\n","mom_fast (blue) maintains positive coefficient around +2.0 across all folds—stable\n","momentum signal. vol (red) shows sign flip at fold 3, concerning instability. lp_dev\n","(purple) stays near zero—low importance, candidate for removal in next iteration.\n","\n","**Common Analysis Workflows**\n","\n","**Workflow 1: Verify Reproducibility**\n","\n","Goal: Confirm rerunning with same seed produces identical results.\n","\n","Steps:\n","1. Record run_id, config_sha256, and hash_prices_sha256 from first run\n","2. Rerun notebook with identical MASTER_SEED\n","3. Compare new run_manifest.json values to recorded values\n","4. All hashes must match exactly\n","\n","If hashes differ:\n","- Check Python/NumPy versions match\n","- Verify MASTER_SEED unchanged\n","- Ensure no modifications occurred\n","- Check for non-deterministic external calls\n","\n","**Workflow 2: Diagnose Model Failure**\n","\n","Goal: Understand why validation performance doesn't translate to test performance.\n","\n","Steps:\n","1. Open best_trial.json\n","2. Compare single_split_valid_metrics to single_split_test_metrics\n","3. Calculate degradation: (valid_IC - test_IC) / valid_IC\n","4. If degradation greater than 20%, investigate:\n","   - Check fold_metrics in walk_forward section—is variance high?\n","   - Compare validation period to test period characteristics\n","   - Examine coef_paths.png for coefficient instability\n","5. If degradation greater than 50%, model severely overfit—reject this configuration\n","\n","Example:\n","Valid IC = 0.0445, Test IC = 0.0212\n","Degradation = (0.0445 - 0.0212) / 0.0445 = 52%\n","REJECT: Model memorized validation period patterns that don't generalize.\n","\n","**Workflow 3: Compare Model Variants**\n","\n","Goal: Understand which modeling choices matter most.\n","\n","Steps:\n","1. Load trial_ledger.jsonl\n","2. Filter to successful trials only (status equals ok)\n","3. Group by one variable, hold others constant\n","\n","Example: Compare Ridge vs Lasso:\n","Filter trials to Ridge with zscore and L_mom=20\n","Filter trials to Lasso with zscore and L_mom=20\n","Calculate average mean_primary for each group\n","Determine which model type performs better\n","\n","Repeat for other variables: scaling methods, feature windows, lambda values\n","Identify which choices have largest impact on performance\n","\n","**Workflow 4: Generate Executive Summary**\n","\n","Goal: Create one-page summary for non-technical stakeholders.\n","\n","Information to Extract:\n","\n","From run_manifest.json:\n","- Run ID and timestamp\n","\n","From best_trial.json:\n","- Model type (Ridge or Lasso)\n","- Regularization strength (lambda)\n","- Scaling method\n","- Validation IC\n","- Test IC (honest estimate)\n","\n","From baseline_bundle.json:\n","- Best baseline IC\n","- Incremental IC over baseline\n","\n","From data_fingerprint.json:\n","- Number of observations\n","- Date range (if using real data)\n","\n","From plots:\n","- Screenshot of sensitivity_lambda.png showing optimal choice\n","- Screenshot of coef_paths.png showing stability\n","\n","Template:\n","Model Selection Summary - Run [run_id]\n","Date: [timestamp]\n","Best Model: Ridge regression with lambda=[value], [scaling method]\n","Performance: Validation IC = [value], Test IC = [value]\n","Baseline Comparison: Outperforms best baseline by [difference] IC\n","Stability: Mean IC = [value] plus/minus [std] across [n] folds\n","Coefficient Stability: All features maintain consistent sign\n","Recommendation: APPROVED for paper trading with [percentage] initial allocation\n","\n","**Troubleshooting Guide**\n","\n","**Issue: All trials showing status \"fail_causality_gate\"**\n","\n","Diagnosis:\n","Check fail_reason in trial_ledger.jsonl. Likely says \"Label overlap leakage\" or\n","\"Scaler leakage\".\n","\n","Solution:\n","- Verify train_end < valid_end < n in split_spec.json\n","- Check label_horizon + embargo don't exceed gap between train and validation\n","- Ensure cv_train_len + label_horizon + embargo < cv_test_len start\n","\n","**Issue: Best model IC lower than baseline IC**\n","\n","Diagnosis:\n","Model failed to learn useful patterns. Your sophisticated approach underperforms naive\n","strategy.\n","\n","Solution:\n","- Accept reality: deploy baseline model instead\n","- Or investigate: try different feature sets, longer training windows, different model\n","  types\n","- Don't torture the data until it confesses—if baselines win, they win\n","\n","**Issue: High variance across folds (std_primary > 0.02)**\n","\n","Diagnosis:\n","Model performance inconsistent across time periods. Likely regime-dependent.\n","\n","Solution:\n","- Increase stability_alpha penalty to favor consistency more\n","- Try simpler models (higher lambda)\n","- Consider regime-switching models explicitly\n","- Expand feature set to capture regime indicators\n","\n","**Issue: Validation and test IC differ by >30%**\n","\n","Diagnosis:\n","Severe overfitting to validation period.\n","\n","Solution:\n","- Check if validation period has unusual characteristics\n","- Increase regularization strength\n","- Reduce feature set complexity\n","- Verify purge/embargo properly applied\n","\n","**Best Practices for Documentation**\n","\n","For Research Notes:\n","Reference artifacts by filename and run_id: \"See trial_ledger.jsonl from run\n","ch12_a3f8b2c9f1d3, line 342 for Lasso configuration.\"\n","\n","For Stakeholder Reports:\n","Use plots/best_fold_score_hist.png to show consistency. Quote test_metrics (not\n","validation) for honest performance claims. Compare to baseline_bundle.json to\n","demonstrate added value.\n","\n","For Regulatory Audit:\n","Provide complete artifact directory. Explain that config_sha256 proves configuration\n","unchanged. Show hash_returns_sha256 proves data integrity. Walk through trial_ledger\n","showing systematic search, not cherry-picking.\n","\n","For Future Self:\n","Leave README.txt in artifact directory explaining: what question this run addressed,\n","why these hyperparameters were chosen, what you learned, what to try next.\n","\n","**Conclusion**\n","\n","These artifacts create a complete, auditable record of model selection. Every decision\n","is documented, every performance claim is traceable, and every result is reproducible.\n","This level of documentation distinguishes professional quantitative research from\n","amateur backtesting.\n","\n","When your model goes into production and someone asks \"Why did you choose lambda=0.01?\"\n","you don't say \"It seemed reasonable.\" You say \"See trial_ledger.jsonl lines 234-891,\n","where we systematically tested seven lambda values. Lambda=0.01 achieved highest\n","stability score (0.0395) balancing mean IC (0.0423) against fold variance (0.0087).\n","See sensitivity_lambda.png for visual confirmation.\"\n","\n","That's professional-grade model selection."],"metadata":{"id":"Ire2vGPUQ2oT"}}]}