{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPnTjOWXHrIdFyKLHXuGelB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**HIDDEN MARKOV MODEL**\n","---"],"metadata":{"id":"LpvUHNZj3Xum"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"B0q2Qx1c3b_g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5V6llIT3Pw3"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"_76-o9zA3d7o"}},{"cell_type":"markdown","source":["\n","\n","Imagine you're sailing a boat across the ocean. Sometimes the water is calm\n","and predictable. Other times, storms appear and everything becomes turbulent\n","and dangerous. If you could detect when conditions are changing from calm to\n","stormy, you'd adjust your sails differently, right?\n","\n","This notebook teaches your trading system to do exactly that with financial\n","markets. Markets don't stay the same - they shift between different \"regimes\"\n","or states. Sometimes markets are calm with low volatility. Sometimes they're\n","stressed with wild price swings. Sometimes trends persist. Sometimes prices\n","bounce around randomly.\n","\n","**The Core Challenge: Seeing the Invisible**\n","\n","Here's the problem: you can't directly see what regime the market is in. You\n","only see prices moving up and down. The true regime is hidden. This is why\n","we use Hidden Markov Models - mathematical tools designed specifically to\n","figure out hidden states from observable data.\n","\n","Think of it like a doctor diagnosing a patient. The doctor can't see the\n","disease directly, but they observe symptoms - temperature, heart rate, blood\n","pressure. From these observations, they infer what's wrong. Similarly, we\n","observe market returns and volatility patterns, and from these we infer what\n","regime the market is likely in.\n","\n","**Why This Matters for Trading**\n","\n","Once you know what regime you're in, you can adapt your strategy. In calm\n","markets, you might use strategies that bet on tiny price differences. In\n","volatile markets, you might reduce your exposure or follow strong trends\n","instead. You wouldn't use the same strategy in both - that would be like\n","wearing a swimsuit in a snowstorm.\n","\n","This notebook shows you three ways regime detection helps:\n","\n","1. **Risk Overlay**: Automatically reduce your trading size when you detect\n","   the market entering a stressed regime\n","2. **Volatility Targeting**: Adjust your leverage based on predicted volatility\n","   from the regime mixture\n","3. **Strategy Blending**: Smoothly mix different trading strategies based on\n","   regime probabilities\n","\n","**The Discipline: No Cheating**\n","\n","There's a critical rule enforced throughout: NO LOOKING INTO THE FUTURE. This\n","sounds obvious, but it's easy to accidentally cheat. For example, some methods\n","can tell you with high confidence what regime you were in yesterday by looking\n","at what happened today. That's useless for trading - you need to know what\n","regime you're in RIGHT NOW using only past information.\n","\n","The notebook implements strict \"causality gates\" - automatic checks that verify\n","we never use future information. It demonstrates both the \"right way\" (using\n","only past data) and the \"wrong way\" (using future data) so you can see the\n","difference in performance. The wrong way always looks better on paper, but it\n","can't be traded.\n","\n","**What You'll Learn**\n","\n","By the end of this notebook, you'll understand:\n","- How to detect market regimes from price data without seeing the future\n","- How to train models that learn regime patterns from historical data\n","- How to periodically update your models as markets evolve\n","- How to convert regime beliefs into actual trading decisions\n","- How to evaluate whether regime detection actually helps your strategy\n","- How to avoid the trap of using future information that inflates backtests\n","\n","This is practical, production-grade regime detection - the kind institutional\n","traders actually use, with all the boring details about causality, timing,\n","and proper evaluation that separate real trading systems from academic toys."],"metadata":{"id":"LeZTPQ2m4J_U"}},{"cell_type":"code","source":[],"metadata":{"id":"fWeC24o73gBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"G6QXYbJR3giv"}},{"cell_type":"code","source":["\n","import numpy as np\n","import math\n","import json\n","import hashlib\n","import os\n","import time\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","# Set global seed for determinism\n","GLOBAL_SEED = 42\n","np.random.seed(GLOBAL_SEED)\n","\n","# Configuration dictionary\n","CONFIG = {\n","    \"seed\": GLOBAL_SEED,\n","    \"chapter\": 14,\n","    \"description\": \"HMM regime detection with EM training and walk-forward evaluation\",\n","\n","    # Synthetic data settings\n","    \"synthetic\": {\n","        \"T\": 1000,  # Number of time steps\n","        \"K\": 2,     # Number of hidden states (regimes)\n","        \"regime_means\": [0.0005, -0.0002],  # Daily return means per regime\n","        \"regime_vols\": [0.01, 0.025],       # Daily return volatilities per regime\n","        \"true_transition_matrix\": [[0.95, 0.05], [0.10, 0.90]],  # Persistence in regimes\n","        \"initial_probs\": [0.5, 0.5],\n","    },\n","\n","    # Feature engineering\n","    \"features\": {\n","        \"rolling_vol_window\": 20,\n","        \"rolling_mean_window\": 20,\n","        \"drawdown_window\": 60,\n","        \"abs_return_window\": 10,\n","        \"winsorize_n_std\": 3.0,\n","        \"vol_normalize\": True,\n","    },\n","\n","    # HMM settings\n","    \"hmm\": {\n","        \"K\": 2,  # Number of states to estimate\n","        \"emission_family\": \"univariate_gaussian\",\n","        \"variance_floor\": 1e-6,\n","        \"transition_pseudocount\": 0.1,  # Dirichlet-like smoothing\n","        \"em_max_iters\": 100,\n","        \"em_tol\": 1e-4,\n","        \"init_transition_diag\": 0.9,  # Diagonal dominance for A init\n","    },\n","\n","    # Walk-forward evaluation\n","    \"walkforward\": {\n","        \"initial_training_window\": 252,  # ~1 year of daily data\n","        \"refit_cadence\": 21,  # Refit every ~1 month\n","        \"training_window_type\": \"expanding\",  # or \"rolling\"\n","        \"rolling_window_size\": 504,  # ~2 years if rolling\n","    },\n","\n","    # Decision rules\n","    \"decisions\": {\n","        \"risk_overlay_stress_threshold\": 0.6,  # Reduce exposure if P(stress) > threshold\n","        \"risk_overlay_min_exposure\": 0.3,\n","        \"risk_overlay_max_exposure\": 1.0,\n","        \"vol_target_annualized\": 0.15,  # 15% annualized vol target\n","        \"trend_lookback\": 20,\n","        \"meanrev_lookback\": 5,\n","        \"regime_blend_smooth\": True,  # Use posterior probs vs hard labels\n","    },\n","}\n","\n","# Create run_id as hash of config\n","def hash_config(config):\n","    config_str = json.dumps(config, sort_keys=True)\n","    return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n","\n","RUN_ID = hash_config(CONFIG)\n","CONFIG[\"run_id\"] = RUN_ID\n","\n","# Create artifact directory\n","ARTIFACT_DIR = f\"artifacts/ch14_run\"\n","os.makedirs(ARTIFACT_DIR, exist_ok=True)\n","os.makedirs(f\"{ARTIFACT_DIR}/params_by_refit\", exist_ok=True)\n","os.makedirs(f\"{ARTIFACT_DIR}/posteriors\", exist_ok=True)\n","os.makedirs(f\"{ARTIFACT_DIR}/plots\", exist_ok=True)\n","\n","print(f\"Run ID: {RUN_ID}\")\n","print(f\"Artifact directory: {ARTIFACT_DIR}\")\n","print(f\"Configuration: {json.dumps(CONFIG, indent=2)}\")\n","print()\n","\n","# Helper function to save JSON\n","def save_json(path, obj):\n","    with open(path, 'w') as f:\n","        json.dump(obj, f, indent=2)\n","    print(f\"Saved: {path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfGqfv2i3l3a","executionInfo":{"status":"ok","timestamp":1766672447353,"user_tz":360,"elapsed":20,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"fd44aa6e-effb-4ad3-ed98-6e4468db547c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Run ID: b79e468dfa049ad5\n","Artifact directory: artifacts/ch14_run\n","Configuration: {\n","  \"seed\": 42,\n","  \"chapter\": 14,\n","  \"description\": \"HMM regime detection with EM training and walk-forward evaluation\",\n","  \"synthetic\": {\n","    \"T\": 1000,\n","    \"K\": 2,\n","    \"regime_means\": [\n","      0.0005,\n","      -0.0002\n","    ],\n","    \"regime_vols\": [\n","      0.01,\n","      0.025\n","    ],\n","    \"true_transition_matrix\": [\n","      [\n","        0.95,\n","        0.05\n","      ],\n","      [\n","        0.1,\n","        0.9\n","      ]\n","    ],\n","    \"initial_probs\": [\n","      0.5,\n","      0.5\n","    ]\n","  },\n","  \"features\": {\n","    \"rolling_vol_window\": 20,\n","    \"rolling_mean_window\": 20,\n","    \"drawdown_window\": 60,\n","    \"abs_return_window\": 10,\n","    \"winsorize_n_std\": 3.0,\n","    \"vol_normalize\": true\n","  },\n","  \"hmm\": {\n","    \"K\": 2,\n","    \"emission_family\": \"univariate_gaussian\",\n","    \"variance_floor\": 1e-06,\n","    \"transition_pseudocount\": 0.1,\n","    \"em_max_iters\": 100,\n","    \"em_tol\": 0.0001,\n","    \"init_transition_diag\": 0.9\n","  },\n","  \"walkforward\": {\n","    \"initial_training_window\": 252,\n","    \"refit_cadence\": 21,\n","    \"training_window_type\": \"expanding\",\n","    \"rolling_window_size\": 504\n","  },\n","  \"decisions\": {\n","    \"risk_overlay_stress_threshold\": 0.6,\n","    \"risk_overlay_min_exposure\": 0.3,\n","    \"risk_overlay_max_exposure\": 1.0,\n","    \"vol_target_annualized\": 0.15,\n","    \"trend_lookback\": 20,\n","    \"meanrev_lookback\": 5,\n","    \"regime_blend_smooth\": true\n","  },\n","  \"run_id\": \"b79e468dfa049ad5\"\n","}\n","\n"]}]},{"cell_type":"markdown","source":["##3.CODE AND IMPLEMENTATION"],"metadata":{"id":"grQz9PF43mz4"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"sDRvIMcw4myt"}},{"cell_type":"markdown","source":["\n","\n","This section builds a fake market where we actually know the truth - we know\n","exactly when regimes change. Think of this as creating a flight simulator\n","before flying a real plane. We need to test our regime detection methods on\n","data where we have the answer key.\n","\n","**What We're Simulating:**\n","\n","We create a market that switches between two states - let's call them \"Normal\"\n","and \"Stressed.\" In the Normal regime, daily returns average around 0.05% with\n","low volatility (1% standard deviation). In the Stressed regime, returns average\n","slightly negative (-0.02%) with much higher volatility (2.5% standard deviation).\n","\n","The market doesn't jump randomly between these states. Once you're in Normal,\n","you're likely to stay Normal (95% chance each day). Once you're in Stressed,\n","you tend to stay Stressed too (90% chance). This persistence mimics real\n","markets - calm periods last for weeks or months, and crises don't flip on and\n","off daily.\n","\n","**Why This Matters:**\n","\n","With synthetic data, we can measure how well our regime detection works because\n","we know the ground truth. Did we detect the regime switch quickly? Did we have\n","false alarms? Are we too slow to react? We can't answer these questions with\n","real market data because we never truly know what regime we were in - we can\n","only guess.\n","\n","**What Gets Created:**\n","\n","We generate 1,000 days of simulated returns. We also create a synthetic volume\n","proxy that behaves differently in each regime - higher and more variable during\n","stress periods, quieter during calm periods. This gives us multiple observable\n","signals to work with.\n","\n","**The Fingerprint:**\n","\n","Everything about this synthetic data gets documented - how many days in each\n","regime, the exact parameters used, any missing data points. This \"data\n","fingerprint\" becomes part of our audit trail. If someone questions our results\n","six months from now, we can recreate this exact dataset.\n","\n","**Visualizations:**\n","\n","We plot three things: the synthetic returns (what a trader would see), the true\n","regime path (what's hidden), and the volume proxy. Looking at these together,\n","you can often see how returns behave differently in different regimes - but\n","remember, in real trading, you wouldn't have that middle plot showing the true\n","regimes. That's what we're trying to discover.\n","```"],"metadata":{"id":"bWKsWXU84qkG"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"PQacRt_S4q41"}},{"cell_type":"code","source":["print(\"=\" * 80)\n","print(\"GENERATING SYNTHETIC MARKET DATA WITH TRUE REGIMES\")\n","print(\"=\" * 80)\n","\n","T = CONFIG[\"synthetic\"][\"T\"]\n","K = CONFIG[\"synthetic\"][\"K\"]\n","true_A = np.array(CONFIG[\"synthetic\"][\"true_transition_matrix\"])\n","true_pi = np.array(CONFIG[\"synthetic\"][\"initial_probs\"])\n","regime_means = np.array(CONFIG[\"synthetic\"][\"regime_means\"])\n","regime_vols = np.array(CONFIG[\"synthetic\"][\"regime_vols\"])\n","\n","# Simulate latent state sequence\n","s_true = np.zeros(T, dtype=int)\n","s_true[0] = np.random.choice(K, p=true_pi)\n","\n","for t in range(1, T):\n","    s_true[t] = np.random.choice(K, p=true_A[s_true[t-1]])\n","\n","# Simulate returns conditional on state\n","returns = np.zeros(T)\n","for t in range(T):\n","    k = s_true[t]\n","    returns[t] = np.random.normal(regime_means[k], regime_vols[k])\n","\n","# Optional: simulate a volume proxy that differs by regime\n","volume_proxy = np.zeros(T)\n","for t in range(T):\n","    k = s_true[t]\n","    # Higher volume in stress regime (k=1 assumed stress)\n","    volume_mean = 1.0 if k == 0 else 1.5\n","    volume_proxy[t] = np.random.lognormal(mean=np.log(volume_mean), sigma=0.3)\n","\n","print(f\"Generated {T} time steps with {K} regimes\")\n","print(f\"True regime distribution: {np.bincount(s_true) / T}\")\n","print(f\"Return stats by regime:\")\n","for k in range(K):\n","    regime_mask = (s_true == k)\n","    regime_returns = returns[regime_mask]\n","    print(f\"  Regime {k}: mean={regime_returns.mean():.6f}, std={regime_returns.std():.6f}, count={regime_mask.sum()}\")\n","print()\n","\n","# Save data fingerprint\n","data_fingerprint = {\n","    \"run_id\": RUN_ID,\n","    \"instruments\": \"synthetic\",\n","    \"frequency\": \"daily\",\n","    \"span\": {\"start\": 0, \"end\": T-1},\n","    \"T\": T,\n","    \"missingness\": {\n","        \"returns_nan_count\": int(np.isnan(returns).sum()),\n","        \"volume_nan_count\": int(np.isnan(volume_proxy).sum()),\n","    },\n","    \"true_regime_distribution\": {f\"regime_{k}\": float(np.bincount(s_true)[k] / T) for k in range(K)},\n","    \"timestamp\": datetime.now().isoformat(),\n","}\n","save_json(f\"{ARTIFACT_DIR}/data_fingerprint.json\", data_fingerprint)\n","\n","# Plot synthetic data and true regimes\n","fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n","\n","axes[0].plot(returns, linewidth=0.5, alpha=0.7)\n","axes[0].set_ylabel(\"Returns\")\n","axes[0].set_title(\"Synthetic Returns\")\n","axes[0].grid(True, alpha=0.3)\n","\n","axes[1].plot(s_true, drawstyle='steps-post', linewidth=1.5, color='black')\n","axes[1].set_ylabel(\"True Regime\")\n","axes[1].set_title(\"True Regime Path (Ground Truth)\")\n","axes[1].set_yticks(range(K))\n","axes[1].grid(True, alpha=0.3)\n","\n","axes[2].plot(volume_proxy, linewidth=0.8, alpha=0.7, color='purple')\n","axes[2].set_ylabel(\"Volume Proxy\")\n","axes[2].set_xlabel(\"Time\")\n","axes[2].set_title(\"Synthetic Volume Proxy\")\n","axes[2].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{ARTIFACT_DIR}/plots/synthetic_data.png\", dpi=150, bbox_inches='tight')\n","plt.close()\n","print(f\"Saved plot: {ARTIFACT_DIR}/plots/synthetic_data.png\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVOX2DXz5I30","executionInfo":{"status":"ok","timestamp":1766672606517,"user_tz":360,"elapsed":2553,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"b53629aa-cc4c-4a23-9cb7-46ab0ffdfcb7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","GENERATING SYNTHETIC MARKET DATA WITH TRUE REGIMES\n","================================================================================\n","Generated 1000 time steps with 2 regimes\n","True regime distribution: [0.681 0.319]\n","Return stats by regime:\n","  Regime 0: mean=0.001820, std=0.009933, count=681\n","  Regime 1: mean=0.000506, std=0.024353, count=319\n","\n","Saved: artifacts/ch14_run/data_fingerprint.json\n","Saved plot: artifacts/ch14_run/plots/synthetic_data.png\n","\n"]}]},{"cell_type":"markdown","source":["##4.FEATURE ENGINEERING"],"metadata":{"id":"SWHPCNTY5Yyp"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"ngGxcvgH5fUk"}},{"cell_type":"markdown","source":["\n","\n","\n","This section creates the observable signals we'll use to detect regimes. Think\n","of these as the vital signs a doctor monitors - each one tells you something\n","about the underlying condition of the market.\n","\n","**The Cardinal Rule: No Future Information**\n","\n","Every feature must be calculated using ONLY past data. This sounds simple but\n","it's where most trading systems break. If you calculate today's volatility\n","using tomorrow's data, your backtest will look amazing but your live trading\n","will fail miserably. We enforce this strictly.\n","\n","**The Four Features We Build:**\n","\n","1. **Rolling Volatility**: How wild have returns been over the past 20 days?\n","   High volatility often signals stressed regimes. We calculate standard\n","   deviation of recent returns - a classic turbulence measure.\n","\n","2. **Rolling Mean**: What's the average return over the past 20 days? This\n","   captures whether we're in a trending environment or not.\n","\n","3. **Drawdown Proxy**: What's the worst return we've seen in the past 60 days?\n","   This measures pain - stressed regimes typically show deeper drawdowns.\n","\n","4. **Absolute Return Average**: How big are price moves on average (ignoring\n","   direction) over the past 10 days? This captures activity level independent\n","   of trend.\n","\n","**Why These Specific Features?**\n","\n","These aren't random choices. Practitioners know that regime shifts often appear\n","first in volatility changes, then in return patterns, then in drawdown\n","severity. We're giving our model multiple ways to \"see\" regime changes.\n","\n","**Data Cleaning Without Cheating:**\n","\n","We apply two cleaning steps, both done causally:\n","\n","- **Winsorization**: Extreme outliers get capped using rolling statistics. But\n","  we only use data up to today to decide what's \"extreme\" - no peeking ahead.\n","\n","- **Volatility Normalization**: We scale features by their own recent\n","  volatility. This prevents one noisy feature from dominating the model.\n","\n","**Handling the Warmup Period:**\n","\n","The first 60 days don't have enough history to calculate all features properly.\n","We set these to zero and mark them clearly. Real trading systems have this\n","same warmup - you can't trade the first month after launch because you don't\n","have enough data yet.\n","\n","**The Feature Manifest:**\n","\n","We document exactly how each feature was created - which window lengths, which\n","transformations, which data it depends on. This manifest becomes part of the\n","model documentation. If a feature behaves strangely in production, we can trace\n","back to its exact construction.\n"],"metadata":{"id":"RA5nSpS95hPm"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"lte2nCQS5h03"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"FEATURE ENGINEERING (CAUSALITY-SAFE ROLLING)\")\n","print(\"=\" * 80)\n","\n","vol_window = CONFIG[\"features\"][\"rolling_vol_window\"]\n","mean_window = CONFIG[\"features\"][\"rolling_mean_window\"]\n","dd_window = CONFIG[\"features\"][\"drawdown_window\"]\n","abs_ret_window = CONFIG[\"features\"][\"abs_return_window\"]\n","winsorize_n_std = CONFIG[\"features\"][\"winsorize_n_std\"]\n","vol_normalize = CONFIG[\"features\"][\"vol_normalize\"]\n","\n","# Manual rolling volatility (std of past L returns)\n","def rolling_std(x, window):\n","    \"\"\"Causal rolling standard deviation.\"\"\"\n","    result = np.full(len(x), np.nan)\n","    for t in range(window-1, len(x)):\n","        result[t] = np.std(x[t-window+1:t+1], ddof=1)\n","    return result\n","\n","# Manual rolling mean\n","def rolling_mean(x, window):\n","    \"\"\"Causal rolling mean.\"\"\"\n","    result = np.full(len(x), np.nan)\n","    for t in range(window-1, len(x)):\n","        result[t] = np.mean(x[t-window+1:t+1])\n","    return result\n","\n","# Drawdown proxy: cumulative min over window\n","def rolling_drawdown(x, window):\n","    \"\"\"Causal rolling drawdown (simplified: min return over window).\"\"\"\n","    result = np.full(len(x), np.nan)\n","    for t in range(window-1, len(x)):\n","        result[t] = np.min(x[t-window+1:t+1])\n","    return result\n","\n","# Absolute return average\n","def rolling_abs_mean(x, window):\n","    \"\"\"Causal rolling mean of absolute returns.\"\"\"\n","    result = np.full(len(x), np.nan)\n","    for t in range(window-1, len(x)):\n","        result[t] = np.mean(np.abs(x[t-window+1:t+1]))\n","    return result\n","\n","# Compute features\n","feat_rolling_vol = rolling_std(returns, vol_window)\n","feat_rolling_mean = rolling_mean(returns, mean_window)\n","feat_drawdown = rolling_drawdown(returns, dd_window)\n","feat_abs_return = rolling_abs_mean(returns, abs_ret_window)\n","\n","# Stack features into matrix (T x D)\n","# We'll use rolling_vol as primary feature for univariate HMM\n","features_list = [feat_rolling_vol, feat_rolling_mean, feat_drawdown, feat_abs_return]\n","X = np.column_stack(features_list)  # Shape: (T, 4)\n","\n","# Winsorization: causal clipping using rolling mean ± k*std\n","def causal_winsorize(x, n_std):\n","    \"\"\"Winsorize using rolling mean and std.\"\"\"\n","    result = x.copy()\n","    window = 60  # Use a reasonable window for rolling stats\n","    for t in range(window, len(x)):\n","        mu = np.mean(x[t-window:t])\n","        sigma = np.std(x[t-window:t], ddof=1)\n","        lower = mu - n_std * sigma\n","        upper = mu + n_std * sigma\n","        result[t] = np.clip(result[t], lower, upper)\n","    return result\n","\n","for d in range(X.shape[1]):\n","    X[:, d] = causal_winsorize(X[:, d], winsorize_n_std)\n","\n","# Volatility normalization (optional)\n","if vol_normalize:\n","    # Normalize by rolling vol of the feature itself\n","    for d in range(X.shape[1]):\n","        feat_vol = rolling_std(X[:, d], vol_window)\n","        # Avoid division by zero\n","        feat_vol = np.where(feat_vol < 1e-8, 1e-8, feat_vol)\n","        X[:, d] = X[:, d] / feat_vol\n","\n","# Handle NaNs: forward fill or set to zero after warmup\n","first_valid = max(vol_window, mean_window, dd_window, abs_ret_window)\n","X[:first_valid, :] = 0.0  # Warmup period\n","for d in range(X.shape[1]):\n","    for t in range(first_valid, T):\n","        if np.isnan(X[t, d]):\n","            X[t, d] = X[t-1, d]  # Forward fill\n","\n","print(f\"Feature matrix shape: {X.shape}\")\n","print(f\"First valid index: {first_valid}\")\n","print(f\"NaN count in X: {np.isnan(X).sum()}\")\n","print(f\"Feature statistics (after warmup):\")\n","for d in range(X.shape[1]):\n","    print(f\"  Feature {d}: mean={X[first_valid:, d].mean():.6f}, std={X[first_valid:, d].std():.6f}\")\n","print()\n","\n","# Save feature manifest\n","feature_manifest = {\n","    \"run_id\": RUN_ID,\n","    \"features\": [\n","        {\"name\": \"rolling_vol\", \"window\": vol_window, \"index\": 0},\n","        {\"name\": \"rolling_mean\", \"window\": mean_window, \"index\": 1},\n","        {\"name\": \"drawdown_proxy\", \"window\": dd_window, \"index\": 2},\n","        {\"name\": \"abs_return_avg\", \"window\": abs_ret_window, \"index\": 3},\n","    ],\n","    \"winsorize_n_std\": winsorize_n_std,\n","    \"vol_normalize\": vol_normalize,\n","    \"first_valid_index\": first_valid,\n","    \"timestamp\": datetime.now().isoformat(),\n","}\n","save_json(f\"{ARTIFACT_DIR}/feature_manifest.json\", feature_manifest)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CRitwcb5pV7","executionInfo":{"status":"ok","timestamp":1766672736926,"user_tz":360,"elapsed":302,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"9139df87-f95d-4232-c56f-483ff937651d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FEATURE ENGINEERING (CAUSALITY-SAFE ROLLING)\n","================================================================================\n","Feature matrix shape: (1000, 4)\n","First valid index: 60\n","NaN count in X: 0\n","Feature statistics (after warmup):\n","  Feature 0: mean=9.472184, std=6.133564\n","  Feature 1: mean=0.979152, std=1.860916\n","  Feature 2: mean=-2041141.183039, std=2626591.877243\n","  Feature 3: mean=5.835010, std=3.239365\n","\n","Saved: artifacts/ch14_run/feature_manifest.json\n"]}]},{"cell_type":"markdown","source":["##5.CAUSALITY GATES"],"metadata":{"id":"hgUfD_6H6cLk"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"ALsfHBDS6fGf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"QWGfyG-76iBp"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"QHYpTfpV6iW2"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"CAUSALITY GATES: PREFIX INVARIANCE TESTS\")\n","print(\"=\" * 80)\n","\n","# Test: recompute features at selected times using only data up to that time\n","test_times = [200, 400, 600, 800]\n","\n","for test_t in test_times:\n","    if test_t < first_valid:\n","        continue\n","\n","    # Recompute features using only data up to test_t\n","    returns_prefix = returns[:test_t+1]\n","\n","    feat_vol_prefix = rolling_std(returns_prefix, vol_window)\n","    feat_mean_prefix = rolling_mean(returns_prefix, mean_window)\n","    feat_dd_prefix = rolling_drawdown(returns_prefix, dd_window)\n","    feat_abs_prefix = rolling_abs_mean(returns_prefix, abs_ret_window)\n","\n","    X_prefix = np.column_stack([feat_vol_prefix, feat_mean_prefix, feat_dd_prefix, feat_abs_prefix])\n","\n","    # Apply same transformations\n","    for d in range(X_prefix.shape[1]):\n","        X_prefix[:, d] = causal_winsorize(X_prefix[:, d], winsorize_n_std)\n","\n","    if vol_normalize:\n","        for d in range(X_prefix.shape[1]):\n","            feat_vol_d = rolling_std(X_prefix[:, d], vol_window)\n","            feat_vol_d = np.where(feat_vol_d < 1e-8, 1e-8, feat_vol_d)\n","            X_prefix[:, d] = X_prefix[:, d] / feat_vol_d\n","\n","    # Compare with precomputed X[test_t]\n","    diff = np.abs(X_prefix[test_t, :] - X[test_t, :])\n","    max_diff = np.max(diff)\n","\n","    print(f\"Time {test_t}: max absolute difference = {max_diff:.10f}\")\n","\n","    assert max_diff < 1e-6, f\"PREFIX INVARIANCE FAILED at t={test_t}: diff={max_diff}\"\n","\n","print(\"✓ All prefix invariance tests PASSED\")\n","print()\n","\n","# Assert no NaNs and strict ordering (time index is implicit)\n","assert not np.any(np.isnan(X)), \"CAUSALITY GATE FAILED: NaNs found in feature matrix\"\n","print(\"✓ No NaNs in feature matrix\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMv6cU956lxp","executionInfo":{"status":"ok","timestamp":1766673031956,"user_tz":360,"elapsed":970,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"78d44895-7a74-495e-d0a4-19e8460d4e04"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","CAUSALITY GATES: PREFIX INVARIANCE TESTS\n","================================================================================\n","Time 200: max absolute difference = 0.0000000000\n","Time 400: max absolute difference = 0.0000000000\n","Time 600: max absolute difference = 0.0000000000\n","Time 800: max absolute difference = 0.0000000000\n","✓ All prefix invariance tests PASSED\n","\n","✓ No NaNs in feature matrix\n","\n"]}]},{"cell_type":"markdown","source":["##6.EMISSION LIKELIHOODS"],"metadata":{"id":"R_nVNRXA6z2l"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"i6PnOMn_68cz"}},{"cell_type":"markdown","source":["\n","\n","**Part 1: Understanding Hidden Markov Models for Practitioners**\n","\n","Hidden Markov Models are the workhorse for regime detection in institutional\n","trading. Let's break down what they actually do in plain English.\n","\n","Imagine you're locked in a windowless room trying to figure out the weather\n","outside. You can't see if it's sunny or rainy (the hidden state), but you can\n","hear sounds - birds chirping, rain hitting the roof, thunder (the observations).\n","Over time, you learn patterns: when you hear birds, it's probably sunny. When\n","you hear rain, it's probably stormy. An HMM does exactly this for markets.\n","\n","**The Three Components:**\n","\n","First, there are hidden states - the market regimes we can't directly observe.\n","We typically start with two: Normal and Stressed. Some practitioners use three\n","(adding a Bull regime) or even four states, but two is the sweet spot for most\n","applications - simple enough to interpret, complex enough to be useful.\n","\n","Second, there are transition probabilities - how likely is the market to switch\n","between regimes? Markets show persistence - calm periods last weeks, crises\n","last weeks. The HMM learns this. It might discover that Normal has a 95% chance\n","of staying Normal tomorrow, and only 5% chance of flipping to Stressed. This\n","persistence is crucial - it prevents the model from seeing regime switches in\n","every random price spike.\n","\n","Third, there are emission probabilities - what observations would we expect to\n","see in each regime? In Normal regime, we might expect low volatility readings\n","(our features from Section 4). In Stressed regime, we expect high volatility\n","readings. The HMM learns the typical \"signature\" of each regime from data.\n","\n","**How It Works in Practice:**\n","\n","Every day, the HMM looks at today's features (volatility, drawdown, etc.) and\n","asks: \"Given what I'm seeing today and what I saw yesterday, what regime am I\n","probably in?\" It doesn't just give you a binary answer - it gives you\n","probabilities. Maybe it says \"70% chance we're in Normal, 30% chance we're in\n","Stressed.\"\n","\n","This probabilistic output is gold for trading. You don't want hard switches -\n","those create whipsaw trades. You want smooth transitions. As the probability\n","of Stressed regime increases from 20% to 40% to 60%, you gradually reduce\n","position sizes. No sudden moves.\n","\n","**The Learning Process:**\n","\n","The HMM learns from historical data using a method called Expectation-\n","Maximization (EM). Think of it as trial and error with math. The model makes\n","an initial guess about regime patterns, then refines that guess by looking at\n","historical data. It asks: \"If these were the regimes, what transitions would\n","explain the data best?\" Then it flips it: \"Given these transitions, what\n","regimes would fit best?\" It iterates back and forth until the story stabilizes.\n","\n","**Why Practitioners Love HMMs:**\n","\n","They're interpretable - you can explain to your CIO what each regime means.\n","They're probabilistic - no false precision. They handle regime persistence\n","naturally. And they've been battle-tested in institutional settings for decades.\n","They're not the fanciest method (neural networks are sexier), but they're\n","reliable, understandable, and they work.\n","\n","**Part 2: Understanding Emission Probabilities (Likelihoods)**\n","\n","Emission probabilities answer the question: \"If I'm in Regime X, how likely am\n","I to see Observation Y?\" This is the bridge between hidden regimes and visible\n","data.\n","\n","For markets, we typically assume each regime generates observations from a\n","Gaussian (normal) distribution. In Normal regime, you might see features with\n","mean volatility of 1% and tight clustering around that mean. In Stressed regime,\n","mean volatility jumps to 2.5% with wider dispersion.\n","\n","Think of it like this: each regime has a \"personality\" defined by its typical\n","feature values. When you observe today's features, you calculate how consistent\n","those observations are with each regime's personality. If today's volatility is\n","1.2%, that's very consistent with Normal (high emission probability) but unusual\n","for Stressed (low emission probability).\n","\n","**The Gaussian Assumption:**\n","\n","We use Gaussian distributions because they're simple and work well in practice.\n","Each regime gets a mean (center point) and variance (spread) for each feature.\n","High variance means that regime is \"messy\" - features vary a lot within that\n","state. Low variance means that regime is stable - features cluster tightly.\n","\n","**Why Log Domain Matters:**\n","\n","Probabilities multiply, and when you multiply many small numbers (like 0.001 ×\n","0.002 × 0.003), computers lose precision and everything becomes zero. So we\n","work in log domain - instead of multiplying probabilities, we add log-\n","probabilities. This is purely a numerical stability trick, but it's essential\n","for robust implementation.\n","\n","**Variance Floors:**\n","\n","In practice, we never let variance drop below a tiny threshold (like 0.000001).\n","Why? Because if variance is truly zero, the model becomes too confident - it\n","thinks it knows exactly what to expect. Markets are never that predictable.\n","The floor prevents overconfidence and numerical explosions when observations\n","fall outside expected ranges.\n","\n","**Part 3: What This Section Actually Does**\n","\n","This section implements the emission likelihood calculation - the core math\n","that lets us score how well today's observations fit each regime hypothesis.\n","\n","**The Function:**\n","\n","We build a function that takes three inputs: an observation (like today's\n","volatility reading), a regime's expected mean, and a regime's variance. It\n","outputs the log-probability of seeing that observation if we're truly in that\n","regime.\n","\n","**Numerical Safeguards:**\n","\n","The implementation includes critical safety features. First, variance floors -\n","we automatically boost any variance below our threshold to prevent numerical\n","instability. Second, we work entirely in log domain to prevent underflow when\n","probabilities get tiny.\n","\n","**Unit Testing:**\n","\n","We don't just implement and hope. We test edge cases: What happens with zero\n","variance? What about extreme observations far in the tail? What about perfectly\n","typical observations at the mean? Each test confirms the function behaves\n","correctly under stress.\n","\n","These tests aren't academic - they catch bugs before they cost money. I've seen\n","production systems fail because nobody tested what happens when volatility\n","spikes to ten times normal levels. We test that here.\n","\n","**Why This Matters:**\n","\n","This function gets called millions of times during model training and\n","evaluation. If it's buggy or slow, everything breaks. If it's numerically\n","unstable, you get nonsense regime probabilities that lead to bad trades.\n","\n","Getting this right - with proper log domain calculations, variance floors, and\n","comprehensive testing - is the difference between a research toy and a\n","production-grade regime detection system."],"metadata":{"id":"rmrz9Fua7lnO"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"fGzuosuw809g"}},{"cell_type":"code","source":["\n","\n","print(\"=\" * 80)\n","print(\"HMM EMISSION LIKELIHOODS (GAUSSIAN, LOG DOMAIN)\")\n","print(\"=\" * 80)\n","\n","variance_floor = CONFIG[\"hmm\"][\"variance_floor\"]\n","\n","def log_gaussian_pdf(x, mu, var):\n","    \"\"\"\n","    Compute log of Gaussian PDF: log N(x | mu, var).\n","\n","    Args:\n","        x: observation (scalar or array)\n","        mu: mean (scalar or array matching x)\n","        var: variance (scalar or array matching x), with floor applied\n","\n","    Returns:\n","        log probability (same shape as x)\n","    \"\"\"\n","    var = np.maximum(var, variance_floor)  # Apply variance floor\n","    log_coeff = -0.5 * np.log(2 * np.pi * var)\n","    log_exp = -0.5 * ((x - mu) ** 2) / var\n","    return log_coeff + log_exp\n","\n","# Unit tests for log_gaussian_pdf\n","print(\"Testing log_gaussian_pdf:\")\n","test_x = 0.0\n","test_mu = 0.0\n","test_var = 1.0\n","log_p = log_gaussian_pdf(test_x, test_mu, test_var)\n","expected = -0.5 * np.log(2 * np.pi)\n","print(f\"  log N(0|0,1) = {log_p:.6f} (expected: {expected:.6f})\")\n","assert np.abs(log_p - expected) < 1e-6, \"Unit test failed for log_gaussian_pdf\"\n","\n","# Test with tiny variance (should use floor)\n","test_var_tiny = 1e-10\n","log_p_floor = log_gaussian_pdf(test_x, test_mu, test_var_tiny)\n","expected_floor = log_gaussian_pdf(test_x, test_mu, variance_floor)\n","print(f\"  log N(0|0,1e-10) with floor = {log_p_floor:.6f}\")\n","assert np.abs(log_p_floor - expected_floor) < 1e-6, \"Variance floor not applied correctly\"\n","\n","# Test with large x (tail probability)\n","test_x_large = 10.0\n","log_p_tail = log_gaussian_pdf(test_x_large, test_mu, test_var)\n","print(f\"  log N(10|0,1) = {log_p_tail:.6f} (should be very negative)\")\n","assert log_p_tail < -40, \"Tail probability not negative enough\"\n","\n","print(\"✓ log_gaussian_pdf unit tests PASSED\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ins4ncFv9IR0","executionInfo":{"status":"ok","timestamp":1766673654730,"user_tz":360,"elapsed":33,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"a2644b8b-7ac6-420c-8dfb-db749f6fc958"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","HMM EMISSION LIKELIHOODS (GAUSSIAN, LOG DOMAIN)\n","================================================================================\n","Testing log_gaussian_pdf:\n","  log N(0|0,1) = -0.918939 (expected: -0.918939)\n","  log N(0|0,1e-10) with floor = 5.988817\n","  log N(10|0,1) = -50.918939 (should be very negative)\n","✓ log_gaussian_pdf unit tests PASSED\n","\n"]}]},{"cell_type":"markdown","source":["## 7.FORWARD ALGORITHM"],"metadata":{"id":"V64Dlvh584jF"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"Bq-X-pgh9Tuv"}},{"cell_type":"markdown","source":["\n","\n","**Part A: Conceptual and Theoretical Foundations of the Forward Algorithm**\n","\n","The Forward Algorithm is how we answer the critical question: \"Given everything\n","I've seen up to today, what regime am I probably in RIGHT NOW?\" This is called\n","filtering, and it's the only regime estimate we're allowed to use for trading\n","decisions.\n","\n","Think of it as a detective gathering evidence sequentially. On day one, you see\n","some observations and form initial beliefs about the regime. On day two, you\n","update those beliefs based on new evidence AND the fact that regimes tend to\n","persist. The Forward Algorithm formalizes this intuitive updating process.\n","\n","**How Beliefs Propagate Forward:**\n","\n","Start with yesterday's regime beliefs - maybe you thought there was 70% chance\n","of Normal, 30% chance of Stressed. Now today arrives with new observations.\n","The algorithm does two things:\n","\n","First, it predicts today's regime using yesterday's beliefs and the transition\n","probabilities. If Normal tends to stay Normal (95% probability), and you\n","thought yesterday was probably Normal (70%), then before seeing today's data,\n","you'd predict roughly 70% chance of Normal today too.\n","\n","Second, it updates this prediction using today's actual observations. If you\n","predicted Normal but today's volatility is screaming Stressed, you revise your\n","belief sharply toward Stressed. The update weighs your prediction against the\n","evidence using Bayes' rule.\n","\n","**Why \"Forward\" Matters:**\n","\n","The algorithm moves forward through time, never looking back. At time t, it\n","uses only data from times 1 through t. This makes it causal - suitable for\n","live trading. You could run this algorithm in real-time as each day's data\n","arrives, and it would give you tradeable regime probabilities.\n","\n","**The Recursion:**\n","\n","Each day builds on the previous day. You take yesterday's filtered beliefs,\n","apply transitions, incorporate today's observations, and get today's filtered\n","beliefs. Tomorrow will build on today. This recursive structure is elegant and\n","computationally efficient - you never need to reprocess historical data.\n","\n","**Normalization:**\n","\n","After updating beliefs, probabilities must sum to 100%. The algorithm includes\n","a normalization step that ensures this. As a side benefit, these normalization\n","factors give us the model's log-likelihood - a measure of how well the HMM\n","explains the data.\n","\n","**Part B: What This Section Does**\n","\n","This section implements the Forward Algorithm with production-grade numerical\n","stability and safety checks. It's the engine that will compute regime\n","probabilities throughout our entire walk-forward evaluation.\n","\n","**The Implementation:**\n","\n","We build a function that takes a sequence of observations (our features over\n","time), the HMM parameters (transition matrix, initial probabilities, emission\n","parameters), and outputs filtered regime probabilities for every time step.\n","\n","The core is a loop that marches forward through time. At each step, it\n","calculates the probability of each possible regime given all prior observations.\n","It does this in log domain for numerical stability - critical when processing\n","hundreds or thousands of time steps.\n","\n","**The Log-Sum-Exp Trick:**\n","\n","We implement a helper function called logsumexp that's essential for stable\n","computation. When you need to add probabilities but you're working in log space,\n","you can't just add the logs. Logsumexp handles this correctly without numerical\n","overflow or underflow.\n","\n","This seems like a technical detail, but I've debugged production systems where\n","missing this caused silent failures - the algorithm returned garbage\n","probabilities that looked plausible but were completely wrong.\n","\n","**Sanity Checks:**\n","\n","After computing filtered probabilities for each time step, we verify they sum\n","to exactly 1.0 (within floating-point tolerance). If they don't, we throw an\n","error immediately. This assertion has caught bugs during development - it's a\n","hard stop that prevents corrupted probabilities from propagating downstream.\n","\n","**What Gets Returned:**\n","\n","The function returns two things: the filtered posteriors (regime probabilities\n","at each time) and the total log-likelihood (how well the model fits the data).\n","The posteriors are what we'll use for trading. The log-likelihood helps us\n","evaluate model quality and detect training problems.\n","\n","**Why This Is the Trading-Legal Algorithm:**\n","\n","Unlike smoothing (which we'll see later), filtering uses only past information.\n","Every probability at time t is computed using only data from times 1 through t.\n","This means you could run this in production, feeding it live data, and the\n","probabilities would be tradeable - no future information, no hindsight bias.\n"],"metadata":{"id":"ySMvx3Zq9jLA"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"Tmp-Gmew9j8r"}},{"cell_type":"code","source":["print(\"=\" * 80)\n","print(\"FORWARD ALGORITHM (FILTERING)\")\n","print(\"=\" * 80)\n","\n","def logsumexp(log_probs):\n","    \"\"\"\n","    Numerically stable log-sum-exp.\n","\n","    Args:\n","        log_probs: array of log probabilities\n","\n","    Returns:\n","        log(sum(exp(log_probs)))\n","    \"\"\"\n","    max_log = np.max(log_probs)\n","    return max_log + np.log(np.sum(np.exp(log_probs - max_log)))\n","\n","def forward_algorithm(X_obs, log_A, log_pi, mus, vars):\n","    \"\"\"\n","    Forward algorithm to compute filtered posteriors in log domain.\n","\n","    Args:\n","        X_obs: observations (T, D) or (T,) for univariate\n","        log_A: log transition matrix (K, K)\n","        log_pi: log initial state probabilities (K,)\n","        mus: emission means (K, D) or (K,) for univariate\n","        vars: emission variances (K, D) or (K,) for univariate\n","\n","    Returns:\n","        gamma_filt: filtered posteriors (T, K) - P(s_t=k | x_1:t)\n","        log_likelihood: total log-likelihood\n","    \"\"\"\n","    if X_obs.ndim == 1:\n","        X_obs = X_obs[:, np.newaxis]  # Make (T, 1)\n","        mus = mus[:, np.newaxis] if mus.ndim == 1 else mus\n","        vars = vars[:, np.newaxis] if vars.ndim == 1 else vars\n","\n","    T, D = X_obs.shape\n","    K = len(log_pi)\n","\n","    # Alpha: log forward probabilities (unnormalized)\n","    log_alpha = np.zeros((T, K))\n","\n","    # Initialize t=0\n","    for k in range(K):\n","        log_emission = np.sum([log_gaussian_pdf(X_obs[0, d], mus[k, d], vars[k, d]) for d in range(D)])\n","        log_alpha[0, k] = log_pi[k] + log_emission\n","\n","    # Normalize to get gamma_filt[0]\n","    log_norm_0 = logsumexp(log_alpha[0])\n","    gamma_filt = np.zeros((T, K))\n","    gamma_filt[0, :] = np.exp(log_alpha[0, :] - log_norm_0)\n","\n","    log_likelihood = log_norm_0\n","\n","    # Forward recursion for t=1..T-1\n","    for t in range(1, T):\n","        for j in range(K):\n","            # Sum over previous states\n","            log_trans_probs = log_alpha[t-1, :] + log_A[:, j]\n","            log_alpha[t, j] = logsumexp(log_trans_probs)\n","\n","            # Add emission\n","            log_emission = np.sum([log_gaussian_pdf(X_obs[t, d], mus[j, d], vars[j, d]) for d in range(D)])\n","            log_alpha[t, j] += log_emission\n","\n","        # Normalize\n","        log_norm_t = logsumexp(log_alpha[t, :])\n","        gamma_filt[t, :] = np.exp(log_alpha[t, :] - log_norm_t)\n","        log_likelihood += log_norm_t\n","\n","    # Sanity check: each gamma_filt[t] should sum to 1\n","    for t in range(T):\n","        assert np.abs(gamma_filt[t, :].sum() - 1.0) < 1e-6, f\"Filtered posterior at t={t} does not sum to 1\"\n","\n","    return gamma_filt, log_likelihood\n","\n","print(\"Forward algorithm implemented.\")\n","print(\"✓ Filtered posteriors will sum to 1 (assertion enforced)\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TufA4BVh9n05","executionInfo":{"status":"ok","timestamp":1766673894623,"user_tz":360,"elapsed":19,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c976c668-de28-446a-c6ad-0b49b3d366ed"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FORWARD ALGORITHM (FILTERING)\n","================================================================================\n","Forward algorithm implemented.\n","✓ Filtered posteriors will sum to 1 (assertion enforced)\n","\n"]}]},{"cell_type":"markdown","source":["### 7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"CUtG7TUm-Fu2"}},{"cell_type":"code","source":["\n","\n","print(\"=\" * 80)\n","print(\"FORWARD ALGORITHM (FILTERING)\")\n","print(\"=\" * 80)\n","\n","def logsumexp(log_probs):\n","    \"\"\"\n","    Numerically stable log-sum-exp.\n","\n","    Args:\n","        log_probs: array of log probabilities\n","\n","    Returns:\n","        log(sum(exp(log_probs)))\n","    \"\"\"\n","    max_log = np.max(log_probs)\n","    return max_log + np.log(np.sum(np.exp(log_probs - max_log)))\n","\n","def forward_algorithm(X_obs, log_A, log_pi, mus, vars):\n","    \"\"\"\n","    Forward algorithm to compute filtered posteriors in log domain.\n","\n","    Args:\n","        X_obs: observations (T, D) or (T,) for univariate\n","        log_A: log transition matrix (K, K)\n","        log_pi: log initial state probabilities (K,)\n","        mus: emission means (K, D) or (K,) for univariate\n","        vars: emission variances (K, D) or (K,) for univariate\n","\n","    Returns:\n","        gamma_filt: filtered posteriors (T, K) - P(s_t=k | x_1:t)\n","        log_likelihood: total log-likelihood\n","    \"\"\"\n","    if X_obs.ndim == 1:\n","        X_obs = X_obs[:, np.newaxis]  # Make (T, 1)\n","        mus = mus[:, np.newaxis] if mus.ndim == 1 else mus\n","        vars = vars[:, np.newaxis] if vars.ndim == 1 else vars\n","\n","    T, D = X_obs.shape\n","    K = len(log_pi)\n","\n","    # Alpha: log forward probabilities (unnormalized)\n","    log_alpha = np.zeros((T, K))\n","\n","    # Initialize t=0\n","    for k in range(K):\n","        log_emission = np.sum([log_gaussian_pdf(X_obs[0, d], mus[k, d], vars[k, d]) for d in range(D)])\n","        log_alpha[0, k] = log_pi[k] + log_emission\n","\n","    # Normalize to get gamma_filt[0]\n","    log_norm_0 = logsumexp(log_alpha[0])\n","    gamma_filt = np.zeros((T, K))\n","    gamma_filt[0, :] = np.exp(log_alpha[0, :] - log_norm_0)\n","\n","    log_likelihood = log_norm_0\n","\n","    # Forward recursion for t=1..T-1\n","    for t in range(1, T):\n","        for j in range(K):\n","            # Sum over previous states\n","            log_trans_probs = log_alpha[t-1, :] + log_A[:, j]\n","            log_alpha[t, j] = logsumexp(log_trans_probs)\n","\n","            # Add emission\n","            log_emission = np.sum([log_gaussian_pdf(X_obs[t, d], mus[j, d], vars[j, d]) for d in range(D)])\n","            log_alpha[t, j] += log_emission\n","\n","        # Normalize\n","        log_norm_t = logsumexp(log_alpha[t, :])\n","        gamma_filt[t, :] = np.exp(log_alpha[t, :] - log_norm_t)\n","        log_likelihood += log_norm_t\n","\n","    # Sanity check: each gamma_filt[t] should sum to 1\n","    for t in range(T):\n","        assert np.abs(gamma_filt[t, :].sum() - 1.0) < 1e-6, f\"Filtered posterior at t={t} does not sum to 1\"\n","\n","    return gamma_filt, log_likelihood\n","\n","print(\"Forward algorithm implemented.\")\n","print(\"✓ Filtered posteriors will sum to 1 (assertion enforced)\")\n","print()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVIYIvLDGWJQ","executionInfo":{"status":"ok","timestamp":1766676065914,"user_tz":360,"elapsed":43,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"cce00d11-8bb3-48ce-dcee-439879d18747"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FORWARD ALGORITHM (FILTERING)\n","================================================================================\n","Forward algorithm implemented.\n","✓ Filtered posteriors will sum to 1 (assertion enforced)\n","\n"]}]},{"cell_type":"markdown","source":["##8.BACKWARD ALGORITHM"],"metadata":{"id":"A5f3LP29GX-j"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"D809dhglGf74"}},{"cell_type":"markdown","source":["\n","**Part A: Conceptual and Theoretical Foundations of the Backward Algorithm**\n","\n","The Backward Algorithm computes smoothed regime probabilities - what regime\n","were we ACTUALLY in at time t, given we now know everything that happened\n","through the end of the data? This is fundamentally different from filtering,\n","and understanding why matters enormously for practitioners.\n","\n","**The Hindsight Advantage:**\n","\n","Imagine you're trying to determine if yesterday was the start of a market\n","crisis. With filtering (Forward Algorithm), you only know what happened up\n","through yesterday. With smoothing (Backward Algorithm), you know that today\n","the market crashed 5%, tomorrow it crashed another 3%, and next week volatility\n","exploded. Armed with this future knowledge, you can say with high confidence:\n","\"Yes, yesterday was definitely the crisis starting.\"\n","\n","Smoothing looks both forward and backward through time. It combines information\n","from the past (like filtering) with information from the future to make better\n","regime estimates. The probabilities are sharper, cleaner, more confident -\n","because they cheat by using data you wouldn't have had in real-time.\n","\n","**Why We Can't Trade With It:**\n","\n","This is the critical point: smoothed probabilities are NOT tradeable. On day\n","100, the smoothed probability uses data from days 101, 102, 103... all the way\n","to the end of your dataset. You didn't have that information on day 100. If\n","you backtest a strategy using smoothed probabilities, you're measuring what\n","would have happened if you had a crystal ball - pure fantasy.\n","\n","I've reviewed countless MBA and practitioner backtests where this mistake\n","appears. Someone uses smoothed HMM states, gets amazing Sharpe ratios, then\n","loses real money because live trading can only use filtered states. The\n","performance gap can be brutal - smoothing might show 2.5 Sharpe while filtering\n","achieves 1.2 Sharpe on the same data.\n","\n","**So Why Use Smoothing At All?**\n","\n","Two legitimate reasons. First, for training the HMM itself (the EM algorithm),\n","we need smoothed probabilities because we're looking at complete historical\n","data to learn patterns. That's fine - we're not making trading decisions, we're\n","estimating parameters.\n","\n","Second, for diagnostics and understanding. Comparing filtered versus smoothed\n","probabilities shows you the cost of causality. If they're vastly different, it\n","means regime changes are hard to detect in real-time - you only know for sure\n","after the fact. This teaches you humility about your model's actual predictive\n","power.\n","\n","**The Backward Recursion:**\n","\n","While filtering moves forward through time, backward smoothing moves backward.\n","Starting from the last observation, it propagates information backward. At each\n","step, it asks: \"Given what I know happens in the future, how should I revise\n","my belief about this time point?\"\n","\n","**Part B: What This Section Does**\n","\n","This section implements both the Backward Algorithm and the combined smoothing\n","computation, with aggressive warnings that this is DIAGNOSTIC ONLY and forbidden\n","for trading signals.\n","\n","**The Warning Label:**\n","\n","We plaster warnings everywhere: \"NOT ALLOWED for live trading signals.\" This\n","isn't paranoia - it's protecting you from an easy-to-make, expensive-to-fix\n","mistake. The code works correctly, but using it for trading decisions would be\n","using it incorrectly.\n","\n","**What Gets Implemented:**\n","\n","We build two functions. The first runs the backward recursion - starting from\n","the final time step and working backward, computing backward probabilities that\n","capture \"future evidence.\" The second combines forward (filtered) and backward\n","probabilities to produce smoothed regime estimates.\n","\n","The smoothing also computes pairwise probabilities - the joint probability of\n","being in regime i at time t and regime j at time t+1, given all the data. These\n","are needed for the EM training algorithm we'll use later.\n","\n","**The Leakage Demonstration:**\n","\n","We explicitly create a diagnostic plot comparing filtered versus smoothed\n","probabilities. This visual proof shows how smoothed estimates are cleaner and\n","more confident - and why that confidence is false from a trading perspective.\n","\n","You'll see periods where the filtered probability says \"maybe 60% chance of\n","stressed regime\" while the smoothed probability says \"definitely 95% stressed.\"\n","That gap is the information you didn't have in real-time. The smooth line looks\n","great, but only the filtered line is honest.\n","\n","**Usage in This Notebook:**\n","\n","We'll use smoothing in exactly two places: (1) inside the EM training loop\n","(Section 9) where we're learning parameters from complete historical data, and\n","(2) creating diagnostic comparisons to educate about the filtering versus\n","smoothing difference.\n","\n","We will NEVER use smoothed probabilities to generate trading signals or compute\n","strategy returns. That would invalidate the entire exercise.\n","\n","**The Discipline:**\n","\n","This section is really about discipline and intellectual honesty. We implement\n","a powerful tool that could make our backtest results look spectacular, then we\n","strictly forbid ourselves from using it where it would matter. This is what\n","separates institutional-grade work from academic curve-fitting."],"metadata":{"id":"6W9e9bd-GiJk"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"oZWRBK99Gihl"}},{"cell_type":"code","source":["# ============================================================================\n","# BACKWARD ALGORITHM (SMOOTHING) (DIAGNOSTIC ONLY)\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"BACKWARD ALGORITHM (SMOOTHING) - DIAGNOSTIC ONLY\")\n","print(\"=\" * 80)\n","print(\"WARNING: Smoothing uses future information and is NOT ALLOWED for live trading signals.\")\n","print(\"It is used here only for:\")\n","print(\"  1. EM training (where we have complete data)\")\n","print(\"  2. Diagnostic comparison to show leakage\")\n","print()\n","\n","def backward_algorithm(X_obs, log_A, mus, vars):\n","    \"\"\"\n","    Backward algorithm to compute smoothed posteriors.\n","\n","    Args:\n","        X_obs: observations (T, D) or (T,)\n","        log_A: log transition matrix (K, K)\n","        mus: emission means (K, D) or (K,)\n","        vars: emission variances (K, D) or (K,)\n","\n","    Returns:\n","        log_beta: log backward probabilities (T, K)\n","    \"\"\"\n","    if X_obs.ndim == 1:\n","        X_obs = X_obs[:, np.newaxis]\n","        mus = mus[:, np.newaxis] if mus.ndim == 1 else mus\n","        vars = vars[:, np.newaxis] if vars.ndim == 1 else vars\n","\n","    T, D = X_obs.shape\n","    K = log_A.shape[0]\n","\n","    log_beta = np.zeros((T, K))\n","\n","    # Initialize at T-1: log_beta[T-1, :] = 0 (log(1) = 0)\n","\n","    # Backward recursion for t=T-2..0\n","    for t in range(T-2, -1, -1):\n","        for i in range(K):\n","            log_probs = []\n","            for j in range(K):\n","                log_emission = np.sum([log_gaussian_pdf(X_obs[t+1, d], mus[j, d], vars[j, d]) for d in range(D)])\n","                log_probs.append(log_A[i, j] + log_emission + log_beta[t+1, j])\n","            log_beta[t, i] = logsumexp(log_probs)\n","\n","    return log_beta\n","\n","def compute_smoothed_posteriors(X_obs, log_A, log_pi, mus, vars):\n","    \"\"\"\n","    Compute smoothed posteriors gamma[t,k] = P(s_t=k | x_1:T) and xi[t,i,j] = P(s_t=i, s_{t+1}=j | x_1:T).\n","\n","    Returns:\n","        gamma_smooth: smoothed state posteriors (T, K)\n","        xi: pairwise posteriors (T-1, K, K)\n","    \"\"\"\n","    if X_obs.ndim == 1:\n","        X_obs = X_obs[:, np.newaxis]\n","        mus = mus[:, np.newaxis] if mus.ndim == 1 else mus\n","        vars = vars[:, np.newaxis] if vars.ndim == 1 else vars\n","\n","    T, D = X_obs.shape\n","    K = len(log_pi)\n","\n","    # Forward pass\n","    gamma_filt, log_likelihood = forward_algorithm(X_obs, log_A, log_pi, mus, vars)\n","\n","    # Backward pass\n","    log_beta = backward_algorithm(X_obs, log_A, mus, vars)\n","\n","    # Compute gamma_smooth\n","    gamma_smooth = np.zeros((T, K))\n","    for t in range(T):\n","        log_gamma = np.log(gamma_filt[t, :] + 1e-10) + log_beta[t, :]\n","        log_norm = logsumexp(log_gamma)\n","        gamma_smooth[t, :] = np.exp(log_gamma - log_norm)\n","\n","    # Compute xi (pairwise posteriors)\n","    xi = np.zeros((T-1, K, K))\n","    for t in range(T-1):\n","        log_xi = np.zeros((K, K))\n","        for i in range(K):\n","            for j in range(K):\n","                log_emission = np.sum([log_gaussian_pdf(X_obs[t+1, d], mus[j, d], vars[j, d]) for d in range(D)])\n","                log_xi[i, j] = (np.log(gamma_filt[t, i] + 1e-10) + log_A[i, j] +\n","                                log_emission + log_beta[t+1, j])\n","\n","        log_norm = logsumexp(log_xi.flatten())\n","        xi[t, :, :] = np.exp(log_xi - log_norm)\n","\n","    return gamma_smooth, xi\n","\n","print(\"Backward algorithm and smoothing implemented.\")\n","print(\"✓ Smoothed posteriors computed for EM training and diagnostics\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-EiwcC1Gx6r","executionInfo":{"status":"ok","timestamp":1766676476016,"user_tz":360,"elapsed":17,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"6c045f8b-ed0b-4e0c-adc8-a2d04f5c2114"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","BACKWARD ALGORITHM (SMOOTHING) - DIAGNOSTIC ONLY\n","================================================================================\n","WARNING: Smoothing uses future information and is NOT ALLOWED for live trading signals.\n","It is used here only for:\n","  1. EM training (where we have complete data)\n","  2. Diagnostic comparison to show leakage\n","\n","Backward algorithm and smoothing implemented.\n","✓ Smoothed posteriors computed for EM training and diagnostics\n","\n"]}]},{"cell_type":"markdown","source":["##9.THE TRAINING LOOP"],"metadata":{"id":"6FsRiYbgH8GW"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"VcStk1GKITi2"}},{"cell_type":"markdown","source":["\n","\n","**Part 1: The Baum-Welch Algorithm Explained for Practitioners **\n","\n","The Baum-Welch algorithm is how we teach our Hidden Markov Model to recognize\n","regime patterns from historical data. It's a specific application of a broader\n","technique called Expectation-Maximization (EM), and understanding how it works\n","gives you insight into what the model can and cannot learn.\n","\n","**The Core Problem:**\n","\n","Imagine you're a detective trying to reconstruct a sequence of events, but some\n","crucial information is missing. You have observable clues (market returns,\n","volatility) but the actual regimes are hidden. You need to simultaneously\n","figure out: (1) what regimes probably occurred when, and (2) what patterns\n","characterize each regime. It's a chicken-and-egg problem.\n","\n","If you knew the true regimes, you could easily calculate their statistics -\n","\"Regime 1 has average volatility of 1.2%, Regime 2 has 2.8%.\" But you don't\n","know the regimes. Conversely, if you knew the regime statistics, you could\n","identify which regime occurred when. But you don't know those either.\n","\n","Baum-Welch solves this by alternating between educated guesses. You start with\n","a rough guess about regime patterns, use that to estimate which regimes\n","probably occurred, then use those regime estimates to refine your pattern\n","understanding, then re-estimate regimes, and so on. Eventually, this back-and-\n","forth converges to a stable solution.\n","\n","**The Two Steps: E and M:**\n","\n","The algorithm has two steps that repeat until convergence.\n","\n","The Expectation step (E-step) asks: \"Given my current understanding of regime\n","patterns, what regimes probably occurred at each point in history?\" This uses\n","the smoothing algorithm from Section 8. We compute the probability that each\n","time point belonged to each regime, given all the data and our current\n","parameter estimates.\n","\n","Critically, these are soft assignments - we don't commit to hard labels like\n","\"day 47 was definitely Regime 2.\" Instead, we say \"day 47 was probably 70%\n","Regime 1, 30% Regime 2.\" This probabilistic approach prevents premature\n","commitment and allows the algorithm to explore different interpretations.\n","\n","The Maximization step (M-step) asks: \"Given these probabilistic regime\n","assignments, what parameter values best explain the data?\" We update three\n","sets of parameters:\n","\n","Initial state probabilities: If day 1 has 80% probability of being Regime 1,\n","then Regime 1's initial probability should be near 0.8.\n","\n","Transition probabilities: If days that are probably Regime 1 tend to be\n","followed by days that are also probably Regime 1, then Regime 1 should have\n","high self-transition probability (persistence).\n","\n","Emission parameters: For each regime, we calculate the mean and variance of\n","features, weighted by the probability that each day belonged to that regime.\n","Days with high regime probability contribute more to that regime's statistics.\n","\n","**Why This Works:**\n","\n","Each EM iteration is guaranteed to improve the model's fit to the data (or at\n","least not make it worse). The log-likelihood - a measure of how well the model\n","explains the observations - increases with each iteration. Eventually,\n","improvements become tiny and we declare convergence.\n","\n","Think of it like climbing a hill in fog. Each step takes you upward (better\n","fit), though you can't see the peak. Eventually, you reach a point where every\n","direction is downward or flat - you've found a local maximum. It might not be\n","the absolute highest point on the mountain, but it's a legitimate hilltop.\n","\n","**The Initialization Problem:**\n","\n","Where you start the climb matters enormously. If you initialize randomly, you\n","might converge to a poor solution - like discovering one regime is \"weekdays\"\n","and the other is \"weekends\" rather than finding meaningful market regimes.\n","\n","We use deterministic initialization based on domain knowledge. We sort\n","historical volatility and split it into quantiles - low volatility days\n","initialize one regime, high volatility days initialize another. This gives the\n","algorithm a sensible starting point aligned with how practitioners think about\n","regimes.\n","\n","For transition probabilities, we initialize with diagonal dominance - each\n","regime has high probability of staying in itself. This builds in the expectation\n","of persistence, which is appropriate for financial regimes that typically last\n","weeks or months, not days.\n","\n","**Regularization and Constraints:**\n","\n","Pure maximum likelihood EM can overfit. If one regime is rarely occupied, its\n","variance estimates might collapse to near-zero, making the model absurdly\n","confident. We add safeguards:\n","\n","Variance floors prevent any variance from dropping below a minimum threshold.\n","This maintains healthy uncertainty and numerical stability.\n","\n","Pseudocounts for transitions act like Dirichlet priors - we add a small\n","fictitious count to each transition, preventing any transition probability from\n","being exactly zero. This keeps the model flexible and prevents it from ruling\n","out regime switches it hasn't seen much in training.\n","\n","Occupancy checks ensure each regime is actually being used. If a regime's\n","expected occupancy (sum of its probabilities across all time points) is too\n","low, we freeze its parameters rather than trying to estimate them from\n","insufficient data.\n","\n","**Convergence Criteria:**\n","\n","We stop iterating when one of three things happens:\n","\n","Parameter convergence: The largest change in any parameter between iterations\n","falls below a threshold (like 0.0001). The model has stabilized.\n","\n","Log-likelihood plateau: The improvement in log-likelihood between iterations\n","becomes negligible. We're no longer learning meaningfully.\n","\n","Maximum iterations reached: After 100 iterations, we give up even if not fully\n","converged. This prevents infinite loops and forces a decision.\n","\n","**What the Algorithm Learns:**\n","\n","After convergence, we have learned parameters that represent the model's best\n","understanding of regime structure in the training data:\n","\n","Transition matrix: How persistent is each regime? How quickly do regimes\n","switch? A Normal regime with 95% self-transition probability means crises are\n","rare but once started, tend to persist.\n","\n","Regime signatures: What does each regime \"look like\" in terms of features?\n","Regime 1 might be characterized by low volatility (mean 0.8%, variance 0.04%),\n","while Regime 2 shows high volatility (mean 2.5%, variance 0.3%).\n","\n","These learned parameters become our model - the lens through which we'll\n","interpret future data.\n","\n","**Limitations and Realities:**\n","\n","Baum-Welch finds local optima, not necessarily global optima. Running it\n","multiple times with different initializations might yield different solutions.\n","Our deterministic initialization mitigates this but doesn't eliminate it.\n","\n","The algorithm assumes our model structure is correct - two regimes, Gaussian\n","emissions, first-order Markov transitions. If reality is more complex (three\n","regimes, fat-tailed distributions, longer-memory dependencies), we'll get the\n","best two-regime Gaussian approximation, not the truth.\n","\n","Sample size matters. With 500 training days, we might get reasonable regime\n","estimates. With 50 days, we're likely overfitting to noise. EM won't warn you\n","- it will confidently converge to garbage.\n","\n","Despite these limitations, Baum-Welch remains the standard for HMM training in\n","finance because it's reliable, interpretable, and well-understood. We know its\n","failure modes and can defend against them.\n","\n","**Part 2: How Baum-Welch Fits the Theoretical HMM Framework **\n","\n","Hidden Markov Models are generative models - they describe a probabilistic\n","process for generating observable data from hidden states. The model says:\n","first, pick an initial regime according to initial state probabilities. Then,\n","generate an observation from that regime's emission distribution. Next day,\n","transition to a new regime (possibly the same one) according to transition\n","probabilities. Generate another observation. Repeat.\n","\n","This generative story has parameters: initial probabilities (π), transitions\n","(A), and emission distributions (means μ and variances σ² for Gaussian case).\n","Given parameters, we can calculate the probability of any observed sequence.\n","\n","But in practice, we face the inverse problem: we have observed data and want\n","to infer the parameters. This is the statistical inference problem, and EM is\n","the solution.\n","\n","**Maximum Likelihood Estimation:**\n","\n","Baum-Welch performs maximum likelihood estimation - it finds parameters that\n","make the observed data most probable. Mathematically, it maximizes P(data |\n","parameters). If the learned parameters are correct, the data we actually\n","observed should seem highly probable, not like a weird fluke.\n","\n","The challenge is that hidden states create ambiguity. The same data sequence\n","could be explained by different regime sequences under different parameters.\n","EM handles this by averaging over all possible regime sequences, weighted by\n","their probability.\n","\n","**The Complete-Data Likelihood:**\n","\n","If we knew the true regime sequence, estimating parameters would be trivial -\n","just compute sample statistics for each regime. EM constructs a \"complete-data\n","likelihood\" that pretends we know the regimes (using probabilistic weights),\n","then maximizes that. This converts an intractable hidden-variable problem into\n","a tractable weighted estimation problem.\n","\n","**Theoretical Guarantees:**\n","\n","EM is guaranteed to find a local maximum of the likelihood function. It won't\n","decrease likelihood (barring numerical errors). But it's not guaranteed to find\n","the global maximum - hence the importance of good initialization.\n","\n","The algorithm assumes data is generated by the model family (HMM with Gaussian\n","emissions). If the true process is different, EM finds the best HMM\n","approximation within that family - useful, but not perfect.\n","\n","**Part 3: What This Section Does**\n","\n","This section implements the complete EM training pipeline with production-grade\n","initialization, regularization, and convergence monitoring. It's the brain\n","that learns regime patterns from historical data.\n","\n","**Deterministic Initialization Function:**\n","\n","We build a function that initializes HMM parameters intelligently rather than\n","randomly. It uses volatility quantiles to seed emission parameters - sorting\n","the training data by realized volatility and assigning low-volatility\n","observations to one regime, high-volatility to another. This gives each regime\n","a coherent initial identity.\n","\n","Transition matrix gets initialized with diagonal dominance - we set\n","self-transition probabilities to 0.9 (90% chance of staying in current regime),\n","which reflects the empirical fact that financial regimes persist.\n","\n","**The Training Loop:**\n","\n","The main EM function iterates between E-step (computing smoothed regime\n","probabilities using current parameters) and M-step (updating parameters using\n","those probabilities). Each iteration computes the log-likelihood and tracks\n","parameter changes.\n","\n","**Regularization Built In:**\n","\n","Variance floors are enforced automatically - no variance can drop below our\n","threshold. Pseudocounts (0.1 by default) are added to transition counts to\n","prevent zero probabilities. Occupancy checks identify regimes with insufficient\n","support and freeze their parameters.\n","\n","**Convergence Monitoring:**\n","\n","We track three metrics: iteration number, log-likelihood, and maximum parameter\n","change. We stop when parameter changes fall below tolerance, when log-likelihood\n","plateaus, or when we hit max iterations. The stop reason gets logged for\n","diagnostics.\n","\n","**The Training Trace:**\n","\n","Every iteration's statistics get saved to an EM trace artifact - iteration\n","number, log-likelihood, max parameter change, eventual stop reason. This trace\n","is auditable evidence. Six months later, you can see exactly how training\n","progressed and whether convergence was clean or problematic.\n","\n","**Output:**\n","\n","The function returns learned parameters (π, A, μ, σ²) and the complete EM trace.\n","These parameters define our trained model, ready to be applied to new data for\n","regime detection.\n"],"metadata":{"id":"xBjqN_xyIX-U"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"ep0VMgC7IYS2"}},{"cell_type":"code","source":[],"metadata":{"id":"bsdOwJ9TJonA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 9\n","# ============================================================================\n","# EM (BAUM-WELCH) TRAINING LOOP (DETERMINISTIC)\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"EM (BAUM-WELCH) TRAINING\")\n","print(\"=\" * 80)\n","\n","def initialize_hmm_params(X_train, K, init_diag):\n","    \"\"\"\n","    Deterministic initialization for HMM parameters.\n","\n","    Strategy:\n","        - Emissions: use volatility quantiles to assign initial means/vars\n","        - Transitions: diagonal-dominant matrix\n","\n","    Args:\n","        X_train: training observations (T_train, D)\n","        K: number of states\n","        init_diag: diagonal value for transition matrix\n","\n","    Returns:\n","        pi: initial state probabilities (K,)\n","        A: transition matrix (K, K)\n","        mus: emission means (K, D)\n","        vars: emission variances (K, D)\n","    \"\"\"\n","    T_train, D = X_train.shape\n","\n","    # Initial state probabilities: uniform\n","    pi = np.ones(K) / K\n","\n","    # Transition matrix: diagonal-dominant\n","    A = np.full((K, K), (1.0 - init_diag) / (K - 1))\n","    np.fill_diagonal(A, init_diag)\n","\n","    # Emissions: use quantiles of first feature (rolling vol)\n","    # Sort by first feature and split into K clusters\n","    first_feat = X_train[:, 0]\n","    quantiles = np.linspace(0, 100, K+1)\n","\n","    mus = np.zeros((K, D))\n","    vars = np.zeros((K, D))\n","\n","    for k in range(K):\n","        lower_pct = quantiles[k]\n","        upper_pct = quantiles[k+1]\n","        lower_val = np.percentile(first_feat, lower_pct)\n","        upper_val = np.percentile(first_feat, upper_pct)\n","\n","        mask = (first_feat >= lower_val) & (first_feat <= upper_val)\n","        if mask.sum() == 0:\n","            mask = np.ones(T_train, dtype=bool)  # Fallback\n","\n","        cluster_data = X_train[mask, :]\n","        mus[k, :] = np.mean(cluster_data, axis=0)\n","        vars[k, :] = np.var(cluster_data, axis=0, ddof=1)\n","        vars[k, :] = np.maximum(vars[k, :], variance_floor)\n","\n","    return pi, A, mus, vars\n","\n","def em_train(X_train, K, max_iters, tol, init_diag, pseudocount):\n","    \"\"\"\n","    EM (Baum-Welch) training for HMM.\n","\n","    Args:\n","        X_train: training observations (T_train, D)\n","        K: number of states\n","        max_iters: maximum EM iterations\n","        tol: convergence tolerance (max param change)\n","        init_diag: diagonal dominance for A init\n","        pseudocount: Dirichlet-like smoothing for transitions\n","\n","    Returns:\n","        pi, A, mus, vars: learned parameters\n","        em_trace: training log\n","    \"\"\"\n","    T_train, D = X_train.shape\n","\n","    # Initialize\n","    pi, A, mus, vars = initialize_hmm_params(X_train, K, init_diag)\n","\n","    em_trace = []\n","    prev_log_likelihood = -np.inf\n","\n","    for iteration in range(max_iters):\n","        # E-step: compute smoothed posteriors\n","        log_A = np.log(A + 1e-10)\n","        log_pi = np.log(pi + 1e-10)\n","        gamma_smooth, xi = compute_smoothed_posteriors(X_train, log_A, log_pi, mus, vars)\n","\n","        # Compute log-likelihood (for monitoring)\n","        gamma_filt, log_likelihood = forward_algorithm(X_train, log_A, log_pi, mus, vars)\n","\n","        # M-step: update parameters\n","\n","        # Update pi\n","        pi_new = gamma_smooth[0, :]\n","        pi_new = pi_new / pi_new.sum()\n","\n","        # Update A with pseudocount smoothing\n","        A_new = np.zeros((K, K))\n","        for i in range(K):\n","            numerator = xi[:, i, :].sum(axis=0) + pseudocount\n","            denominator = gamma_smooth[:-1, i].sum() + K * pseudocount\n","            A_new[i, :] = numerator / denominator\n","\n","        # Update emission parameters (Gaussian)\n","        mus_new = np.zeros((K, D))\n","        vars_new = np.zeros((K, D))\n","\n","        for k in range(K):\n","            gamma_k = gamma_smooth[:, k]\n","            gamma_k_sum = gamma_k.sum()\n","\n","            if gamma_k_sum < 1.0:  # Effective sample size check\n","                # State k is barely occupied; keep previous params\n","                mus_new[k, :] = mus[k, :]\n","                vars_new[k, :] = vars[k, :]\n","            else:\n","                for d in range(D):\n","                    mus_new[k, d] = np.sum(gamma_k * X_train[:, d]) / gamma_k_sum\n","                    vars_new[k, d] = np.sum(gamma_k * (X_train[:, d] - mus_new[k, d])**2) / gamma_k_sum\n","                    vars_new[k, d] = max(vars_new[k, d], variance_floor)\n","\n","        # Check convergence\n","        max_change = max(\n","            np.max(np.abs(pi_new - pi)),\n","            np.max(np.abs(A_new - A)),\n","            np.max(np.abs(mus_new - mus)),\n","            np.max(np.abs(vars_new - vars)),\n","        )\n","\n","        # Log iteration\n","        em_trace.append({\n","            \"iteration\": iteration,\n","            \"log_likelihood\": float(log_likelihood),\n","            \"max_param_change\": float(max_change),\n","        })\n","\n","        # Update parameters\n","        pi, A, mus, vars = pi_new, A_new, mus_new, vars_new\n","\n","        # Check for convergence\n","        if max_change < tol:\n","            em_trace[-1][\"stop_reason\"] = \"converged\"\n","            break\n","\n","        if np.abs(log_likelihood - prev_log_likelihood) < tol * 0.1:\n","            em_trace[-1][\"stop_reason\"] = \"log_likelihood_plateau\"\n","            break\n","\n","        prev_log_likelihood = log_likelihood\n","\n","    else:\n","        em_trace[-1][\"stop_reason\"] = \"max_iters_reached\"\n","\n","    return pi, A, mus, vars, em_trace\n","\n","print(\"EM training function implemented with:\")\n","print(\"  - Deterministic initialization via volatility quantiles\")\n","print(\"  - Dirichlet-like pseudocount smoothing for transitions\")\n","print(\"  - Variance floors for numerical stability\")\n","print(\"  - Occupancy checks for effective sample size\")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qde6OjtyIaqu","executionInfo":{"status":"ok","timestamp":1766676941124,"user_tz":360,"elapsed":51,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"05a4cef4-9487-4e05-f1b9-0261f441541d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","EM (BAUM-WELCH) TRAINING\n","================================================================================\n","EM training function implemented with:\n","  - Deterministic initialization via volatility quantiles\n","  - Dirichlet-like pseudocount smoothing for transitions\n","  - Variance floors for numerical stability\n","  - Occupancy checks for effective sample size\n","\n"]}]},{"cell_type":"markdown","source":["##10.WALK FORWARD EVALUATION"],"metadata":{"id":"NRtqlnKUJt7y"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"38bcMK2EJ2dU"}},{"cell_type":"markdown","source":["\n","**What This Section Does**\n","\n","This section implements the realistic way institutional traders actually use\n","regime detection - with periodic model updates and strict time-series discipline.\n","It's the difference between a backtest and a production system.\n","\n","**The Walk-Forward Concept:**\n","\n","In real trading, you don't train one model on all historical data and use it\n","forever. Markets evolve - regime characteristics from 2010 don't necessarily\n","apply in 2020. Instead, you periodically retrain your model as new data arrives.\n","\n","We simulate this exactly. We start with an initial training window (252 days,\n","roughly one year). We train an HMM on that data using our EM algorithm. Then\n","we apply that trained model for the next month (21 trading days), computing\n","filtered regime probabilities using fixed parameters.\n","\n","After a month, we refit. We expand our training window to include the new data\n","(expanding window approach), retrain the HMM completely, and use these updated\n","parameters for the next month. This cycle repeats throughout our entire dataset.\n","\n","**Why This Matters:**\n","\n","This mimics production reality. Every month, your model gets refreshed with the\n","latest data. Regime characteristics adapt - if volatility patterns change, the\n","model learns the new patterns. But critically, within each month, parameters\n","are frozen. You're not peeking ahead and adjusting parameters based on what's\n","about to happen.\n","\n","**The Causality Gate:**\n","\n","We implement a critical test called the \"future perturbation test.\" We\n","deliberately corrupt data after some time point and verify that filtered\n","probabilities at that time point don't change. This proves those probabilities\n","truly use only past data.\n","\n","If this test fails, we stop immediately with an error. It means we've\n","accidentally introduced lookahead bias - the filtered probabilities are somehow\n","seeing the future. This would invalidate every trading result that follows.\n","\n","**What Gets Saved:**\n","\n","For each refit, we save the complete parameter set - transition matrix, emission\n","parameters, training window used. We save all filtered posteriors for the entire\n","evaluation period. This creates a complete audit trail.\n","\n","If someone questions your regime calls from six months ago, you can show exactly\n","which model version was running, what parameters it had, and how it computed\n","those probabilities. Full transparency.\n","\n","**The Output:**\n","\n","We produce filtered regime probabilities for every day in our dataset, computed\n","using only information available up to that day. These probabilities are\n","tradeable - they respect causality and could be generated in real-time in a\n","production system.\n","```"],"metadata":{"id":"u0zpULXbJ4i9"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"jioKwhxzJ5Ab"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"WALK-FORWARD EVALUATION WITH PERIODIC REFITS\")\n","print(\"=\" * 80)\n","\n","initial_train_window = CONFIG[\"walkforward\"][\"initial_training_window\"]\n","refit_cadence = CONFIG[\"walkforward\"][\"refit_cadence\"]\n","training_type = CONFIG[\"walkforward\"][\"training_window_type\"]\n","rolling_window = CONFIG[\"walkforward\"][\"rolling_window_size\"]\n","\n","K_hmm = CONFIG[\"hmm\"][\"K\"]\n","em_max_iters = CONFIG[\"hmm\"][\"em_max_iters\"]\n","em_tol = CONFIG[\"hmm\"][\"em_tol\"]\n","init_diag = CONFIG[\"hmm\"][\"init_transition_diag\"]\n","pseudocount = CONFIG[\"hmm\"][\"transition_pseudocount\"]\n","\n","# We'll use univariate observations (just rolling vol as primary feature)\n","X_obs = X[:, 0]  # Shape: (T,)\n","\n","# Determine refit times\n","refit_times = list(range(initial_train_window, T, refit_cadence))\n","print(f\"Refit times: {refit_times[:5]}... (total: {len(refit_times)})\")\n","\n","# Storage for walk-forward results\n","params_by_refit = []\n","gamma_filt_all = np.zeros((T, K_hmm))  # Filtered posteriors for entire period\n","em_traces_all = []\n","\n","# Walk-forward loop\n","for refit_idx, refit_t in enumerate(refit_times):\n","    print(f\"\\n--- Refit {refit_idx+1}/{len(refit_times)} at t={refit_t} ---\")\n","\n","    # Define training window\n","    if training_type == \"expanding\":\n","        train_start = first_valid\n","        train_end = refit_t\n","    else:  # rolling\n","        train_start = max(first_valid, refit_t - rolling_window)\n","        train_end = refit_t\n","\n","    X_train = X_obs[train_start:train_end, np.newaxis]\n","    print(f\"Training window: [{train_start}, {train_end}), size={train_end - train_start}\")\n","\n","    # Train HMM via EM\n","    pi_est, A_est, mus_est, vars_est, em_trace = em_train(\n","        X_train, K_hmm, em_max_iters, em_tol, init_diag, pseudocount\n","    )\n","\n","    print(f\"EM converged in {len(em_trace)} iterations ({em_trace[-1].get('stop_reason', 'unknown')})\")\n","    print(f\"Final log-likelihood: {em_trace[-1]['log_likelihood']:.2f}\")\n","\n","    # Save parameters\n","    params_entry = {\n","        \"refit_idx\": refit_idx,\n","        \"refit_t\": refit_t,\n","        \"train_window\": [train_start, train_end],\n","        \"pi\": pi_est.tolist(),\n","        \"A\": A_est.tolist(),\n","        \"mus\": mus_est.tolist(),\n","        \"vars\": vars_est.tolist(),\n","    }\n","    params_by_refit.append(params_entry)\n","    save_json(f\"{ARTIFACT_DIR}/params_by_refit/refit_{refit_idx:03d}.json\", params_entry)\n","    em_traces_all.append({\"refit_idx\": refit_idx, \"refit_t\": refit_t, \"em_trace\": em_trace})\n","\n","    # Determine application window (from refit_t to next refit or end)\n","    if refit_idx < len(refit_times) - 1:\n","        apply_end = refit_times[refit_idx + 1]\n","    else:\n","        apply_end = T\n","\n","    # Compute filtered posteriors for application window using fixed params\n","    X_apply = X_obs[refit_t:apply_end, np.newaxis]\n","    log_A_est = np.log(A_est + 1e-10)\n","    log_pi_est = np.log(pi_est + 1e-10)\n","\n","    gamma_filt_segment, _ = forward_algorithm(X_apply, log_A_est, log_pi_est, mus_est, vars_est)\n","    gamma_filt_all[refit_t:apply_end, :] = gamma_filt_segment\n","\n","    print(f\"Applied filtered posteriors to [{refit_t}, {apply_end})\")\n","\n","# Fill in warmup period with uniform posteriors\n","gamma_filt_all[:refit_times[0], :] = 1.0 / K_hmm\n","\n","print(\"\\n✓ Walk-forward evaluation complete\")\n","print(f\"Filtered posteriors computed for entire period [0, {T})\")\n","print()\n","\n","# Save EM trace\n","save_json(f\"{ARTIFACT_DIR}/em_trace.json\", {\"run_id\": RUN_ID, \"em_traces\": em_traces_all})\n","\n","# Save filtered posteriors\n","np.save(f\"{ARTIFACT_DIR}/posteriors/gamma_filt_all.npy\", gamma_filt_all)\n","print(f\"Saved: {ARTIFACT_DIR}/posteriors/gamma_filt_all.npy\")\n","\n","# CAUSALITY GATE: Future perturbation test\n","print(\"\\n--- CAUSALITY GATE: Future Perturbation Test ---\")\n","test_t_perturb = 500\n","if test_t_perturb < refit_times[-1]:\n","    # Perturb data after test_t_perturb\n","    X_obs_perturbed = X_obs.copy()\n","    X_obs_perturbed[test_t_perturb+10:] += 0.1  # Add large perturbation\n","\n","    # Find which refit segment test_t_perturb belongs to\n","    refit_segment_idx = 0\n","    for idx, rt in enumerate(refit_times):\n","        if test_t_perturb >= rt:\n","            refit_segment_idx = idx\n","\n","    # Use params from that refit\n","    params_test = params_by_refit[refit_segment_idx]\n","    pi_test = np.array(params_test[\"pi\"])\n","    A_test = np.array(params_test[\"A\"])\n","    mus_test = np.array(params_test[\"mus\"])\n","    vars_test = np.array(params_test[\"vars\"])\n","\n","    # Recompute filtered posterior at test_t_perturb with perturbed data\n","    refit_t_test = params_test[\"refit_t\"]\n","    X_test_segment = X_obs_perturbed[refit_t_test:test_t_perturb+1, np.newaxis]\n","\n","    log_A_test = np.log(A_test + 1e-10)\n","    log_pi_test = np.log(pi_test + 1e-10)\n","    gamma_filt_perturbed, _ = forward_algorithm(X_test_segment, log_A_test, log_pi_test, mus_test, vars_test)\n","\n","    # Compare with original\n","    original_posterior = gamma_filt_all[test_t_perturb, :]\n","    perturbed_posterior = gamma_filt_perturbed[test_t_perturb - refit_t_test, :]\n","\n","    diff_perturb = np.max(np.abs(original_posterior - perturbed_posterior))\n","    print(f\"Perturbation test at t={test_t_perturb}: max diff = {diff_perturb:.10f}\")\n","    assert diff_perturb < 1e-6, f\"CAUSALITY VIOLATION: Future perturbation affected past posterior (diff={diff_perturb})\"\n","\n","    print(\"✓ Future perturbation test PASSED\")\n","else:\n","    print(\"(Skipping future perturbation test: test_t beyond refit range)\")\n","\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfGK0rtaKBst","executionInfo":{"status":"ok","timestamp":1766677348481,"user_tz":360,"elapsed":229601,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"952136ae-cdcb-4b77-fa60-2e957cd1f20f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","WALK-FORWARD EVALUATION WITH PERIODIC REFITS\n","================================================================================\n","Refit times: [252, 273, 294, 315, 336]... (total: 36)\n","\n","--- Refit 1/36 at t=252 ---\n","Training window: [60, 252), size=192\n","EM converged in 21 iterations (converged)\n","Final log-likelihood: -53901.92\n","Saved: artifacts/ch14_run/params_by_refit/refit_000.json\n","Applied filtered posteriors to [252, 273)\n","\n","--- Refit 2/36 at t=273 ---\n","Training window: [60, 273), size=213\n","EM converged in 18 iterations (converged)\n","Final log-likelihood: -66114.63\n","Saved: artifacts/ch14_run/params_by_refit/refit_001.json\n","Applied filtered posteriors to [273, 294)\n","\n","--- Refit 3/36 at t=294 ---\n","Training window: [60, 294), size=234\n","EM converged in 19 iterations (converged)\n","Final log-likelihood: -79424.00\n","Saved: artifacts/ch14_run/params_by_refit/refit_002.json\n","Applied filtered posteriors to [294, 315)\n","\n","--- Refit 4/36 at t=315 ---\n","Training window: [60, 315), size=255\n","EM converged in 21 iterations (converged)\n","Final log-likelihood: -94257.21\n","Saved: artifacts/ch14_run/params_by_refit/refit_003.json\n","Applied filtered posteriors to [315, 336)\n","\n","--- Refit 5/36 at t=336 ---\n","Training window: [60, 336), size=276\n","EM converged in 25 iterations (converged)\n","Final log-likelihood: -109929.72\n","Saved: artifacts/ch14_run/params_by_refit/refit_004.json\n","Applied filtered posteriors to [336, 357)\n","\n","--- Refit 6/36 at t=357 ---\n","Training window: [60, 357), size=297\n","EM converged in 14 iterations (converged)\n","Final log-likelihood: -125996.53\n","Saved: artifacts/ch14_run/params_by_refit/refit_005.json\n","Applied filtered posteriors to [357, 378)\n","\n","--- Refit 7/36 at t=378 ---\n","Training window: [60, 378), size=318\n","EM converged in 40 iterations (converged)\n","Final log-likelihood: -143983.51\n","Saved: artifacts/ch14_run/params_by_refit/refit_006.json\n","Applied filtered posteriors to [378, 399)\n","\n","--- Refit 8/36 at t=399 ---\n","Training window: [60, 399), size=339\n","EM converged in 17 iterations (converged)\n","Final log-likelihood: -163089.70\n","Saved: artifacts/ch14_run/params_by_refit/refit_007.json\n","Applied filtered posteriors to [399, 420)\n","\n","--- Refit 9/36 at t=420 ---\n","Training window: [60, 420), size=360\n","EM converged in 12 iterations (converged)\n","Final log-likelihood: -183386.19\n","Saved: artifacts/ch14_run/params_by_refit/refit_008.json\n","Applied filtered posteriors to [420, 441)\n","\n","--- Refit 10/36 at t=441 ---\n","Training window: [60, 441), size=381\n","EM converged in 11 iterations (converged)\n","Final log-likelihood: -204717.73\n","Saved: artifacts/ch14_run/params_by_refit/refit_009.json\n","Applied filtered posteriors to [441, 462)\n","\n","--- Refit 11/36 at t=462 ---\n","Training window: [60, 462), size=402\n","EM converged in 10 iterations (converged)\n","Final log-likelihood: -227467.54\n","Saved: artifacts/ch14_run/params_by_refit/refit_010.json\n","Applied filtered posteriors to [462, 483)\n","\n","--- Refit 12/36 at t=483 ---\n","Training window: [60, 483), size=423\n","EM converged in 15 iterations (converged)\n","Final log-likelihood: -251286.74\n","Saved: artifacts/ch14_run/params_by_refit/refit_011.json\n","Applied filtered posteriors to [483, 504)\n","\n","--- Refit 13/36 at t=504 ---\n","Training window: [60, 504), size=444\n","EM converged in 24 iterations (converged)\n","Final log-likelihood: -276391.76\n","Saved: artifacts/ch14_run/params_by_refit/refit_012.json\n","Applied filtered posteriors to [504, 525)\n","\n","--- Refit 14/36 at t=525 ---\n","Training window: [60, 525), size=465\n","EM converged in 33 iterations (converged)\n","Final log-likelihood: -302516.37\n","Saved: artifacts/ch14_run/params_by_refit/refit_013.json\n","Applied filtered posteriors to [525, 546)\n","\n","--- Refit 15/36 at t=546 ---\n","Training window: [60, 546), size=486\n","EM converged in 18 iterations (converged)\n","Final log-likelihood: -330587.82\n","Saved: artifacts/ch14_run/params_by_refit/refit_014.json\n","Applied filtered posteriors to [546, 567)\n","\n","--- Refit 16/36 at t=567 ---\n","Training window: [60, 567), size=507\n","EM converged in 18 iterations (converged)\n","Final log-likelihood: -359424.00\n","Saved: artifacts/ch14_run/params_by_refit/refit_015.json\n","Applied filtered posteriors to [567, 588)\n","\n","--- Refit 17/36 at t=588 ---\n","Training window: [60, 588), size=528\n","EM converged in 15 iterations (converged)\n","Final log-likelihood: -389788.36\n","Saved: artifacts/ch14_run/params_by_refit/refit_016.json\n","Applied filtered posteriors to [588, 609)\n","\n","--- Refit 18/36 at t=609 ---\n","Training window: [60, 609), size=549\n","EM converged in 20 iterations (converged)\n","Final log-likelihood: -420495.38\n","Saved: artifacts/ch14_run/params_by_refit/refit_017.json\n","Applied filtered posteriors to [609, 630)\n","\n","--- Refit 19/36 at t=630 ---\n","Training window: [60, 630), size=570\n","EM converged in 21 iterations (converged)\n","Final log-likelihood: -452893.15\n","Saved: artifacts/ch14_run/params_by_refit/refit_018.json\n","Applied filtered posteriors to [630, 651)\n","\n","--- Refit 20/36 at t=651 ---\n","Training window: [60, 651), size=591\n","EM converged in 26 iterations (converged)\n","Final log-likelihood: -488394.46\n","Saved: artifacts/ch14_run/params_by_refit/refit_019.json\n","Applied filtered posteriors to [651, 672)\n","\n","--- Refit 21/36 at t=672 ---\n","Training window: [60, 672), size=612\n","EM converged in 30 iterations (converged)\n","Final log-likelihood: -523709.20\n","Saved: artifacts/ch14_run/params_by_refit/refit_020.json\n","Applied filtered posteriors to [672, 693)\n","\n","--- Refit 22/36 at t=693 ---\n","Training window: [60, 693), size=633\n","EM converged in 50 iterations (converged)\n","Final log-likelihood: -564739.03\n","Saved: artifacts/ch14_run/params_by_refit/refit_021.json\n","Applied filtered posteriors to [693, 714)\n","\n","--- Refit 23/36 at t=714 ---\n","Training window: [60, 714), size=654\n","EM converged in 42 iterations (converged)\n","Final log-likelihood: -600681.09\n","Saved: artifacts/ch14_run/params_by_refit/refit_022.json\n","Applied filtered posteriors to [714, 735)\n","\n","--- Refit 24/36 at t=735 ---\n","Training window: [60, 735), size=675\n","EM converged in 54 iterations (converged)\n","Final log-likelihood: -641563.63\n","Saved: artifacts/ch14_run/params_by_refit/refit_023.json\n","Applied filtered posteriors to [735, 756)\n","\n","--- Refit 25/36 at t=756 ---\n","Training window: [60, 756), size=696\n","EM converged in 39 iterations (converged)\n","Final log-likelihood: -678793.61\n","Saved: artifacts/ch14_run/params_by_refit/refit_024.json\n","Applied filtered posteriors to [756, 777)\n","\n","--- Refit 26/36 at t=777 ---\n","Training window: [60, 777), size=717\n","EM converged in 42 iterations (converged)\n","Final log-likelihood: -721019.69\n","Saved: artifacts/ch14_run/params_by_refit/refit_025.json\n","Applied filtered posteriors to [777, 798)\n","\n","--- Refit 27/36 at t=798 ---\n","Training window: [60, 798), size=738\n","EM converged in 57 iterations (converged)\n","Final log-likelihood: -764979.94\n","Saved: artifacts/ch14_run/params_by_refit/refit_026.json\n","Applied filtered posteriors to [798, 819)\n","\n","--- Refit 28/36 at t=819 ---\n","Training window: [60, 819), size=759\n","EM converged in 46 iterations (converged)\n","Final log-likelihood: -810191.24\n","Saved: artifacts/ch14_run/params_by_refit/refit_027.json\n","Applied filtered posteriors to [819, 840)\n","\n","--- Refit 29/36 at t=840 ---\n","Training window: [60, 840), size=780\n","EM converged in 45 iterations (converged)\n","Final log-likelihood: -853550.06\n","Saved: artifacts/ch14_run/params_by_refit/refit_028.json\n","Applied filtered posteriors to [840, 861)\n","\n","--- Refit 30/36 at t=861 ---\n","Training window: [60, 861), size=801\n","EM converged in 45 iterations (converged)\n","Final log-likelihood: -899074.91\n","Saved: artifacts/ch14_run/params_by_refit/refit_029.json\n","Applied filtered posteriors to [861, 882)\n","\n","--- Refit 31/36 at t=882 ---\n","Training window: [60, 882), size=822\n","EM converged in 31 iterations (converged)\n","Final log-likelihood: -945925.70\n","Saved: artifacts/ch14_run/params_by_refit/refit_030.json\n","Applied filtered posteriors to [882, 903)\n","\n","--- Refit 32/36 at t=903 ---\n","Training window: [60, 903), size=843\n","EM converged in 31 iterations (converged)\n","Final log-likelihood: -994115.05\n","Saved: artifacts/ch14_run/params_by_refit/refit_031.json\n","Applied filtered posteriors to [903, 924)\n","\n","--- Refit 33/36 at t=924 ---\n","Training window: [60, 924), size=864\n","EM converged in 10 iterations (converged)\n","Final log-likelihood: -1043736.84\n","Saved: artifacts/ch14_run/params_by_refit/refit_032.json\n","Applied filtered posteriors to [924, 945)\n","\n","--- Refit 34/36 at t=945 ---\n","Training window: [60, 945), size=885\n","EM converged in 64 iterations (converged)\n","Final log-likelihood: -1095756.30\n","Saved: artifacts/ch14_run/params_by_refit/refit_033.json\n","Applied filtered posteriors to [945, 966)\n","\n","--- Refit 35/36 at t=966 ---\n","Training window: [60, 966), size=906\n","EM converged in 26 iterations (converged)\n","Final log-likelihood: -1147852.80\n","Saved: artifacts/ch14_run/params_by_refit/refit_034.json\n","Applied filtered posteriors to [966, 987)\n","\n","--- Refit 36/36 at t=987 ---\n","Training window: [60, 987), size=927\n","EM converged in 11 iterations (converged)\n","Final log-likelihood: -1201231.93\n","Saved: artifacts/ch14_run/params_by_refit/refit_035.json\n","Applied filtered posteriors to [987, 1000)\n","\n","✓ Walk-forward evaluation complete\n","Filtered posteriors computed for entire period [0, 1000)\n","\n","Saved: artifacts/ch14_run/em_trace.json\n","Saved: artifacts/ch14_run/posteriors/gamma_filt_all.npy\n","\n","--- CAUSALITY GATE: Future Perturbation Test ---\n","Perturbation test at t=500: max diff = 0.0000000000\n","✓ Future perturbation test PASSED\n","\n"]}]},{"cell_type":"markdown","source":["##11.FROM REGIME BELIEFS TO TRADING DECISIONS"],"metadata":{"id":"VvfxJ995Kf08"}},{"cell_type":"markdown","source":["###11.1.OVERVIEW"],"metadata":{"id":"2HMZjXm5Kj-t"}},{"cell_type":"markdown","source":["\n","\n","This section is where theory meets practice - we convert probabilistic regime\n","beliefs into concrete trading decisions. This is what your PM or CIO actually\n","cares about: \"Okay, you think there's 70% chance we're in a stressed regime.\n","Now what do we DO about it?\"\n","\n","We implement three decision layers that show how regime information enhances\n","trading in different ways. These aren't the only approaches possible, but\n","they're representative of how institutional traders actually use regime\n","detection.\n","\n","**Decision Layer 1: Risk Overlay (Exposure Management)**\n","\n","The simplest use of regime detection is risk management. When regime\n","probabilities signal elevated stress, reduce your trading exposure\n","proportionally.\n","\n","We implement a smooth risk overlay. If the probability of stressed regime\n","stays below 60%, we maintain full exposure (multiplier of 1.0). As stress\n","probability rises above 60%, we gradually reduce exposure down to a minimum of\n","30%. This happens smoothly, not in jarring steps.\n","\n","Why smooth? Because hard switches create whipsaw. If you go from 100% exposure\n","to 30% exposure instantly when stress probability crosses 60.1%, then back to\n","100% when it drops to 59.9%, you're churning your portfolio. Smooth transitions\n","based on probabilities (not hard regime labels) prevent this.\n","\n","This is conservative risk management. You're not trying to profit from regime\n","detection - you're using it to avoid getting crushed during crisis regimes.\n","Even if your regime detection is imperfect, reducing exposure when stress\n","signals rise is defensible to risk committees.\n","\n","**Decision Layer 2: Volatility Targeting**\n","\n","More sophisticated is using regime beliefs to predict forward volatility, then\n","scaling position sizes to hit a volatility target.\n","\n","Each regime has different volatility characteristics. Normal regime might have\n","1% daily vol, stressed regime 2.5%. At any time, we compute a mixture\n","volatility using regime probabilities as weights. If we're 70% confident we're\n","in Normal and 30% in Stressed, our predicted volatility is roughly\n","0.7×1% + 0.3×2.5% = 1.45%.\n","\n","Then we apply volatility targeting: if you want 15% annualized portfolio\n","volatility and you predict 20% based on regime mixture, you scale down leverage\n","to 15/20 = 0.75x. This automatically deleverages in high-vol regimes and\n","leverages up in low-vol regimes.\n","\n","This is more aggressive than simple risk overlay - you're actively using regime\n","information to optimize leverage. It requires confidence in your regime\n","detection and volatility predictions.\n","\n","**Decision Layer 3: Strategy Blending**\n","\n","The most sophisticated approach is blending different trading strategies based\n","on which regime you believe you're in. Different market conditions favor\n","different strategies.\n","\n","We implement two toy strategies to illustrate:\n","\n","A trend-following strategy that bets on momentum continuing. It looks at\n","cumulative returns over the past 20 days - if positive, go long; if negative,\n","go short.\n","\n","A mean-reversion strategy that bets against recent moves. It looks at the past\n","5 days - if returns were negative, go long expecting a bounce; if positive, go\n","short expecting reversal.\n","\n","These strategies perform differently in different regimes. In stressed, high-\n","volatility regimes, trends often persist as panic or euphoria builds. Mean\n","reversion fails because \"buy the dip\" gets punished. In calm, low-volatility\n","regimes, mean reversion often works as markets oscillate in ranges.\n","\n","So we blend them using regime probabilities. If we're 80% confident in Normal\n","regime, we weight 70% toward mean-reversion and 30% toward trend. If we're 80%\n","confident in Stressed, we flip it - 30% mean-reversion, 70% trend. The weights\n","shift continuously as regime beliefs evolve.\n","\n","**The Critical Timing Detail:**\n","\n","Throughout all three layers, we enforce proper action timing: signal at time t,\n","trade at time t+1. You compute today's regime probabilities using data through\n","today, but you don't act until tomorrow. This prevents lookahead bias where\n","you're somehow trading on today's close using today's close.\n","\n","We shift all signals forward by one day before computing positions. This seems\n","pedantic but it's essential. I've seen backtests that looked spectacular\n","because they implicitly assumed you could trade at today's close knowing today's\n","close - impossible in reality.\n","\n","**The Combined Decision:**\n","\n","The final position each day is: blended strategy signal × risk overlay exposure\n","multiplier × volatility targeting leverage. All three layers work together.\n","\n","For example: the blended signal says +0.8 (moderately long). The risk overlay\n","says 0.6 (elevated stress, reduce exposure). The vol targeting says 1.2 (low\n","predicted vol, leverage up slightly). Final position: 0.8 × 0.6 × 1.2 = 0.576.\n","\n","**What Gets Saved:**\n","\n","We save complete decision traces - every intermediate calculation for every day.\n","If a trade loses money, you can trace back through: What was the regime\n","probability? What did risk overlay recommend? What did vol targeting suggest?\n","What was the final position? Full accountability.\n","\n","This transparency is essential for institutional use. You need to explain\n","decisions to PMs, risk managers, and investors. \"Black box\" doesn't cut it."],"metadata":{"id":"GjZ8gXqXKnV4"}},{"cell_type":"markdown","source":["###11.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"Cer9FuzoKocT"}},{"cell_type":"code","source":["print(\"=\" * 80)\n","print(\"MAPPING REGIME BELIEFS TO TRADING DECISIONS\")\n","print(\"=\" * 80)\n","\n","# We'll implement three decision layers as per Chapter 14 scope:\n","# 1. Risk overlay: exposure multiplier based on stress probability\n","# 2. Volatility targeting: use regime mixture predicted vol to scale leverage\n","# 3. Strategy blending: blend trend and mean-reversion signals using regime posterior\n","\n","# Assume K=2: regime 0 = low vol (normal), regime 1 = high vol (stress)\n","# (This is aligned with our synthetic data generation)\n","\n","stress_threshold = CONFIG[\"decisions\"][\"risk_overlay_stress_threshold\"]\n","min_exposure = CONFIG[\"decisions\"][\"risk_overlay_min_exposure\"]\n","max_exposure = CONFIG[\"decisions\"][\"risk_overlay_max_exposure\"]\n","vol_target_annual = CONFIG[\"decisions\"][\"vol_target_annualized\"]\n","trend_lookback = CONFIG[\"decisions\"][\"trend_lookback\"]\n","meanrev_lookback = CONFIG[\"decisions\"][\"meanrev_lookback\"]\n","use_smooth_blend = CONFIG[\"decisions\"][\"regime_blend_smooth\"]\n","\n","# Decision Layer 1: Risk Overlay\n","# Reduce exposure if P(stress regime) > threshold\n","# exposure_mult = max_exposure if P(stress) < threshold, else linearly reduce to min_exposure\n","def compute_risk_overlay(gamma_filt, stress_idx=1):\n","    \"\"\"\n","    Compute exposure multiplier based on stress regime probability.\n","\n","    Returns:\n","        exposure_mult: array of shape (T,)\n","    \"\"\"\n","    T = gamma_filt.shape[0]\n","    exposure_mult = np.ones(T)\n","\n","    p_stress = gamma_filt[:, stress_idx]\n","\n","    for t in range(T):\n","        if p_stress[t] > stress_threshold:\n","            # Linearly reduce from max_exposure to min_exposure\n","            # as p_stress goes from threshold to 1.0\n","            slope = (min_exposure - max_exposure) / (1.0 - stress_threshold)\n","            exposure_mult[t] = max_exposure + slope * (p_stress[t] - stress_threshold)\n","        else:\n","            exposure_mult[t] = max_exposure\n","\n","    return exposure_mult\n","\n","exposure_mult = compute_risk_overlay(gamma_filt_all, stress_idx=1)\n","print(f\"Risk overlay computed: exposure range [{exposure_mult.min():.2f}, {exposure_mult.max():.2f}]\")\n","\n","# Decision Layer 2: Volatility Targeting\n","# Use regime mixture to predict volatility, then scale leverage inversely\n","def compute_vol_target_leverage(gamma_filt, returns, vol_target_annual):\n","    \"\"\"\n","    Compute leverage based on regime mixture predicted volatility.\n","\n","    Args:\n","        gamma_filt: filtered posteriors (T, K)\n","        returns: historical returns (T,)\n","        vol_target_annual: target annualized volatility\n","\n","    Returns:\n","        leverage: array of shape (T,)\n","    \"\"\"\n","    T = gamma_filt.shape[0]\n","    K = gamma_filt.shape[1]\n","    leverage = np.ones(T)\n","\n","    # For each time, compute regime mixture volatility\n","    # We'll use the estimated variances from the most recent refit\n","    # For simplicity, use a rolling estimate or the last known params\n","\n","    # Here we'll use a simplified approach: rolling realized vol per regime\n","    # and weight by posterior\n","\n","    vol_window = 20\n","    for t in range(vol_window, T):\n","        # Estimate vol per regime using recent data weighted by posterior\n","        regime_vols = np.zeros(K)\n","        for k in range(K):\n","            # Weight returns by posterior probability of regime k\n","            weights = gamma_filt[t-vol_window:t, k]\n","            weighted_returns = returns[t-vol_window:t] * weights\n","            regime_vols[k] = np.sqrt(np.sum(weighted_returns**2) / (weights.sum() + 1e-8))\n","\n","        # Mixture volatility (predicted for next period)\n","        mixture_vol = np.sqrt(np.sum(gamma_filt[t, :] * regime_vols**2))\n","\n","        # Annualize (assuming daily data, ~252 trading days)\n","        mixture_vol_annual = mixture_vol * np.sqrt(252)\n","\n","        # Leverage = vol_target / predicted_vol\n","        leverage[t] = vol_target_annual / (mixture_vol_annual + 1e-6)\n","\n","    # Cap leverage at reasonable bounds\n","    leverage = np.clip(leverage, 0.5, 3.0)\n","\n","    return leverage\n","\n","leverage_vol_target = compute_vol_target_leverage(gamma_filt_all, returns, vol_target_annual)\n","print(f\"Vol targeting leverage computed: range [{leverage_vol_target.min():.2f}, {leverage_vol_target.max():.2f}]\")\n","\n","# Decision Layer 3: Strategy Blending\n","# Define two toy strategies: trend-following and mean-reversion\n","def compute_trend_signal(returns, lookback):\n","    \"\"\"Simple trend signal: sign of cumulative return over lookback.\"\"\"\n","    T = len(returns)\n","    signal = np.zeros(T)\n","    for t in range(lookback, T):\n","        cum_ret = np.sum(returns[t-lookback:t])\n","        signal[t] = np.sign(cum_ret)  # +1 long, -1 short, 0 neutral\n","    return signal\n","\n","def compute_meanrev_signal(returns, lookback):\n","    \"\"\"Simple mean-reversion signal: negative sign of recent return.\"\"\"\n","    T = len(returns)\n","    signal = np.zeros(T)\n","    for t in range(lookback, T):\n","        recent_ret = np.sum(returns[t-lookback:t])\n","        signal[t] = -np.sign(recent_ret)  # Opposite of trend\n","    return signal\n","\n","trend_signal = compute_trend_signal(returns, trend_lookback)\n","meanrev_signal = compute_meanrev_signal(returns, meanrev_lookback)\n","\n","# Blend strategies using regime posteriors\n","# Assume: regime 0 (low vol) favors mean-reversion, regime 1 (high vol) favors trend\n","def compute_blended_signal(gamma_filt, trend_signal, meanrev_signal):\n","    \"\"\"\n","    Blend trend and mean-reversion signals using regime posteriors.\n","\n","    Regime 0 (low vol): 70% mean-rev, 30% trend\n","    Regime 1 (high vol): 30% mean-rev, 70% trend\n","    \"\"\"\n","    T = gamma_filt.shape[0]\n","    blended = np.zeros(T)\n","\n","    for t in range(T):\n","        p_low_vol = gamma_filt[t, 0]\n","        p_high_vol = gamma_filt[t, 1]\n","\n","        # Weights for each strategy\n","        w_meanrev = 0.7 * p_low_vol + 0.3 * p_high_vol\n","        w_trend = 0.3 * p_low_vol + 0.7 * p_high_vol\n","\n","        blended[t] = w_meanrev * meanrev_signal[t] + w_trend * trend_signal[t]\n","\n","    return blended\n","\n","blended_signal = compute_blended_signal(gamma_filt_all, trend_signal, meanrev_signal)\n","print(f\"Blended signal computed: range [{blended_signal.min():.2f}, {blended_signal.max():.2f}]\")\n","\n","# Final combined decision: signal * exposure_mult * leverage\n","# IMPORTANT: Action timing - signal at t, trade at t+1\n","# So we shift signals forward by 1 to avoid lookahead\n","positions = np.zeros(T)\n","positions[1:] = blended_signal[:-1] * exposure_mult[:-1] * leverage_vol_target[:-1]\n","\n","print(f\"Final positions computed with proper timing (signal at t, trade at t+1)\")\n","print(f\"Position range: [{positions.min():.2f}, {positions.max():.2f}]\")\n","\n","# Save decision traces\n","decision_trace = {\n","    \"run_id\": RUN_ID,\n","    \"exposure_mult\": exposure_mult.tolist(),\n","    \"leverage_vol_target\": leverage_vol_target.tolist(),\n","    \"trend_signal\": trend_signal.tolist(),\n","    \"meanrev_signal\": meanrev_signal.tolist(),\n","    \"blended_signal\": blended_signal.tolist(),\n","    \"final_positions\": positions.tolist(),\n","}\n","save_json(f\"{ARTIFACT_DIR}/decision_trace.json\", decision_trace)\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnDpRh7vMDl4","executionInfo":{"status":"ok","timestamp":1766677947642,"user_tz":360,"elapsed":49,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c911f0ac-88d5-442c-9b60-29ec8ade8c54"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","MAPPING REGIME BELIEFS TO TRADING DECISIONS\n","================================================================================\n","Risk overlay computed: exposure range [0.30, 1.00]\n","Vol targeting leverage computed: range [0.50, 3.00]\n","Blended signal computed: range [-1.00, 1.00]\n","Final positions computed with proper timing (signal at t, trade at t+1)\n","Position range: [-3.00, 2.20]\n","Saved: artifacts/ch14_run/decision_trace.json\n","\n"]}]},{"cell_type":"markdown","source":["##12.EVALUATION METRICS"],"metadata":{"id":"Jifrel5qNpAg"}},{"cell_type":"markdown","source":["###12.1.OVERVIEW"],"metadata":{"id":"N1VSB0anNsyZ"}},{"cell_type":"markdown","source":["\n","This section is where we confront reality - does regime detection actually help,\n","or are we just adding complexity that looks smart but delivers nothing? We\n","implement rigorous evaluation across multiple dimensions to answer this honestly.\n","\n","**The Four Strategies We Compare:**\n","\n","We don't just measure our regime-enhanced approach in isolation. That would be\n","meaningless - maybe it's profitable, but maybe a simpler approach works just as\n","well. We compare four variants:\n","\n","Baseline: Just the blended strategy signal with no regime overlay whatsoever.\n","This is our simplest benchmark - does raw strategy blending work at all?\n","\n","Regime Overlay (Filtered): Our main approach using causally-computed filtered\n","posteriors. This is the only version you could actually trade - it uses only\n","past information.\n","\n","Regime Overlay (Smoothed): The same approach but using smoothed posteriors that\n","peek into the future. This is INVALID for trading but we include it to measure\n","the cost of causality. How much better would we do if we had a crystal ball?\n","\n","Volatility Targeting Only: Uses regime-based volatility prediction but not the\n","risk overlay. This isolates whether volatility targeting alone adds value, or\n","if you need the full regime machinery.\n","\n","**Why the Smoothed Comparison Matters:**\n","\n","The gap between filtered and smoothed performance is brutally educational. In\n","my experience reviewing institutional strategies, I've seen smoothed backtests\n","show 2.5 Sharpe ratios while filtered implementation delivers 1.2 Sharpe - less\n","than half the performance.\n","\n","This gap reveals how much your strategy relies on knowing regime changes before\n","they're obvious. A small gap means regime detection is robust - you identify\n","transitions quickly. A large gap means you're mostly benefiting from hindsight\n","in the backtest, and live trading will disappoint.\n","\n","We explicitly label smoothed results as \"INVALID FOR TRADING\" in the output.\n","This warning protects against the temptation to rationalize using smoothed\n","results (\"maybe we could approximate this with...\"). No. The answer is no.\n","\n","**Trading Metrics We Compute:**\n","\n","For each strategy variant, we calculate standard performance metrics:\n","\n","Total return over the evaluation period - did you make money?\n","\n","Annualized return - adjusting for time to compare across periods.\n","\n","Annualized volatility - how wild was the ride? Calculated from daily returns,\n","scaled to annual using the square root of 252 trading days.\n","\n","Sharpe ratio - return per unit of volatility. The classic risk-adjusted\n","performance measure. A Sharpe above 1.0 is good, above 2.0 is excellent (and\n","suspect if it's a backtest).\n","\n","Maximum drawdown - the worst peak-to-trough decline. This is what keeps risk\n","managers awake. You can have great Sharpe but if you lost 40% at some point,\n","you might not survive to enjoy the long-term performance.\n","\n","**Statistical Evaluation: Predictive Log-Likelihood:**\n","\n","Beyond trading metrics, we evaluate the model statistically. For test segments\n","(data the model hasn't trained on), we compute the log-likelihood - how\n","probable did the model consider the data it actually saw?\n","\n","High log-likelihood means the model isn't surprised by new data - its regime\n","characterizations generalize well. Low log-likelihood means the model learned\n","patterns that don't persist out-of-sample - possible overfitting.\n","\n","This is a reality check independent of trading performance. Even if your\n","strategy makes money, if the HMM assigns low probability to what actually\n","happens, the model doesn't truly understand regime structure. You might be\n","getting lucky.\n","\n","**Regime Diagnostics:**\n","\n","We compute metrics that evaluate regime detection quality directly:\n","\n","Posterior entropy: How confident is the model in its regime assignments? Low\n","entropy means decisive (e.g., 95% sure it's Regime 1). High entropy means\n","uncertain (e.g., 50-50 split). Average entropy tells you if the model is making\n","clear calls or constantly ambivalent.\n","\n","Switch counts: How often do hard regime labels change? Too many switches (say,\n","30% of days) suggests the model is noisy - calling every volatility spike a\n","regime change. Too few switches suggests the model is sticky - maybe stuck in\n","one regime the whole time.\n","\n","Average regime duration: How long does each regime last on average? Real\n","financial regimes typically last weeks to months. If your average duration is\n","3 days, you're probably detecting noise, not regimes. If it's 200 days, you\n","might have only one effective regime.\n","\n","**Detection Quality (Thanks to Synthetic Data):**\n","\n","Because we used synthetic data with known true regimes, we can measure detection\n","accuracy directly. We compute:\n","\n","Detection lag: When the true regime switches, how long until the filtered\n","posterior recognizes it (crosses 50% probability for the new regime)? Average\n","lag of 2-3 days is excellent. 10+ days means you're slow to react.\n","\n","False switch rate: What percentage of detected switches don't align with true\n","regime changes? High false switch rate means you're crying wolf - overreacting\n","to noise.\n","\n","These metrics are only possible with synthetic data, which is why we used it.\n","In real trading, you never know the true regimes, so you can't measure these\n","directly. But synthetic testing teaches you what good detection looks like.\n","\n","**The Interpretation Section:**\n","\n","We print clear interpretations, not just numbers. \"Smoothed posteriors show\n","inflated performance due to hindsight\" - making the point explicit. \"Filtered\n","posteriors show realistic performance achievable in live trading\" - highlighting\n","what you can actually expect.\n","\n","This interpretation is critical for MBA students and practitioners who need to\n","present results to non-technical stakeholders. Numbers without context are\n","useless."],"metadata":{"id":"GfEyfZqoNuqY"}},{"cell_type":"markdown","source":["###12.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"xD4oRgbLNu7R"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"EVALUATION METRICS\")\n","print(\"=\" * 80)\n","\n","# We'll compute metrics for:\n","# 1. Baseline strategy (no regime overlay) - just blended signal\n","# 2. Regime overlay with FILTERED beliefs (our main approach)\n","# 3. Regime overlay with SMOOTHED beliefs (INVALID FOR TRADING - hindsight comparison)\n","# 4. Volatility targeting without regimes (continuous proxy baseline)\n","\n","# First, compute smoothed posteriors for the entire period (diagnostic only)\n","print(\"Computing smoothed posteriors for diagnostic comparison...\")\n","X_obs_full = X_obs[:, np.newaxis]\n","\n","# Use params from last refit for smoothing (just for illustration)\n","last_params = params_by_refit[-1]\n","pi_smooth = np.array(last_params[\"pi\"])\n","A_smooth = np.array(last_params[\"A\"])\n","mus_smooth = np.array(last_params[\"mus\"])\n","vars_smooth = np.array(last_params[\"vars\"])\n","\n","log_A_smooth = np.log(A_smooth + 1e-10)\n","log_pi_smooth = np.log(pi_smooth + 1e-10)\n","\n","# Apply smoothing to a segment for comparison (full period would be expensive)\n","smooth_segment_start = refit_times[0]\n","smooth_segment_end = min(smooth_segment_start + 500, T)\n","X_smooth_segment = X_obs[smooth_segment_start:smooth_segment_end, np.newaxis]\n","\n","gamma_smooth_segment, _ = compute_smoothed_posteriors(\n","    X_smooth_segment, log_A_smooth, log_pi_smooth, mus_smooth, vars_smooth\n",")\n","\n","# Extend to full period (for simplicity, reuse filtered for non-segment)\n","gamma_smooth_all = gamma_filt_all.copy()\n","gamma_smooth_all[smooth_segment_start:smooth_segment_end, :] = gamma_smooth_segment\n","\n","print(f\"Smoothed posteriors computed for segment [{smooth_segment_start}, {smooth_segment_end})\")\n","print(\"WARNING: Smoothed posteriors use future information - DIAGNOSTIC ONLY\")\n","print()\n","\n","# Define evaluation period (after warmup)\n","eval_start = refit_times[0]\n","eval_end = T\n","\n","returns_eval = returns[eval_start:eval_end]\n","T_eval = len(returns_eval)\n","\n","# Helper function to compute strategy returns\n","def compute_strategy_returns(positions_full, returns_full, eval_start, eval_end):\n","    \"\"\"\n","    Compute strategy returns with proper timing.\n","    Position at t generates return at t+1.\n","    \"\"\"\n","    positions_eval = positions_full[eval_start:eval_end]\n","    returns_strat = positions_eval[:-1] * returns_full[eval_start+1:eval_end]\n","    return returns_strat\n","\n","# Helper function to compute metrics\n","def compute_metrics(returns_strat, name):\n","    \"\"\"Compute trading metrics for a strategy.\"\"\"\n","    T_strat = len(returns_strat)\n","\n","    cum_ret = np.cumprod(1 + returns_strat) - 1\n","    total_ret = cum_ret[-1] if len(cum_ret) > 0 else 0.0\n","    annual_ret = (1 + total_ret) ** (252 / T_strat) - 1\n","\n","    annual_vol = np.std(returns_strat, ddof=1) * np.sqrt(252)\n","    sharpe = annual_ret / annual_vol if annual_vol > 1e-8 else 0.0\n","\n","    cum_wealth = np.cumprod(1 + returns_strat)\n","    running_max = np.maximum.accumulate(cum_wealth)\n","    drawdown = (cum_wealth - running_max) / running_max\n","    max_dd = np.min(drawdown)\n","\n","    # Turnover proxy: sum of absolute position changes\n","    # (Positions are already computed, so we use them from decision layer)\n","\n","    return {\n","        \"name\": name,\n","        \"total_return\": float(total_ret),\n","        \"annual_return\": float(annual_ret),\n","        \"annual_vol\": float(annual_vol),\n","        \"sharpe_ratio\": float(sharpe),\n","        \"max_drawdown\": float(max_dd),\n","    }\n","\n","# Strategy 1: Baseline (no regime overlay, just blended signal)\n","positions_baseline = np.zeros(T)\n","positions_baseline[1:] = blended_signal[:-1]  # No scaling by exposure or leverage\n","returns_baseline = compute_strategy_returns(positions_baseline, returns, eval_start, eval_end)\n","metrics_baseline = compute_metrics(returns_baseline, \"Baseline (no regime)\")\n","\n","# Strategy 2: Regime overlay with FILTERED beliefs (our main approach)\n","returns_filtered = compute_strategy_returns(positions, returns, eval_start, eval_end)\n","metrics_filtered = compute_metrics(returns_filtered, \"Regime overlay (FILTERED)\")\n","\n","# Strategy 3: Regime overlay with SMOOTHED beliefs (INVALID - hindsight)\n","# Recompute decisions using smoothed posteriors\n","exposure_mult_smooth = compute_risk_overlay(gamma_smooth_all, stress_idx=1)\n","leverage_vol_target_smooth = compute_vol_target_leverage(gamma_smooth_all, returns, vol_target_annual)\n","blended_signal_smooth = compute_blended_signal(gamma_smooth_all, trend_signal, meanrev_signal)\n","\n","positions_smooth = np.zeros(T)\n","positions_smooth[1:] = blended_signal_smooth[:-1] * exposure_mult_smooth[:-1] * leverage_vol_target_smooth[:-1]\n","\n","returns_smooth = compute_strategy_returns(positions_smooth, returns, eval_start, eval_end)\n","metrics_smooth = compute_metrics(returns_smooth, \"Regime overlay (SMOOTHED - INVALID)\")\n","\n","# Strategy 4: Volatility targeting without regimes (continuous baseline)\n","positions_vol_only = np.zeros(T)\n","positions_vol_only[1:] = blended_signal[:-1] * leverage_vol_target[:-1]  # No regime exposure mult\n","returns_vol_only = compute_strategy_returns(positions_vol_only, returns, eval_start, eval_end)\n","metrics_vol_only = compute_metrics(returns_vol_only, \"Vol targeting (no regime)\")\n","\n","# Print metrics comparison\n","print(\"=\" * 80)\n","print(\"METRICS COMPARISON\")\n","print(\"=\" * 80)\n","for metrics in [metrics_baseline, metrics_filtered, metrics_smooth, metrics_vol_only]:\n","    print(f\"\\n{metrics['name']}:\")\n","    print(f\"  Total Return: {metrics['total_return']:.2%}\")\n","    print(f\"  Annual Return: {metrics['annual_return']:.2%}\")\n","    print(f\"  Annual Vol: {metrics['annual_vol']:.2%}\")\n","    print(f\"  Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n","    print(f\"  Max Drawdown: {metrics['max_drawdown']:.2%}\")\n","\n","print()\n","print(\"INTERPRETATION:\")\n","print(\"  - Smoothed posteriors (INVALID) typically show inflated performance due to hindsight.\")\n","print(\"  - Filtered posteriors (VALID) show realistic performance achievable in live trading.\")\n","print(\"  - Comparison demonstrates the cost of causality constraints.\")\n","print()\n","\n","# Statistical evaluation: predictive log-likelihood on test segments\n","# We'll compute it for the last refit segment\n","if len(refit_times) > 1:\n","    test_start = refit_times[-1]\n","    test_end = T\n","\n","    X_test = X_obs[test_start:test_end, np.newaxis]\n","\n","    # Use last refit params\n","    last_params = params_by_refit[-1]\n","    pi_test = np.array(last_params[\"pi\"])\n","    A_test = np.array(last_params[\"A\"])\n","    mus_test = np.array(last_params[\"mus\"])\n","    vars_test = np.array(last_params[\"vars\"])\n","\n","    log_A_test = np.log(A_test + 1e-10)\n","    log_pi_test = np.log(pi_test + 1e-10)\n","\n","    _, test_log_likelihood = forward_algorithm(X_test, log_A_test, log_pi_test, mus_test, vars_test)\n","    test_ll_per_obs = test_log_likelihood / len(X_test)\n","\n","    print(f\"Test segment predictive log-likelihood (per obs): {test_ll_per_obs:.4f}\")\n","else:\n","    print(\"(Insufficient refits for test segment evaluation)\")\n","\n","print()\n","\n","# Regime diagnostics\n","print(\"=\" * 80)\n","print(\"REGIME DIAGNOSTICS\")\n","print(\"=\" * 80)\n","\n","# Posterior entropy (average)\n","def compute_entropy(gamma):\n","    \"\"\"Shannon entropy of posterior distribution.\"\"\"\n","    return -np.sum(gamma * np.log(gamma + 1e-10), axis=1)\n","\n","entropy_filt = compute_entropy(gamma_filt_all[eval_start:eval_end, :])\n","print(f\"Average filtered posterior entropy: {entropy_filt.mean():.4f} (max: {np.log(K_hmm):.4f})\")\n","\n","# Switch counts (hard label switches)\n","hard_labels_filt = np.argmax(gamma_filt_all, axis=1)\n","switches_filt = np.sum(hard_labels_filt[eval_start+1:eval_end] != hard_labels_filt[eval_start:eval_end-1])\n","print(f\"Hard label switches (filtered): {switches_filt} ({switches_filt / T_eval * 100:.2f}% of period)\")\n","\n","# Average regime durations\n","durations = []\n","current_regime = hard_labels_filt[eval_start]\n","current_duration = 1\n","for t in range(eval_start+1, eval_end):\n","    if hard_labels_filt[t] == current_regime:\n","        current_duration += 1\n","    else:\n","        durations.append(current_duration)\n","        current_regime = hard_labels_filt[t]\n","        current_duration = 1\n","durations.append(current_duration)\n","\n","print(f\"Average regime duration: {np.mean(durations):.1f} steps (median: {np.median(durations):.1f})\")\n","print()\n","\n","# Save evaluation metrics\n","evaluation_metrics = {\n","    \"run_id\": RUN_ID,\n","    \"eval_period\": [eval_start, eval_end],\n","    \"strategies\": {\n","        \"baseline\": metrics_baseline,\n","        \"filtered\": metrics_filtered,\n","        \"smoothed_invalid\": metrics_smooth,\n","        \"vol_targeting_only\": metrics_vol_only,\n","    },\n","    \"regime_diagnostics\": {\n","        \"avg_entropy\": float(entropy_filt.mean()),\n","        \"hard_label_switches\": int(switches_filt),\n","        \"avg_regime_duration\": float(np.mean(durations)),\n","        \"median_regime_duration\": float(np.median(durations)),\n","    },\n","}\n","save_json(f\"{ARTIFACT_DIR}/evaluation_metrics.json\", evaluation_metrics)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dbsEBh7N5Qi","executionInfo":{"status":"ok","timestamp":1766678461942,"user_tz":360,"elapsed":163,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5934a8a1-7ab6-4086-8c6a-4da84292af7c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","EVALUATION METRICS\n","================================================================================\n","Computing smoothed posteriors for diagnostic comparison...\n","Smoothed posteriors computed for segment [252, 752)\n","WARNING: Smoothed posteriors use future information - DIAGNOSTIC ONLY\n","\n","================================================================================\n","METRICS COMPARISON\n","================================================================================\n","\n","Baseline (no regime):\n","  Total Return: 6.75%\n","  Annual Return: 2.23%\n","  Annual Vol: 17.80%\n","  Sharpe Ratio: 0.13\n","  Max Drawdown: -27.45%\n","\n","Regime overlay (FILTERED):\n","  Total Return: 17.02%\n","  Annual Return: 5.45%\n","  Annual Vol: 11.46%\n","  Sharpe Ratio: 0.47\n","  Max Drawdown: -10.36%\n","\n","Regime overlay (SMOOTHED - INVALID):\n","  Total Return: -3.34%\n","  Annual Return: -1.14%\n","  Annual Vol: 9.54%\n","  Sharpe Ratio: -0.12\n","  Max Drawdown: -15.99%\n","\n","Vol targeting (no regime):\n","  Total Return: 28.48%\n","  Annual Return: 8.82%\n","  Annual Vol: 14.66%\n","  Sharpe Ratio: 0.60\n","  Max Drawdown: -13.78%\n","\n","INTERPRETATION:\n","  - Smoothed posteriors (INVALID) typically show inflated performance due to hindsight.\n","  - Filtered posteriors (VALID) show realistic performance achievable in live trading.\n","  - Comparison demonstrates the cost of causality constraints.\n","\n","Test segment predictive log-likelihood (per obs): -26.7221\n","\n","================================================================================\n","REGIME DIAGNOSTICS\n","================================================================================\n","Average filtered posterior entropy: 0.0869 (max: 0.6931)\n","Hard label switches (filtered): 57 (7.62% of period)\n","Average regime duration: 12.9 steps (median: 12.0)\n","\n","Saved: artifacts/ch14_run/evaluation_metrics.json\n"]}]},{"cell_type":"markdown","source":["##13.FAILURE ANALYSIS AND PLOTS"],"metadata":{"id":"GTei6RdpPiDp"}},{"cell_type":"markdown","source":["###13.1.OVERVIEW"],"metadata":{"id":"6heRUu6uTeMS"}},{"cell_type":"markdown","source":["\n","\n","This section is about ruthless honesty and transparency - we dig into how and\n","where our regime detection fails, visualize the problems, and create an audit\n","trail that would satisfy any risk committee or regulator. This is what separates\n","institutional-grade work from academic exercises.\n","\n","**Why Failure Analysis Matters:**\n","\n","Every model fails. The question is whether you understand HOW it fails, or\n","whether you discover failure modes in production when real money is at stake.\n","We proactively identify failure patterns in our controlled environment.\n","\n","Good risk management isn't about claiming your model is perfect - it's about\n","knowing your model's weaknesses, documenting them, and having monitoring in\n","place to detect when those weaknesses are causing problems. This section builds\n","that foundation.\n","\n","**The Five Audit Plots We Generate:**\n","\n","**Plot 1: Posterior Timeline (Filtered)** - This is your main operational\n","dashboard. It shows three aligned time series: actual returns (what traders see),\n","filtered regime probabilities (what the model believes in real-time), and hard\n","regime labels derived from those probabilities.\n","\n","You can visually inspect: Does the model call high-vol regimes when volatility\n","spikes? Does it react quickly or with lag? Does it flip-flop rapidly or show\n","stable regime assignments? This plot would go in your monthly performance\n","report to senior management.\n","\n","We also overlay the true regime path (since we have it from synthetic data) so\n","you can see detection accuracy visually. In production with real data, you\n","wouldn't have this ground truth, but the filtered probabilities and returns\n","alone still tell a story.\n","\n","**Plot 2: Filtered vs Smoothed Comparison (Leakage Diagnostic)** - This is the\n","educational plot that shows the cost of causality. We plot both filtered and\n","smoothed probabilities for the same regime, then show their difference.\n","\n","You'll see smoothed probabilities are sharper - they jump to 90%+ confidence\n","quickly because they know what happens next. Filtered probabilities are fuzzier -\n","40%, 55%, 65% - because they're making real-time inferences without future\n","information.\n","\n","The difference plot (filtered minus smoothed) visually demonstrates information\n","leakage. Negative differences mean \"the smoothed version knew something we\n","didn't in real-time.\" This plot is your defense when someone asks why live\n","performance doesn't match the backtest - you can show exactly how much\n","hindsight was inflating results.\n","\n","**Plot 3: Switch Frequency Over Time (Stability Check)** - Using a rolling\n","window (63 days, roughly 3 months), we count how many regime switches occur in\n","each window and plot this over time.\n","\n","Stable periods show low switch counts - the model sees persistent regimes.\n","Unstable periods show high switch counts - the model is confused, rapidly\n","flipping between regime assignments.\n","\n","If switch frequency suddenly spikes, it might mean: (a) markets genuinely\n","became more turbulent and regimes are changing rapidly, or (b) your model is\n","breaking down and calling noise as regime changes. Either way, it's a warning\n","flag that deserves investigation.\n","\n","**Plot 4: Drawdown Comparison** - We plot the underwater curves (drawdowns from\n","peak wealth) for all strategy variants. This shows where each approach loses\n","money and how deep the pain gets.\n","\n","If the regime-enhanced strategy has shallower drawdowns than baseline, regime\n","detection is providing risk protection - it's reducing exposure before or\n","during crashes. If drawdowns are similar or worse, regime detection isn't\n","helping with risk management, despite adding complexity.\n","\n","The smoothed version typically shows unrealistically shallow drawdowns - it\n","\"predicts\" crashes because it knows they're coming. The gap between smoothed\n","and filtered drawdowns shows how much your backtest is lying.\n","\n","**Plot 5: Transition Matrix Heatmaps** - We visualize the learned transition\n","matrices from first and last refits as heatmaps with probabilities annotated.\n","\n","This shows whether the model is learning sensible regime persistence. A good\n","transition matrix has high diagonal values (regimes persist) and low off-\n","diagonal values (switches are relatively rare).\n","\n","If you see a transition matrix with 50-50 probabilities everywhere, the model\n","hasn't learned meaningful persistence - regimes might as well be random. If\n","diagonal values are 99%+, the model is too sticky - it almost never switches,\n","meaning you effectively have just one regime.\n","\n","Comparing first and last refit matrices shows parameter stability. If they're\n","wildly different, regime characteristics changed drastically over your sample,\n","or your model is unstable. If they're similar, you have consistent regime\n","structure.\n","\n","**The Failure Analysis Printout:**\n","\n","Beyond plots, we compute and print diagnostic metrics:\n","\n","**Churn Rate:** What percentage of days show regime label changes? High churn\n","(>20%) suggests noisy detection. Low churn (< 2%) suggests the model barely\n","switches.\n","\n","**Detection Lag Statistics:** Using our ground truth, we measure average and\n","median lag between true regime switches and when filtered probabilities\n","recognize them. Median lag of 5+ days means you're slow to react - regime\n","changes are well underway before you notice.\n","\n","**False Switch Precision:** What percentage of detected switches align with\n","true regime changes (within ±5 days tolerance)? Low precision ( < 50%) means\n","most of your regime calls are false alarms.\n","\n","These metrics give you concrete, quantitative assessments of detection quality.\n","When your CIO asks \"how reliable is this regime detection?\", you have specific\n","numbers, not vague claims.\n","\n","**The Audit Trail:**\n","\n","Everything gets saved: plots as PNG files, metrics as JSON. Six months later,\n","if someone questions a specific regime call or trading decision, you can\n","recreate the exact model state, show the probabilities, display the audit\n","plots.\n","\n","This is governance in action - not bureaucracy, but genuine accountability and\n","transparency that lets you defend your methodology under scrutiny."],"metadata":{"id":"UciTGWdEZsa-"}},{"cell_type":"markdown","source":["###13.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"wyvq5yhKaTxB"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"FAILURE ANALYSIS & AUDIT PLOTS\")\n","print(\"=\" * 80)\n","\n","# Plot 1: Posterior timelines (filtered)\n","fig, axes = plt.subplots(3, 1, figsize=(16, 10), sharex=True)\n","\n","# Subplot 1: Returns and true regimes\n","axes[0].plot(returns[eval_start:eval_end], linewidth=0.5, alpha=0.6, label='Returns')\n","axes[0].set_ylabel(\"Returns\")\n","axes[0].set_title(\"Returns and True Regime Path\")\n","axes[0].grid(True, alpha=0.3)\n","\n","ax0_twin = axes[0].twinx()\n","ax0_twin.plot(s_true[eval_start:eval_end], drawstyle='steps-post',\n","              linewidth=2, color='black', alpha=0.7, label='True Regime')\n","ax0_twin.set_ylabel(\"True Regime\")\n","ax0_twin.set_yticks(range(K))\n","\n","# Subplot 2: Filtered posteriors\n","for k in range(K_hmm):\n","    axes[1].plot(gamma_filt_all[eval_start:eval_end, k],\n","                 label=f'P(Regime {k})', linewidth=1.5, alpha=0.8)\n","axes[1].set_ylabel(\"Filtered Posterior\")\n","axes[1].set_title(\"Filtered Posteriors (Causal)\")\n","axes[1].legend(loc='upper right')\n","axes[1].grid(True, alpha=0.3)\n","axes[1].set_ylim([0, 1])\n","\n","# Subplot 3: Hard labels (filtered)\n","axes[2].plot(hard_labels_filt[eval_start:eval_end],\n","             drawstyle='steps-post', linewidth=1.5, color='blue', label='Filtered')\n","axes[2].set_ylabel(\"Hard Label\")\n","axes[2].set_xlabel(\"Time\")\n","axes[2].set_title(\"Hard Regime Labels (Filtered)\")\n","axes[2].set_yticks(range(K_hmm))\n","axes[2].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{ARTIFACT_DIR}/plots/posterior_timeline_filtered.png\", dpi=150, bbox_inches='tight')\n","plt.close()\n","print(f\"Saved: {ARTIFACT_DIR}/plots/posterior_timeline_filtered.png\")\n","\n","# Plot 2: Filtered vs Smoothed comparison (leakage diagnostic)\n","fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n","\n","seg_start = smooth_segment_start\n","seg_end = min(smooth_segment_end, eval_end)\n","\n","# Show for regime 1 (stress regime)\n","k_show = 1\n","axes[0].plot(gamma_filt_all[seg_start:seg_end, k_show],\n","             label=f'Filtered P(Regime {k_show})', linewidth=1.5, alpha=0.8)\n","axes[0].plot(gamma_smooth_all[seg_start:seg_end, k_show],\n","             label=f'Smoothed P(Regime {k_show}) - DIAGNOSTIC ONLY',\n","             linewidth=1.5, alpha=0.8, linestyle='--')\n","axes[0].set_ylabel(\"Posterior Prob\")\n","axes[0].set_title(f\"Filtered vs Smoothed Posteriors (Regime {k_show})\")\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","axes[0].set_ylim([0, 1])\n","\n","# Difference plot\n","diff_filt_smooth = gamma_filt_all[seg_start:seg_end, k_show] - gamma_smooth_all[seg_start:seg_end, k_show]\n","axes[1].plot(diff_filt_smooth, linewidth=1.5, color='red', alpha=0.8)\n","axes[1].axhline(0, color='black', linestyle='--', linewidth=0.8)\n","axes[1].set_ylabel(\"Filtered - Smoothed\")\n","axes[1].set_xlabel(\"Time\")\n","axes[1].set_title(\"Difference: Filtered vs Smoothed (shows leakage)\")\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{ARTIFACT_DIR}/plots/filtered_vs_smoothed_leakage.png\", dpi=150, bbox_inches='tight')\n","plt.close()\n","print(f\"Saved: {ARTIFACT_DIR}/plots/filtered_vs_smoothed_leakage.png\")\n","\n","# Plot 3: Switch frequency over time (rolling window)\n","switch_window = 63  # ~3 months\n","rolling_switches = np.zeros(eval_end - eval_start - switch_window)\n","for i in range(len(rolling_switches)):\n","    window_start = eval_start + i\n","    window_end = window_start + switch_window\n","    labels_window = hard_labels_filt[window_start:window_end]\n","    switches_in_window = np.sum(labels_window[1:] != labels_window[:-1])\n","    rolling_switches[i] = switches_in_window\n","\n","fig, ax = plt.subplots(figsize=(16, 5))\n","ax.plot(rolling_switches, linewidth=1.5, alpha=0.8)\n","ax.set_ylabel(f\"Switches (rolling {switch_window}-day window)\")\n","ax.set_xlabel(\"Time\")\n","ax.set_title(\"Regime Switch Frequency Over Time (Filtered)\")\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f\"{ARTIFACT_DIR}/plots/switch_frequency_over_time.png\", dpi=150, bbox_inches='tight')\n","plt.close()\n","print(f\"Saved: {ARTIFACT_DIR}/plots/switch_frequency_over_time.png\")\n","\n","# Plot 4: Drawdown curves (baseline vs regime-enhanced)\n","def compute_drawdown_series(returns_strat):\n","    cum_wealth = np.cumprod(1 + returns_strat)\n","    running_max = np.maximum.accumulate(cum_wealth)\n","    drawdown = (cum_wealth - running_max) / running_max\n","    return drawdown\n","\n","dd_baseline = compute_drawdown_series(returns_baseline)\n","dd_filtered = compute_drawdown_series(returns_filtered)\n","dd_smooth = compute_drawdown_series(returns_smooth)\n","\n","fig, ax = plt.subplots(figsize=(16, 6))\n","ax.plot(dd_baseline, label='Baseline (no regime)', linewidth=1.5, alpha=0.8)\n","ax.plot(dd_filtered, label='Regime overlay (FILTERED)', linewidth=1.5, alpha=0.8)\n","ax.plot(dd_smooth, label='Regime overlay (SMOOTHED - INVALID)',\n","        linewidth=1.5, alpha=0.8, linestyle='--')\n","ax.set_ylabel(\"Drawdown\")\n","ax.set_xlabel(\"Time\")\n","ax.set_title(\"Drawdown Comparison\")\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f\"{ARTIFACT_DIR}/plots/drawdown_comparison.png\", dpi=150, bbox_inches='tight')\n","plt.close()\n","print(f\"Saved: {ARTIFACT_DIR}/plots/drawdown_comparison.png\")\n","\n","# Plot 5: Transition matrix heatmap per refit (show first and last)\n","def plot_transition_matrix(A, title, filename):\n","    fig, ax = plt.subplots(figsize=(6, 5))\n","    im = ax.imshow(A, cmap='Blues', vmin=0, vmax=1)\n","    ax.set_xticks(range(K_hmm))\n","    ax.set_yticks(range(K_hmm))\n","    ax.set_xlabel(\"To State\")\n","    ax.set_ylabel(\"From State\")\n","    ax.set_title(title)\n","\n","    # Annotate with values\n","    for i in range(K_hmm):\n","        for j in range(K_hmm):\n","            ax.text(j, i, f'{A[i, j]:.3f}', ha='center', va='center', color='black')\n","\n","    plt.colorbar(im, ax=ax)\n","    plt.tight_layout()\n","    plt.savefig(filename, dpi=150, bbox_inches='tight')\n","    plt.close()\n","\n","# First refit\n","A_first = np.array(params_by_refit[0][\"A\"])\n","plot_transition_matrix(A_first, \"Transition Matrix (First Refit)\",\n","                       f\"{ARTIFACT_DIR}/plots/transition_matrix_first.png\")\n","print(f\"Saved: {ARTIFACT_DIR}/plots/transition_matrix_first.png\")\n","\n","# Last refit\n","A_last = np.array(params_by_refit[-1][\"A\"])\n","plot_transition_matrix(A_last, \"Transition Matrix (Last Refit)\",\n","                       f\"{ARTIFACT_DIR}/plots/transition_matrix_last.png\")\n","print(f\"Saved: {ARTIFACT_DIR}/plots/transition_matrix_last.png\")\n","\n","print()\n","\n","# Failure Analysis Printout\n","print(\"=\" * 80)\n","print(\"FAILURE ANALYSIS\")\n","print(\"=\" * 80)\n","\n","# Churn metrics: how often does hard label change?\n","churn_rate = switches_filt / (eval_end - eval_start - 1)\n","print(f\"Label churn rate: {churn_rate:.4f} ({churn_rate * 100:.2f}% of steps)\")\n","\n","# Detection lag: compare filtered labels to true regimes (synthetic truth available)\n","# We'll compute average lag when true regime switches\n","true_switches = []\n","for t in range(eval_start+1, eval_end):\n","    if s_true[t] != s_true[t-1]:\n","        true_switches.append(t)\n","\n","detection_lags = []\n","for switch_t in true_switches:\n","    # Find when filtered posterior crosses 0.5 for new regime\n","    new_regime = s_true[switch_t]\n","\n","    # Look forward up to 20 steps\n","    detected = False\n","    for lag in range(1, 21):\n","        if switch_t + lag >= eval_end:\n","            break\n","        if gamma_filt_all[switch_t + lag, new_regime] > 0.5:\n","            detection_lags.append(lag)\n","            detected = True\n","            break\n","\n","    if not detected:\n","        detection_lags.append(20)  # Cap at 20\n","\n","if len(detection_lags) > 0:\n","    avg_lag = np.mean(detection_lags)\n","    med_lag = np.median(detection_lags)\n","    print(f\"Detection lag (steps until P>0.5 after true switch): avg={avg_lag:.1f}, median={med_lag:.1f}\")\n","else:\n","    print(\"No true regime switches detected in eval period\")\n","\n","# False switch counts: filtered switches that don't align with true switches\n","# Define alignment: within ±5 steps\n","aligned_switches = 0\n","for filt_t in range(eval_start+1, eval_end):\n","    if hard_labels_filt[filt_t] != hard_labels_filt[filt_t-1]:\n","        # Check if there's a true switch within ±5 steps\n","        for true_t in true_switches:\n","            if abs(filt_t - true_t) <= 5:\n","                aligned_switches += 1\n","                break\n","\n","false_switches = switches_filt - aligned_switches\n","print(f\"False switches (not aligned with true regime changes): {false_switches} / {switches_filt}\")\n","print(f\"Switch precision: {aligned_switches / switches_filt * 100:.1f}%\")\n","\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jx8s6MA7aX1w","executionInfo":{"status":"ok","timestamp":1766681319324,"user_tz":360,"elapsed":3137,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5223d6a6-77cf-4357-f4bf-e99bf9c8ce1f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FAILURE ANALYSIS & AUDIT PLOTS\n","================================================================================\n","Saved: artifacts/ch14_run/plots/posterior_timeline_filtered.png\n","Saved: artifacts/ch14_run/plots/filtered_vs_smoothed_leakage.png\n","Saved: artifacts/ch14_run/plots/switch_frequency_over_time.png\n","Saved: artifacts/ch14_run/plots/drawdown_comparison.png\n","Saved: artifacts/ch14_run/plots/transition_matrix_first.png\n","Saved: artifacts/ch14_run/plots/transition_matrix_last.png\n","\n","================================================================================\n","FAILURE ANALYSIS\n","================================================================================\n","Label churn rate: 0.0763 (7.63% of steps)\n","Detection lag (steps until P>0.5 after true switch): avg=6.2, median=1.0\n","False switches (not aligned with true regime changes): 38 / 57\n","Switch precision: 33.3%\n","\n"]}]},{"cell_type":"markdown","source":["##14.SAVING THE ARTIFACT BUNDLE AND CECKLIST"],"metadata":{"id":"KClu8WHLacae"}},{"cell_type":"markdown","source":["###14.1.OVERVIEW"],"metadata":{"id":"Fna4dZLQamY0"}},{"cell_type":"markdown","source":["\n","\n","This section is the grand finale - we package everything we've created into a\n","complete, auditable artifact bundle and run through a final checklist to verify\n","we've maintained discipline throughout. Think of this as closing the books on a\n","trading day - everything must be reconciled and documented.\n","\n","**The Run Manifest:**\n","\n","We create a master manifest that ties everything together. This JSON file\n","contains the complete configuration we used, the unique run ID (hash of the\n","config), timestamp of when we ran the notebook, and a directory of all artifacts\n","we created.\n","\n","Why does this matter? Because six months from now, someone (maybe you) will\n","look at results and ask: \"What exactly did we do here?\" The manifest answers\n","that question definitively. No ambiguity, no \"I think we used these settings.\"\n","\n","**The Model Specification Document:**\n","\n","Separate from the run manifest, we create a focused model specification that\n","describes the HMM itself - two states, Gaussian emissions, EM training with\n","deterministic initialization, variance floors, pseudocounts, walk-forward\n","evaluation with monthly refits.\n","\n","This is the document you'd hand to a new team member or show to regulators. It\n","explains what the model is and how it was trained without requiring them to\n","read code or sift through hundreds of config parameters.\n","\n","**Complete File Inventory:**\n","\n","We've created artifacts across multiple directories:\n","- Data fingerprint (what data we used)\n","- Feature manifest (how features were constructed)\n","- EM trace (how training progressed)\n","- Model spec (what the model is)\n","- Evaluation metrics (how it performed)\n","- Decision traces (every trading decision)\n","- Parameters by refit (21+ JSON files, one per monthly refit)\n","- Filtered posteriors (NumPy array of regime probabilities)\n","- Eight diagnostic plots\n","\n","Every single file gets listed with its path. This inventory proves nothing is\n","hidden - you can see the entire evidence base.\n","\n","**The Final Checklist:**\n","\n","We print a systematic checklist verifying we maintained discipline:\n","\n","✓ Global seed set - deterministic results, reproducible\n","✓ Config hash generated - every run has unique, verifiable identity\n","✓ Causality tests passed - both prefix invariance and future perturbation tests\n","✓ Filtered-only invariants respected - smoothing clearly labeled diagnostic only\n","✓ Proper action timing enforced - signal at t, trade at t+1, no lookahead\n","✓ All files written - complete audit trail exists\n","\n","Each checkmark represents a potential failure mode we've defended against. The\n","seed ensures reproducibility - run this notebook again with the same config,\n","get identical results. The causality tests prove we didn't accidentally leak\n","future information. The filtered-only discipline ensures our trading results\n","are honest, not inflated by hindsight.\n","\n","**The Key Takeaways Section:**\n","\n","We end with explicit lessons for practitioners:\n","\n","1. HMM filtering provides causal regime beliefs - you can trade on these\n","2. Smoothing offers better fit but uses future information - diagnostic only\n","3. Walk-forward with refits maintains time-series integrity - don't train once\n","4. Regime-to-decision mapping is flexible - risk overlay, vol targeting, blending\n","5. Causality gates are non-negotiable - discipline prevents expensive mistakes\n","\n","These aren't abstract academic points. Each one represents a decision point\n","where practitioners commonly make mistakes that cost money. We've shown the\n","right way.\n","\n","**What This Enables:**\n","\n","With this artifact bundle, you could:\n","- Reproduce these exact results\n","- Audit every decision made\n","- Extend the analysis to real data\n","- Defend the methodology to skeptical stakeholders\n","- Train new team members on proper regime detection\n","- Implement this in production with confidence\n","\n","This is production-grade documentation. Not perfect (nothing is), but thorough,\n","honest, and defensible. That's the standard institutional work requires."],"metadata":{"id":"CLm1n7iLaoWk"}},{"cell_type":"markdown","source":["###14.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"8qqc-eWlaonc"}},{"cell_type":"code","source":["\n","print(\"=\" * 80)\n","print(\"SAVING ARTIFACT BUNDLE\")\n","print(\"=\" * 80)\n","\n","# Write run manifest\n","run_manifest = {\n","    \"run_id\": RUN_ID,\n","    \"chapter\": CONFIG[\"chapter\"],\n","    \"timestamp\": datetime.now().isoformat(),\n","    \"config\": CONFIG,\n","    \"artifacts_written\": {\n","        \"data_fingerprint\": f\"{ARTIFACT_DIR}/data_fingerprint.json\",\n","        \"feature_manifest\": f\"{ARTIFACT_DIR}/feature_manifest.json\",\n","        \"em_trace\": f\"{ARTIFACT_DIR}/em_trace.json\",\n","        \"evaluation_metrics\": f\"{ARTIFACT_DIR}/evaluation_metrics.json\",\n","        \"decision_trace\": f\"{ARTIFACT_DIR}/decision_trace.json\",\n","        \"params_by_refit\": f\"{ARTIFACT_DIR}/params_by_refit/\",\n","        \"posteriors\": f\"{ARTIFACT_DIR}/posteriors/\",\n","        \"plots\": f\"{ARTIFACT_DIR}/plots/\",\n","    },\n","}\n","save_json(f\"{ARTIFACT_DIR}/run_manifest.json\", run_manifest)\n","\n","# Model specification\n","model_spec = {\n","    \"run_id\": RUN_ID,\n","    \"model_type\": \"Hidden Markov Model (HMM)\",\n","    \"emission_family\": CONFIG[\"hmm\"][\"emission_family\"],\n","    \"num_states\": CONFIG[\"hmm\"][\"K\"],\n","    \"training_method\": \"EM (Baum-Welch)\",\n","    \"initialization\": \"deterministic_quantile_based\",\n","    \"variance_floor\": CONFIG[\"hmm\"][\"variance_floor\"],\n","    \"transition_pseudocount\": CONFIG[\"hmm\"][\"transition_pseudocount\"],\n","    \"walk_forward\": {\n","        \"initial_training_window\": CONFIG[\"walkforward\"][\"initial_training_window\"],\n","        \"refit_cadence\": CONFIG[\"walkforward\"][\"refit_cadence\"],\n","        \"training_window_type\": CONFIG[\"walkforward\"][\"training_window_type\"],\n","        \"num_refits\": len(params_by_refit),\n","    },\n","}\n","save_json(f\"{ARTIFACT_DIR}/model_spec.json\", model_spec)\n","\n","print()\n","print(\"=\" * 80)\n","print(\"FINAL CHECKLIST\")\n","print(\"=\" * 80)\n","print(f\"✓ Global seed set: {GLOBAL_SEED}\")\n","print(f\"✓ Config hash / run_id: {RUN_ID}\")\n","print(f\"✓ Causality tests passed:\")\n","print(f\"    - Prefix invariance for features: PASSED\")\n","print(f\"    - Future perturbation test for filtered posteriors: PASSED\")\n","print(f\"✓ Filtered-only invariants respected:\")\n","print(f\"    - Smoothing clearly labeled as DIAGNOSTIC ONLY\")\n","print(f\"    - All trading decisions based on filtered posteriors\")\n","print(f\"    - Proper action timing enforced (signal at t, trade at t+1)\")\n","print(f\"✓ Files written:\")\n","print(f\"    - run_manifest.json\")\n","print(f\"    - data_fingerprint.json\")\n","print(f\"    - feature_manifest.json\")\n","print(f\"    - model_spec.json\")\n","print(f\"    - em_trace.json\")\n","print(f\"    - evaluation_metrics.json\")\n","print(f\"    - decision_trace.json\")\n","print(f\"    - params_by_refit/ ({len(params_by_refit)} files)\")\n","print(f\"    - posteriors/ (gamma_filt_all.npy)\")\n","print(f\"    - plots/ (8 png files)\")\n","print()\n","print(\"=\" * 80)\n","print(\"CHAPTER 14 NOTEBOOK COMPLETE\")\n","print(\"=\" * 80)\n","print()\n","print(\"KEY TAKEAWAYS:\")\n","print(\"  1. HMM filtering provides causal regime beliefs suitable for live trading\")\n","print(\"  2. Smoothing offers better fit but uses future information (diagnostic only)\")\n","print(\"  3. Walk-forward evaluation with periodic refits maintains time-series integrity\")\n","print(\"  4. Regime-to-decision mapping enables risk overlay, vol targeting, and strategy blending\")\n","print(\"  5. Causality gates enforce no-leakage discipline throughout the pipeline\")\n","print()\n","print(\"All artifacts saved to:\", ARTIFACT_DIR)\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOO9v3_Jautr","executionInfo":{"status":"ok","timestamp":1766681630017,"user_tz":360,"elapsed":61,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"7f353030-f811-4fe7-bd28-7ee68b436c15"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","SAVING ARTIFACT BUNDLE\n","================================================================================\n","Saved: artifacts/ch14_run/run_manifest.json\n","Saved: artifacts/ch14_run/model_spec.json\n","\n","================================================================================\n","FINAL CHECKLIST\n","================================================================================\n","✓ Global seed set: 42\n","✓ Config hash / run_id: b79e468dfa049ad5\n","✓ Causality tests passed:\n","    - Prefix invariance for features: PASSED\n","    - Future perturbation test for filtered posteriors: PASSED\n","✓ Filtered-only invariants respected:\n","    - Smoothing clearly labeled as DIAGNOSTIC ONLY\n","    - All trading decisions based on filtered posteriors\n","    - Proper action timing enforced (signal at t, trade at t+1)\n","✓ Files written:\n","    - run_manifest.json\n","    - data_fingerprint.json\n","    - feature_manifest.json\n","    - model_spec.json\n","    - em_trace.json\n","    - evaluation_metrics.json\n","    - decision_trace.json\n","    - params_by_refit/ (36 files)\n","    - posteriors/ (gamma_filt_all.npy)\n","    - plots/ (8 png files)\n","\n","================================================================================\n","CHAPTER 14 NOTEBOOK COMPLETE\n","================================================================================\n","\n","KEY TAKEAWAYS:\n","  1. HMM filtering provides causal regime beliefs suitable for live trading\n","  2. Smoothing offers better fit but uses future information (diagnostic only)\n","  3. Walk-forward evaluation with periodic refits maintains time-series integrity\n","  4. Regime-to-decision mapping enables risk overlay, vol targeting, and strategy blending\n","  5. Causality gates enforce no-leakage discipline throughout the pipeline\n","\n","All artifacts saved to: artifacts/ch14_run\n","\n"]}]},{"cell_type":"markdown","source":["##15.REAL DATA ADAPTER"],"metadata":{"id":"q2yPmXU-awps"}},{"cell_type":"markdown","source":["###15.1.OVERVIEW"],"metadata":{"id":"U426ACpIa5Vf"}},{"cell_type":"markdown","source":["\n","\n","This section demonstrates how you would adapt the entire notebook to work with\n","real market data instead of synthetic data. It's completely optional and off by\n","default - the notebook runs perfectly without it. Think of this as the bridge\n","between our controlled learning environment and production reality.\n","\n","**Why It's Isolated:**\n","\n","We deliberately keep real data separate because it introduces complications\n","that would distract from learning the core concepts. Real data has:\n","- Missing days (market holidays, delisted stocks)\n","- Corporate actions (splits, dividends) that create jumps\n","- Different time periods with different regime characteristics\n","- No ground truth - you never know if you detected regimes correctly\n","\n","By isolating this in a separate, optional cell, students can master the\n","methodology on clean synthetic data first, then tackle real-world messiness\n","when they're ready.\n","\n","**How It Works When Enabled:**\n","\n","If you set the flag to True and have yfinance installed, this section downloads\n","real historical price data for an ETF like SPY (S&P 500). It converts the\n","pandas DataFrame that yfinance returns into pure NumPy arrays - maintaining our\n","\"no pandas\" discipline.\n","\n","It then runs the same feature engineering pipeline we built for synthetic data -\n","rolling volatility, rolling mean, drawdown proxy, absolute return average. Same\n","transformations, same causality discipline, applied to real market data.\n","\n","Finally, it trains a single HMM on a segment of real data as a proof-of-concept,\n","showing that all the machinery we built works on actual markets, not just our\n","carefully crafted synthetic examples.\n","\n","**The Current Error:**\n","\n","The error messages you see occur when yfinance returns empty or incomplete data -\n","perhaps due to network issues, API changes, or the requested date range being\n","unavailable. Since this cell is optional and ENABLE_REAL_DATA defaults to False,\n","these errors don't affect the notebook's core functionality.\n","\n","**For Production Use:**\n","\n","To actually use real data throughout the notebook, you wouldn't just enable\n","this cell. You'd replace the synthetic data generation in Section 3 entirely\n","with a robust real data loader that handles:\n","- Missing data gracefully\n","- Corporate action adjustments\n","- Multiple instruments if desired\n","- Data quality checks and validation\n","\n","This cell is a starting point showing the concept, not a production-ready\n","implementation. That's intentional - production data infrastructure is complex\n","enough to deserve its own dedicated engineering effort.\n"],"metadata":{"id":"IqOgXTw4bMDZ"}},{"cell_type":"markdown","source":["###15.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"rSf1OhKXbL1B"}},{"cell_type":"code","source":["# Cell 15 (Optional)\n","# ============================================================================\n","# REAL DATA ADAPTER (ISOLATED; OFF BY DEFAULT)\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"OPTIONAL: REAL DATA ADAPTER\")\n","print(\"=\" * 80)\n","print(\"This cell is OPTIONAL and not required for the notebook to run.\")\n","print(\"It demonstrates how to adapt the notebook to real market data using yfinance.\")\n","print(\"The cell is OFF by default. To enable, install yfinance and set ENABLE_REAL_DATA = True.\")\n","print()\n","\n","ENABLE_REAL_DATA = True  # Set to True to enable\n","\n","if ENABLE_REAL_DATA:\n","    try:\n","        import yfinance as yf\n","\n","        print(\"Downloading real market data (SPY)...\")\n","        ticker = \"SPY\"\n","        start_date = \"2020-01-01\"\n","        end_date = \"2024-12-01\"\n","\n","        # CRITICAL FIX: Use tickers= parameter (yfinance API change)\n","        data = yf.download(\n","            tickers=ticker,\n","            start=start_date,\n","            end=end_date,\n","            interval=\"1d\",\n","            auto_adjust=True,\n","            progress=False\n","        )\n","\n","        # Check if data is empty immediately\n","        if data.empty or len(data) == 0:\n","            print(f\"ERROR: yfinance returned empty DataFrame for {ticker}\")\n","            print(f\"  Attempted date range: {start_date} to {end_date}\")\n","            print()\n","            print(\"NOTE: This cell is OPTIONAL. The notebook completed successfully without it.\")\n","            raise ValueError(\"Empty data returned from yfinance\")\n","\n","        print(f\"✓ Downloaded {len(data)} days of data\")\n","        print(f\"  Date range: {data.index[0]} to {data.index[-1]}\")\n","        print(f\"  Columns: {list(data.columns)}\")\n","        print()\n","\n","        # CRITICAL FIX: Handle MultiIndex columns from yfinance\n","        # When downloading a single ticker, yfinance may return MultiIndex with (column, ticker)\n","        if isinstance(data.columns, pd.MultiIndex):\n","            print(\"Detected MultiIndex columns, flattening...\")\n","            # For single ticker, just take the first level (column names)\n","            data.columns = data.columns.get_level_values(0)\n","            print(f\"  Flattened columns: {list(data.columns)}\")\n","\n","        # Now access Close column (should work after flattening)\n","        if 'Close' not in data.columns:\n","            print(f\"ERROR: 'Close' column not found after flattening\")\n","            print(f\"  Available columns: {list(data.columns)}\")\n","            raise ValueError(\"Missing 'Close' column\")\n","\n","        # Convert to NumPy arrays (NO PANDAS beyond this point)\n","        close_prices = data['Close'].to_numpy()\n","        dates = data.index.to_numpy()\n","\n","        # Remove any NaN values from close prices\n","        valid_mask = ~np.isnan(close_prices)\n","        close_prices = close_prices[valid_mask]\n","        dates = dates[valid_mask]\n","\n","        # Validation: Check minimum data length after cleaning\n","        min_required_days = 300\n","        if len(close_prices) < min_required_days:\n","            print(f\"ERROR: Insufficient data after cleaning\")\n","            print(f\"  Got {len(close_prices)} valid days, need at least {min_required_days}\")\n","            raise ValueError(\"Insufficient data for analysis\")\n","\n","        print(f\"✓ Cleaned to {len(close_prices)} valid days of {ticker} data\")\n","        print(f\"  Actual date range: {dates[0]} to {dates[-1]}\")\n","\n","        # Compute returns (log returns)\n","        returns_real = np.diff(np.log(close_prices))\n","        T_real = len(returns_real)\n","\n","        print(f\"✓ Computed {T_real} returns from price data\")\n","\n","        # Basic statistics with validation\n","        if T_real > 0:\n","            mean_ret = np.mean(returns_real)\n","            std_ret = np.std(returns_real, ddof=1)\n","            min_ret = np.min(returns_real)\n","            max_ret = np.max(returns_real)\n","\n","            print(f\"\\nReal data statistics:\")\n","            print(f\"  Mean return: {mean_ret:.6f} ({mean_ret * 252:.2%} annualized)\")\n","            print(f\"  Std return:  {std_ret:.6f} ({std_ret * np.sqrt(252):.2%} annualized)\")\n","            print(f\"  Min return:  {min_ret:.6f}\")\n","            print(f\"  Max return:  {max_ret:.6f}\")\n","        else:\n","            raise ValueError(\"No returns computed - empty price series\")\n","\n","        print()\n","\n","        # Build feature matrix using same process as synthetic data\n","        print(\"Building features from real data...\")\n","\n","        feat_vol_real = rolling_std(returns_real, vol_window)\n","        feat_mean_real = rolling_mean(returns_real, mean_window)\n","        feat_dd_real = rolling_drawdown(returns_real, dd_window)\n","        feat_abs_real = rolling_abs_mean(returns_real, abs_ret_window)\n","\n","        X_real = np.column_stack([feat_vol_real, feat_mean_real, feat_dd_real, feat_abs_real])\n","\n","        print(f\"✓ Feature matrix created: shape {X_real.shape}\")\n","\n","        # Apply same transformations\n","        print(\"Applying feature transformations (winsorization, normalization)...\")\n","        for d in range(X_real.shape[1]):\n","            X_real[:, d] = causal_winsorize(X_real[:, d], winsorize_n_std)\n","\n","        if vol_normalize:\n","            for d in range(X_real.shape[1]):\n","                feat_vol_d = rolling_std(X_real[:, d], vol_window)\n","                feat_vol_d = np.where(feat_vol_d < 1e-8, 1e-8, feat_vol_d)\n","                X_real[:, d] = X_real[:, d] / feat_vol_d\n","\n","        # Handle NaNs\n","        first_valid_real = max(vol_window, mean_window, dd_window, abs_ret_window)\n","        X_real[:first_valid_real, :] = 0.0\n","        for d in range(X_real.shape[1]):\n","            for t in range(first_valid_real, T_real):\n","                if np.isnan(X_real[t, d]):\n","                    X_real[t, d] = X_real[t-1, d]\n","\n","        nan_count = np.isnan(X_real).sum()\n","        print(f\"✓ Features processed. NaN count: {nan_count}\")\n","\n","        if nan_count > 0:\n","            print(\"WARNING: NaNs remain after processing. Check data quality.\")\n","\n","        print()\n","\n","        # Train HMM on real data (single training run as example)\n","        # Use a safe training window\n","        train_window_size = min(500, T_real - first_valid_real - 100)\n","\n","        if train_window_size < 100:\n","            print(f\"ERROR: Insufficient data for training. Need at least 100 days after warmup.\")\n","            print(f\"Available: {T_real - first_valid_real} days\")\n","            raise ValueError(\"Insufficient training data\")\n","\n","        train_start_real = first_valid_real\n","        train_end_real = first_valid_real + train_window_size\n","\n","        X_train_real = X_real[train_start_real:train_end_real, :]\n","\n","        print(f\"Training HMM on real data...\")\n","        print(f\"  Training window: [{train_start_real}, {train_end_real})\")\n","        print(f\"  Training samples: {train_window_size}\")\n","\n","        pi_real, A_real, mus_real, vars_real, em_trace_real = em_train(\n","            X_train_real, K_hmm, em_max_iters, em_tol, init_diag, pseudocount\n","        )\n","\n","        print(f\"✓ EM converged in {len(em_trace_real)} iterations\")\n","        print(f\"  Stop reason: {em_trace_real[-1].get('stop_reason', 'unknown')}\")\n","        print(f\"  Final log-likelihood: {em_trace_real[-1]['log_likelihood']:.2f}\")\n","        print()\n","\n","        # Display learned parameters\n","        print(\"Learned HMM parameters from real data:\")\n","        print(f\"  Initial probabilities: {pi_real}\")\n","        print(f\"  Transition matrix:\")\n","        for i in range(K_hmm):\n","            print(f\"    {A_real[i, :]}\")\n","        print(f\"  Emission means (feature 0 - rolling vol):\")\n","        for k in range(K_hmm):\n","            print(f\"    Regime {k}: {mus_real[k, 0]:.6f}\")\n","        print()\n","\n","        # Quick visualization of real data\n","        fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n","\n","        # Plot returns\n","        axes[0].plot(returns_real, linewidth=0.5, alpha=0.7)\n","        axes[0].set_ylabel(\"Returns\")\n","        axes[0].set_title(f\"{ticker} Returns ({dates[1]} to {dates[-1]})\")\n","        axes[0].grid(True, alpha=0.3)\n","\n","        # Plot rolling volatility feature\n","        axes[1].plot(X_real[:, 0], linewidth=0.8, alpha=0.7, color='orange')\n","        axes[1].set_ylabel(\"Rolling Vol (normalized)\")\n","        axes[1].set_xlabel(\"Time\")\n","        axes[1].set_title(\"Rolling Volatility Feature\")\n","        axes[1].grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.savefig(f\"{ARTIFACT_DIR}/plots/real_data_example.png\", dpi=150, bbox_inches='tight')\n","        plt.close()\n","        print(f\"✓ Saved plot: {ARTIFACT_DIR}/plots/real_data_example.png\")\n","        print()\n","\n","        print(\"=\" * 80)\n","        print(\"REAL DATA ADAPTER DEMONSTRATION COMPLETE\")\n","        print(\"=\" * 80)\n","        print()\n","        print(\"Next steps to use real data throughout the notebook:\")\n","        print(\"  1. Replace synthetic data generation in Section 3 with this download code\")\n","        print(\"  2. Add robust data quality checks and validation\")\n","        print(\"  3. Handle corporate actions (splits, dividends) if using individual stocks\")\n","        print(\"  4. Implement proper missing data handling for weekends/holidays\")\n","        print(\"  5. Consider multiple instruments and correlation structures\")\n","        print()\n","        print(\"The core HMM machinery (Sections 6-14) works identically on real data.\")\n","        print(\"All causality gates, walk-forward evaluation, and decision mapping apply.\")\n","        print()\n","\n","    except ImportError as e:\n","        print(\"=\" * 80)\n","        print(\"IMPORT ERROR: Required package not installed\")\n","        print(\"=\" * 80)\n","        print()\n","        print(f\"Error: {str(e)}\")\n","        print()\n","        print(\"To use real data, install yfinance:\")\n","        print(\"  !pip install yfinance\")\n","        print()\n","        print(\"Then set ENABLE_REAL_DATA = True and re-run this cell.\")\n","        print()\n","        print(\"Note: This is optional. The notebook completed successfully without real data.\")\n","        print()\n","\n","    except Exception as e:\n","        print(\"=\" * 80)\n","        print(f\"ERROR IN REAL DATA ADAPTER: {type(e).__name__}\")\n","        print(\"=\" * 80)\n","        print()\n","        print(f\"Error details: {str(e)}\")\n","        print()\n","        print(\"This is EXPECTED BEHAVIOR for an optional cell.\")\n","        print(\"Possible causes:\")\n","        print(\"  - Network issues or yfinance API problems\")\n","        print(\"  - Invalid date range or ticker symbol\")\n","        print(\"  - Insufficient data returned (empty DataFrame)\")\n","        print(\"  - Rate limiting from Yahoo Finance\")\n","        print(\"  - Temporary API maintenance\")\n","        print()\n","        print(\"IMPORTANT: The notebook has COMPLETED SUCCESSFULLY despite this error.\")\n","        print(\"All core functionality (Sections 1-14) executed properly with synthetic data.\")\n","        print(\"Real data integration is optional and can be debugged separately.\")\n","        print()\n","        print(\"If you need real data:\")\n","        print(\"  1. Verify internet connection\")\n","        print(\"  2. Try updating yfinance: pip install --upgrade yfinance\")\n","        print(\"  3. Try a more recent date range\")\n","        print(\"  4. Wait a few minutes (Yahoo may be rate limiting)\")\n","        print()\n","\n","else:\n","    print(\"Real data adapter is OFF (ENABLE_REAL_DATA = False)\")\n","    print()\n","    print(\"This is the default and recommended setting for learning.\")\n","    print(\"The notebook demonstrates all concepts using synthetic data where\")\n","    print(\"we have ground truth and can measure detection accuracy precisely.\")\n","    print()\n","    print(\"To experiment with real market data:\")\n","    print(\"  1. Install yfinance: !pip install yfinance\")\n","    print(\"  2. Set ENABLE_REAL_DATA = True in this cell\")\n","    print(\"  3. Re-run the cell\")\n","    print()\n","\n","print()\n","print(\"=\" * 80)\n","print(\"END OF NOTEBOOK\")\n","print(\"=\" * 80)\n","print()\n","print(\"Summary:\")\n","print(\"  ✓ Synthetic market data generated with known regimes\")\n","print(\"  ✓ Features engineered with causality-safe rolling computations\")\n","print(\"  ✓ HMM filtering and smoothing implemented from scratch\")\n","print(\"  ✓ EM (Baum-Welch) training with deterministic initialization\")\n","print(\"  ✓ Walk-forward evaluation with periodic refits\")\n","print(\"  ✓ Regime beliefs mapped to trading decisions\")\n","print(\"  ✓ Rigorous evaluation across multiple dimensions\")\n","print(\"  ✓ Failure analysis and audit plots generated\")\n","print(\"  ✓ Complete artifact bundle saved\")\n","print()\n","print(\"Key principle enforced throughout:\")\n","print(\"  FILTERED POSTERIORS ONLY for trading signals\")\n","print(\"  (Smoothing is diagnostic only - uses future information)\")\n","print()\n","print(f\"All artifacts saved to: {ARTIFACT_DIR}\")\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJhsTBxReBfJ","executionInfo":{"status":"ok","timestamp":1766682301456,"user_tz":360,"elapsed":11913,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"e06d63ca-6b38-4785-8836-c94ef97db2da"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","OPTIONAL: REAL DATA ADAPTER\n","================================================================================\n","This cell is OPTIONAL and not required for the notebook to run.\n","It demonstrates how to adapt the notebook to real market data using yfinance.\n","The cell is OFF by default. To enable, install yfinance and set ENABLE_REAL_DATA = True.\n","\n","Downloading real market data (SPY)...\n","✓ Downloaded 1237 days of data\n","  Date range: 2020-01-02 00:00:00 to 2024-11-29 00:00:00\n","  Columns: [('Close', 'SPY'), ('High', 'SPY'), ('Low', 'SPY'), ('Open', 'SPY'), ('Volume', 'SPY')]\n","\n","Detected MultiIndex columns, flattening...\n","  Flattened columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n","✓ Cleaned to 1237 valid days of SPY data\n","  Actual date range: 2020-01-02T00:00:00.000000000 to 2024-11-29T00:00:00.000000000\n","✓ Computed 1236 returns from price data\n","\n","Real data statistics:\n","  Mean return: 0.000558 (14.07% annualized)\n","  Std return:  0.013342 (21.18% annualized)\n","  Min return:  -0.115887\n","  Max return:  0.086731\n","\n","Building features from real data...\n","✓ Feature matrix created: shape (1236, 4)\n","Applying feature transformations (winsorization, normalization)...\n","✓ Features processed. NaN count: 0\n","\n","Training HMM on real data...\n","  Training window: [60, 560)\n","  Training samples: 500\n","✓ EM converged in 39 iterations\n","  Stop reason: log_likelihood_plateau\n","  Final log-likelihood: -2895169.62\n","\n","Learned HMM parameters from real data:\n","  Initial probabilities: [1.00000000e+00 7.60747733e-12]\n","  Transition matrix:\n","    [0.96375969 0.03624031]\n","    [0.063801 0.936199]\n","  Emission means (feature 0 - rolling vol):\n","    Regime 0: 5.079077\n","    Regime 1: 15.049990\n","\n","✓ Saved plot: artifacts/ch14_run/plots/real_data_example.png\n","\n","================================================================================\n","REAL DATA ADAPTER DEMONSTRATION COMPLETE\n","================================================================================\n","\n","Next steps to use real data throughout the notebook:\n","  1. Replace synthetic data generation in Section 3 with this download code\n","  2. Add robust data quality checks and validation\n","  3. Handle corporate actions (splits, dividends) if using individual stocks\n","  4. Implement proper missing data handling for weekends/holidays\n","  5. Consider multiple instruments and correlation structures\n","\n","The core HMM machinery (Sections 6-14) works identically on real data.\n","All causality gates, walk-forward evaluation, and decision mapping apply.\n","\n","\n","================================================================================\n","END OF NOTEBOOK\n","================================================================================\n","\n","Summary:\n","  ✓ Synthetic market data generated with known regimes\n","  ✓ Features engineered with causality-safe rolling computations\n","  ✓ HMM filtering and smoothing implemented from scratch\n","  ✓ EM (Baum-Welch) training with deterministic initialization\n","  ✓ Walk-forward evaluation with periodic refits\n","  ✓ Regime beliefs mapped to trading decisions\n","  ✓ Rigorous evaluation across multiple dimensions\n","  ✓ Failure analysis and audit plots generated\n","  ✓ Complete artifact bundle saved\n","\n","Key principle enforced throughout:\n","  FILTERED POSTERIORS ONLY for trading signals\n","  (Smoothing is diagnostic only - uses future information)\n","\n","All artifacts saved to: artifacts/ch14_run\n","\n"]}]},{"cell_type":"markdown","source":["##16.CONCLUSIONS"],"metadata":{"id":"QtU4C5i3dxoP"}},{"cell_type":"markdown","source":["\n","This notebook has taken you through the complete lifecycle of institutional-grade\n","regime detection - from mathematical foundations to production-ready implementation.\n","We've built a Hidden Markov Model system that doesn't just detect market regimes\n","in theory, but does so in a way that respects the brutal constraints of real\n","trading: causality, time-series integrity, and honest evaluation.\n","\n","The journey began with synthetic data where we controlled the ground truth. This\n","wasn't an academic indulgence - it was strategic. You cannot debug what you cannot\n","measure. By knowing the true regimes, we could quantify detection lag, measure\n","false switch rates, and understand exactly where and why our model succeeds or\n","fails. This is the difference between hoping your model works and knowing it works.\n","\n","**What Makes This Implementation Production-Grade**\n","\n","Three principles distinguish this notebook from typical academic exercises or\n","research prototypes.\n","\n","First, **causality discipline is non-negotiable**. We didn't just talk about\n","avoiding lookahead bias - we enforced it mechanically through causality gates.\n","The prefix invariance tests verify that features computed at time t depend only\n","on data through time t. The future perturbation test proves that changing data\n","after time t doesn't affect filtered posteriors at time t. These aren't optional\n","nice-to-haves; they're hard stops that throw errors if violated. This discipline\n","prevents the single most common backtesting failure: accidentally using information\n","you wouldn't have had in real-time.\n","\n","Second, **we separated filtering from smoothing with extreme prejudice**. Smoothing\n","produces cleaner, more confident regime assignments - and is completely unusable\n","for trading. By implementing both and explicitly comparing them, we demonstrated\n","the cost of causality. The gap between smoothed and filtered performance isn't a\n","bug; it's the price of honesty. When someone shows you a backtest with suspiciously\n","good regime detection, ask whether they used smoothing. If they did, or if they\n","don't know what smoothing means, walk away.\n","\n","Third, **walk-forward evaluation with periodic refits mirrors production reality**.\n","Markets evolve. Regime characteristics from 2019 don't necessarily apply in 2023.\n","We don't train one model and use it forever - we retrain monthly as new data\n","arrives, exactly as you would in production. Each refit uses only data available\n","at that point. Each application period uses fixed parameters until the next refit.\n","This isn't just realistic; it's a test of model stability. If parameters swing\n","wildly between refits, your model isn't capturing stable regime structure - it's\n","overfitting to noise.\n","\n","**The Power of Probabilistic Regime Assignment**\n","\n","One of the most important lessons embedded in this implementation is the value of\n","probabilities over hard labels. We never force the model to commit to \"you're\n","definitely in Regime 2 right now.\" Instead, it says \"I think there's 65% chance\n","you're in Regime 2, 35% chance you're in Regime 1.\"\n","\n","This probabilistic output is gold for trading. Hard regime switches create\n","whipsaw - you flip from 100% exposure to 30% exposure and back as the model\n","changes its mind. Probabilistic blending creates smooth transitions. As stress\n","probability rises from 40% to 50% to 60%, you gradually reduce exposure. No\n","sudden moves, no unnecessary trading costs, no artificial regime boundaries.\n","\n","This also provides natural risk management. When posterior entropy is high (the\n","model is uncertain, probabilities close to 50-50), you know you're in a murky\n","period. Maybe you reduce position sizes across the board, regardless of which\n","regime the model leans toward. When entropy is low (95% confident in one regime),\n","you can trade more aggressively. The model's confidence becomes a meta-signal for\n","position sizing.\n","\n","**From Regime Detection to Trading Decisions**\n","\n","The three decision layers we implemented - risk overlay, volatility targeting,\n","and strategy blending - demonstrate how regime information flows into actual\n","trading logic. But they're templates, not prescriptions. Your implementation\n","might look completely different based on your strategies, risk tolerance, and\n","market focus.\n","\n","The risk overlay is conservative: reduce exposure when stress probability exceeds\n","a threshold. This is defensible to risk committees and doesn't require believing\n","your regime detection is perfect - you're just reducing risk when warning signals\n","appear.\n","\n","Volatility targeting is more aggressive: you're actively using regime-based\n","volatility predictions to optimize leverage. This requires confidence in your\n","model and careful monitoring. If your volatility predictions are poor, you'll\n","over-leverage in high-vol regimes and under-leverage in low-vol regimes - exactly\n","wrong.\n","\n","Strategy blending is most sophisticated: different strategies for different\n","regimes, smoothly weighted by posterior probabilities. This requires you to have\n","multiple strategies with different regime preferences, and to understand which\n","strategies work when. The blending prevents you from getting locked into the\n","wrong strategy when regimes shift.\n","\n","All three share a critical feature: **proper action timing**. Signal at time t,\n","trade at time t+1. This seems pedantic until you realize how many backtests\n","implicitly assume you can trade at today's close knowing today's close - impossible\n","in practice. The one-period lag is reality.\n","**The Evaluation Discipline**\n","\n","Perhaps the most valuable section is the evaluation framework. We don't just\n","compute a Sharpe ratio and declare victory. We evaluate across multiple dimensions:\n","\n","Trading performance metrics (return, volatility, Sharpe, drawdown) tell you whether\n","the system makes money, but they don't tell you why or whether it will continue.\n","\n","Statistical evaluation (predictive log-likelihood) tells you whether the model\n","actually understands regime structure or just got lucky. High likelihood on\n","out-of-sample data suggests genuine pattern recognition. Low likelihood suggests\n","overfitting.\n","\n","Regime diagnostics (entropy, switch frequency, duration) tell you whether the\n","model is making sensible regime calls. Too many switches means it's noisy. Too\n","few switches means it's not detecting anything. Durations of 3 days mean you're\n","detecting noise; durations of 200 days mean you have one effective regime.\n","\n","Detection quality metrics (lag, false switches) - available only with synthetic\n","data - tell you ground truth about accuracy. This teaches you what good detection\n","looks like, calibrating your intuition for when you move to real data where you\n","don't have ground truth.\n","\n","The comparison between baseline (no regimes), filtered regime overlay, smoothed\n","regime overlay (invalid), and volatility-only targeting is crucial. It shows\n","whether regime detection adds value, and how much of apparent value comes from\n","hindsight bias.\n","\n","**The Artifacts and Governance**\n","\n","Every serious trading system needs an audit trail. The artifact bundle we create -\n","manifests, parameters, traces, plots - provides complete transparency. Six months\n","from now, you can reconstruct exactly what the model knew, when it knew it, and\n","what decisions it made.\n","\n","This isn't bureaucracy; it's accountability. When a trade loses money, you trace\n","back through the decision chain: What was the regime probability? What did risk\n","overlay recommend? What was the final position? Was this a model failure or bad\n","luck? You can't answer these questions without comprehensive logging.\n","\n","The run manifest with its config hash ensures reproducibility. The EM trace shows\n","training wasn't just a black box - you can see convergence, log-likelihood\n","progression, stopping criteria. The parameter history across refits shows whether\n","the model is stable or drifting. The causality test results prove you maintained\n","discipline.\n","\n","### Moving to Production\n","\n","This notebook is a foundation, not a finished product. Moving to production\n","requires additional infrastructure:\n","\n","Real-time data pipelines that handle market holidays, missing data, corporate\n","actions, and late reports. The optional real-data adapter shows the concept, but\n","production data engineering is its own discipline.\n","\n","Monitoring and alerting systems that detect when the model is behaving strangely -\n","excessive switching, parameter drift beyond thresholds, likelihood dropping,\n","prediction errors spiking.\n","\n","Position reconciliation that verifies the positions you think you have match what\n","your broker says you have. Model bugs that create position mismatches can be\n","catastrophic.\n","\n","Risk controls that override the model when portfolio heat exceeds limits,\n","regardless of what regime probabilities suggest. Models serve risk management,\n","not the other way around.\n","\n","\n","If you take one thing from this notebook, let it be this: **intellectual honesty\n","in quantitative finance is not optional**. The markets will find every shortcut\n","you took, every assumption you didn't validate, every causality violation you\n","dismissed as \"probably fine.\"\n","\n","Smoothed posteriors look great in backtests - don't use them for trading. Hard\n","regime labels are tempting - use probabilistic beliefs instead. Training once on\n","all historical data is easier - retrain periodically in walk-forward mode.\n","Ignoring causality constraints is faster - enforce them mechanically with gates.\n","\n","The discipline required to do this properly - to build something that's not just\n","clever but actually honest - separates professionals from amateurs, and surviving\n","traders from failed ones.\n","\n","This is the standard. Now go build something real.\n"],"metadata":{"id":"nKn3S7rkeeLb"}},{"cell_type":"code","source":[],"metadata":{"id":"wN3Hyelbedqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1D99UXaed0FU"},"execution_count":null,"outputs":[]}]}